@article{allisonMultipleImputationMissing2000,
  title = {Multiple {{Imputation}} for {{Missing Data}}: {{A Cautionary Tale}}},
  shorttitle = {Multiple {{Imputation}} for {{Missing Data}}},
  author = {Allison, Paul D.},
  year = {2000},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {28},
  number = {3},
  pages = {301--309},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124100028003003},
  abstract = {Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. Software using a propensity score classifier with the approximate Bayesian bootstrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.},
  langid = {english},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Allison2000\\Allison_2000_Multiple Imputation for Missing Data.pdf}
}

@article{azurMultipleImputationChained2011,
  title = {Multiple Imputation by Chained Equations: What Is It and How Does It Work?},
  shorttitle = {Multiple Imputation by Chained Equations},
  author = {Azur, Melissa J. and Stuart, Elizabeth A. and Frangakis, Constantine and Leaf, Philip J.},
  year = {2011},
  month = feb,
  journal = {International Journal of Methods in Psychiatric Research},
  volume = {20},
  number = {1},
  pages = {40--49},
  issn = {1049-8931},
  doi = {10.1002/mpr.329},
  abstract = {Multivariate imputation by chained equations (MICE) has emerged as a principled method of dealing with missing data. Despite properties that make MICE particularly useful for large imputation procedures and advances in software development that now make it accessible to many researchers, many psychiatric researchers have not been trained in these methods and few practical resources exist to guide researchers in the implementation of this technique. This paper provides an introduction to the MICE method with a focus on practical aspects and challenges in using this method. A brief review of software programs available to implement MICE and then analyze multiply imputed data is also provided. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  pmcid = {PMC3074241},
  pmid = {21499542},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/azurMultipleImputationChained2011.md;C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Notes\\Literature_Review_Notes\\azurMultipleImputationChained2011.md;G\:\\My Drive\\Zotero\\Research\\Missing Data\\Azur et al2011\\Azur et al_2011_Multiple imputation by chained equations.pdf}
}

@article{balesRacialEthnicDifferentials2012,
  title = {Racial/{{Ethnic Differentials}} in {{Sentencing}} to {{Incarceration}}},
  author = {Bales, William D. and Piquero, Alex R.},
  year = {2012},
  month = oct,
  journal = {Justice Quarterly},
  volume = {29},
  number = {5},
  pages = {742--773},
  publisher = {{Routledge}},
  issn = {0741-8825},
  doi = {10.1080/07418825.2012.659674},
  abstract = {Few criminological topics are as controversial as the relationships between race, ethnicity, crime, and criminal justice outcomes\textemdash especially incarceration. This paper assesses whether Blacks and Hispanics are disadvantaged at the sentencing phase of the justice system and whether the findings depend on the use of traditional regression-based methods to control for legally relevant variables vs. the use of precision matching methods, which attend to potential sample selection bias that occurs when there are not exact matches for those sentenced to incarceration and non-incarceration. Analysis of the population of Florida offenders from 1994 to 2006 using both methodologies indicates that Black offenders continue to be disproportionately incarcerated compared to White or Hispanic offenders, and that Hispanic offenders were slightly more likely than White offenders to be incarcerated.},
  keywords = {disparity,incarceration,minorities,over-representation,precision matching,Read},
  annotation = {\_eprint: https://doi.org/10.1080/07418825.2012.659674},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/balesRacialEthnicDifferentials2012.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\Bales_Piquero2012\\Bales_Piquero_2012_Racial-Ethnic Differentials in Sentencing to Incarceration.pdf;C\:\\Users\\stocb\\Zotero\\storage\\URGMJABA\\07418825.2012.html}
}

@article{bartlettAsymptoticallyUnbiasedEstimation2015,
  title = {Asymptotically {{Unbiased Estimation}} of {{Exposure Odds Ratios}} in {{Complete Records Logistic Regression}}},
  author = {Bartlett, Jonathan W. and Harel, Ofer and Carpenter, James R.},
  year = {2015},
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {182},
  number = {8},
  pages = {730--736},
  issn = {1476-6256},
  doi = {10.1093/aje/kwv114},
  abstract = {Missing data are a commonly occurring threat to the validity and efficiency of epidemiologic studies. Perhaps the most common approach to handling missing data is to simply drop those records with 1 or more missing values, in so-called "complete records" or "complete case" analysis. In this paper, we bring together earlier-derived yet perhaps now somewhat neglected results which show that a logistic regression complete records analysis can provide asymptotically unbiased estimates of the association of an exposure of interest with an outcome, adjusted for a number of confounders, under a surprisingly wide range of missing-data assumptions. We give detailed guidance describing how the observed data can be used to judge the plausibility of these assumptions. The results mean that in large epidemiologic studies which are affected by missing data and analyzed by logistic regression, exposure associations may be estimated without bias in a number of settings where researchers might otherwise assume that bias would occur.},
  langid = {english},
  pmcid = {PMC4597800},
  pmid = {26429998},
  keywords = {Aviation,Bias,Cohort Studies,complete case analysis,Data Interpretation; Statistical,Guidelines as Topic,Humans,Logistic Models,logistic regression,Medical Records Systems; Computerized,missing data,Mortality,Occupational Exposure,odds ratio,Odds Ratio,Read,United Kingdom,Workforce},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Bartlett et al2015\\Bartlett et al_2015_Asymptotically Unbiased Estimation of Exposure Odds Ratios in Complete Records.pdf}
}

@article{berkWhatYouCan2010,
  title = {What {{You Can}} and {{Can}}'t {{Properly Do}} with {{Regression}}},
  author = {Berk, Richard},
  year = {2010},
  month = dec,
  journal = {Journal of Quantitative Criminology},
  volume = {26},
  number = {4},
  pages = {481--487},
  issn = {0748-4518, 1573-7799},
  doi = {10.1007/s10940-010-9116-4},
  langid = {english},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Berk2010\\Berk_2010_What You Can and Canâ€™t Properly Do with Regression.pdf}
}

@article{bodnerWhatImprovesIncreased2008,
  title = {What {{Improves}} with {{Increased Missing Data Imputations}}?},
  author = {Bodner, Todd E.},
  year = {2008},
  month = oct,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {15},
  number = {4},
  pages = {651--675},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10.1080/10705510802339072},
  abstract = {When using multiple imputation in the analysis of incomplete data, a prominent guideline suggests that more than 10 imputed data values are seldom needed. This article calls into question the optimism of this guideline and illustrates that important quantities (e.g., p values, confidence interval half-widths, and estimated fractions of missing information) suffer from substantial imprecision with a small number of imputations. Substantively, a researcher can draw categorically different conclusions about null hypothesis rejection, estimation precision, and missing information in distinct multiple imputation runs for the same data and analysis with few imputations. This article explores the factors associated with this imprecision, demonstrates that precision improves by increasing the number of imputations, and provides practical guidelines for choosing a reasonable number of imputations to reduce imprecision for each of these quantities.},
  keywords = {Unread},
  annotation = {\_eprint: https://doi.org/10.1080/10705510802339072},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Bodner2008\\Bodner_2008_What Improves with Increased Missing Data Imputations.pdf}
}

@article{carpenterSensitivityAnalysisMultiple2007,
  title = {Sensitivity Analysis after Multiple Imputation under Missing at Random: A                 Weighting Approach},
  shorttitle = {Sensitivity Analysis after Multiple Imputation under Missing at Random},
  author = {Carpenter, James R. and Kenward, Michael G. and White, Ian R.},
  year = {2007},
  month = jun,
  journal = {Statistical Methods in Medical Research},
  volume = {16},
  number = {3},
  pages = {259--275},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  doi = {10.1177/0962280206075303},
  abstract = {Multiple imputation (MI) is now well established as a flexible, general, method for the analysis of data sets with missing values. Most implementations assume the missing data are `missing at random' (MAR), that is, given the observed data, the reason for the missing data does not depend on the unseen data. However, although this is a helpful and simplifying working assumption, it is unlikely to be true in practice. Assessing the sensitivity of the analysis to the MAR assumption is therefore important. However, there is very limited MI software for this. Further, analysis of a data set with missing values that are not missing at random (NMAR) is complicated by the need to extend the MAR imputation model to include a model for the reason for dropout. Here, we propose a simple alternative. We first impute under MAR and obtain parameter estimates for each imputed data set. The overall NMAR parameter estimate is a weighted average of these parameter estimates, where the weights depend on the assumed degree of departure from MAR. In some settings, this approach gives results that closely agree with joint modelling as the number of imputations increases. In others, it provides ball-park estimates of the results of full NMAR modelling, indicating the extent to which it is necessary and providing a check on its results. We illustrate our approach with a small simulation study, and the analysis of data from a trial of interventions to improve the quality of peer review.},
  langid = {english},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Carpenter et al2007\\Carpenter et al_2007_Sensitivity analysis after multiple imputation under missing at random.pdf}
}

@article{cassidyDoesSentenceType2020,
  title = {Does {{Sentence Type}} and {{Length Matter}}?: {{Interactions}} of {{Age}}, {{Race}}, {{Ethnicity}}, and {{Gender}} on {{Jail}} and {{Prison Sentences}}},
  shorttitle = {Does {{Sentence Type}} and {{Length Matter}}?},
  author = {Cassidy, Michael and Rydberg, Jason},
  year = {2020},
  journal = {Criminal Justice and Behavior},
  volume = {47},
  pages = {61},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Cassidy_Rydberg2020\\Cassidy_Rydberg_2020_Does Sentence Type and Length Matter.pdf;C\:\\Users\\stocb\\Zotero\\storage\\YCB3YKIY\\LandingPage.html}
}

@article{croSensitivityAnalysisClinical2020a,
  title = {Sensitivity Analysis for Clinical Trials with Missing Continuous Outcome Data Using Controlled Multiple Imputation: {{A}} Practical Guide},
  shorttitle = {Sensitivity Analysis for Clinical Trials with Missing Continuous Outcome Data Using Controlled Multiple Imputation},
  author = {Cro, Suzie and Morris, Tim P. and Kenward, Michael G. and Carpenter, James R.},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  number = {21},
  pages = {2815--2842},
  issn = {1097-0258},
  doi = {10.1002/sim.8569},
  abstract = {Missing data due to loss to follow-up or intercurrent events are unintended, but unfortunately inevitable in clinical trials. Since the true values of missing data are never known, it is necessary to assess the impact of untestable and unavoidable assumptions about any unobserved data in sensitivity analysis. This tutorial provides an overview of controlled multiple imputation (MI) techniques and a practical guide to their use for sensitivity analysis of trials with missing continuous outcome data. These include {$\delta$}- and reference-based MI procedures. In {$\delta$}-based imputation, an offset term, {$\delta$}, is typically added to the expected value of the missing data to assess the impact of unobserved participants having a worse or better response than those observed. Reference-based imputation draws imputed values with some reference to observed data in other groups of the trial, typically in other treatment arms. We illustrate the accessibility of these methods using data from a pediatric eczema trial and a chronic headache trial and provide Stata code to facilitate adoption. We discuss issues surrounding the choice of {$\delta$} in {$\delta$}-based sensitivity analysis. We also review the debate on variance estimation within reference-based analysis and justify the use of Rubin's variance estimator in this setting, since as we further elaborate on within, it provides information anchored inference.},
  langid = {english},
  keywords = {clinical trials,controlled multiple imputation,missing data,multiple imputation,sensitivity analysis,Unread},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8569},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Cro et al2020\\Cro et al_2020_Sensitivity analysis for clinical trials with missing continuous outcome data.pdf;C\:\\Users\\stocb\\Zotero\\storage\\2TA584B6\\sim.html}
}

@article{dempsterMaximumLikelihoodIncomplete1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {39},
  number = {1},
  pages = {1--38},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Dempster et al1977\\Dempster et al_1977_Maximum Likelihood from Incomplete Data via the EM Algorithm.pdf}
}

@article{glynnMultipleImputationMixture1993,
  title = {Multiple {{Imputation}} in {{Mixture Models}} for {{Nonignorable Nonresponse With Follow-ups}}},
  author = {Glynn, Robert J. and Laird, Nan M. and Rubin, Donald B.},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {423},
  pages = {984--993},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290790},
  abstract = {One approach to inference for means or linear regression parameters when the outcome is subject to nonignorable nonresponse is mixture modeling. Mixture models assume separate parameters for respondents and nonrespondents; implementation by multiple imputation consists of repeatedly filling in missing values for nonrespondents, estimating parameters using the filled-in data, and then adjusting for variability between imputations. We evaluated the performance of this scheme using simulated data with a 25\% sample of nonrespondents followed up. We conclude that it provides a generally satisfactory and robust approach to inference for means and regression parameters in this case, although a greater number of imputations may be required for good performance compared to the number required for estimation when nonresponse is ignorable.},
  keywords = {pattern mixture,Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Glynn et al1993\\Glynn et al_1993_Multiple Imputation in Mixture Models for Nonignorable Nonresponse With.pdf}
}

@article{grahamHowManyImputations2007,
  title = {How {{Many Imputations}} Are {{Really Needed}}? {{Some Practical Clarifications}} of {{Multiple Imputation Theory}}},
  shorttitle = {How {{Many Imputations}} Are {{Really Needed}}?},
  author = {Graham, John W. and Olchowski, Allison E. and Gilreath, Tamika D.},
  year = {2007},
  month = sep,
  journal = {Prevention Science},
  volume = {8},
  number = {3},
  pages = {206--213},
  issn = {1573-6695},
  doi = {10.1007/s11121-007-0070-9},
  abstract = {Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information ({$\gamma$}) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which {$\gamma$} and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on {$\gamma$}, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.},
  langid = {english},
  keywords = {Full information maximum likelihood,Missing data,Multiple imputation,Number of imputations,Read,Statistical power},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Graham et al2007\\Graham et al_2007_How Many Imputations are Really Needed.pdf}
}

@article{harelInferencesMissingInformation2007,
  title = {Inferences on Missing Information under Multiple Imputation and Two-Stage Multiple Imputation},
  author = {Harel, Ofer},
  year = {2007},
  month = jan,
  journal = {Statistical Methodology},
  volume = {4},
  number = {1},
  pages = {75--89},
  issn = {15723127},
  doi = {10.1016/j.stamet.2006.03.002},
  abstract = {In the presence of missing values, researchers may be interested in the rates of missing information. The rates of missing information are (a) important for assessing how the missing information contributes to inferential uncertainty about, Q, the population quantity of interest, (b) are an important component in the decision of the number of imputations, and (c) can be used to test model uncertainty and model fitting. In this article I will derive the asymptotic distribution of the rates of missing information in two scenarios: the conventional multiple imputation (MI), and the two-stage MI. Numerically I will show that the proposed asymptotic distribution agrees with the simulated one. I will also suggest the number of imputations needed to obtain reliable missing information rate estimates for each method, based on the asymptotic distribution. c 2006 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {Read},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Literature_Review_Notes\\harelInferencesMissingInformation2007.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\Harel2007\\Harel_2007_Inferences on missing information under multiple imputation and two-stage.pdf}
}

@article{harelMultipleImputationIncomplete2018,
  title = {Multiple {{Imputation}} for {{Incomplete Data}} in {{Epidemiologic Studies}}},
  author = {Harel, Ofer and Mitchell, Emily M and Perkins, Neil J and Cole, Stephen R and Tchetgen Tchetgen, Eric J and Sun, BaoLuo and Schisterman, Enrique F},
  year = {2018},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {187},
  number = {3},
  pages = {576--584},
  issn = {0002-9262},
  doi = {10.1093/aje/kwx349},
  abstract = {Epidemiologic studies are frequently susceptible to missing information. Omitting observations with missing variables remains a common strategy in epidemiologic studies, yet this simple approach can often severely bias parameter estimates of interest if the values are not missing completely at random. Even when missingness is completely random, complete-case analysis can reduce the efficiency of estimated parameters, because large amounts of available data are simply tossed out with the incomplete observations. Alternative methods for mitigating the influence of missing information, such as multiple imputation, are becoming an increasing popular strategy in order to retain all available information, reduce potential bias, and improve efficiency in parameter estimation. In this paper, we describe the theoretical underpinnings of multiple imputation, and we illustrate application of this method as part of a collaborative challenge to assess the performance of various techniques for dealing with missing data (Am J Epidemiol. 2018;187(3):568\textendash 575). We detail the steps necessary to perform multiple imputation on a subset of data from the Collaborative Perinatal Project (1959\textendash 1974), where the goal is to estimate the odds of spontaneous abortion associated with smoking during pregnancy.},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Harel et al2018\\Harel et al_2018_Multiple Imputation for Incomplete Data in Epidemiologic Studies.pdf;C\:\\Users\\stocb\\Zotero\\storage\\J5U58K2L\\4642952.html}
}

@article{harelMultipleImputationReview2007,
  title = {Multiple Imputation: Review of Theory, Implementation and Software},
  shorttitle = {Multiple Imputation},
  author = {Harel, Ofer and Zhou, Xiao-Hua},
  year = {2007},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {26},
  number = {16},
  pages = {3057--3077},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.2787},
  abstract = {Missing data is a common complication in data analysis. In many medical settings missing data can cause difficulties in estimation, precision and inference. Multiple imputation (MI) (Multiple Imputation for Nonresponse in Surveys. Wiley: New York, 1987) is a simulation-based approach to deal with incomplete data. Although there are many different methods to deal with incomplete data, MI has become one of the leading methods. Since the late 1980s we observed a constant increase in the use and publication of MI-related research. This tutorial does not attempt to cover all the material concerning MI, but rather provides an overview and combines together the theory behind MI, the implementation of MI, and discusses increasing possibilities of the use of MI using commercial and free software. We illustrate some of the major points using an example from an Alzheimer disease (AD) study. In this AD study, while clinical data are available for all subjects, postmortem data are only available for the subset of those who died and underwent an autopsy. Analysis of incomplete data requires making unverifiable assumptions. These assumptions are discussed in detail in the text. Relevant S-Plus code is provided. Copyright q 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Read},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\SPL3FK4W\\Harel and Zhou - 2007 - Multiple imputation review of theory, implementat.pdf}
}

@article{hortonMultipleImputationPractice2001,
  title = {Multiple {{Imputation}} in {{Practice}}},
  author = {Horton, Nicholas J and Lipsitz, Stuart R},
  year = {2001},
  month = aug,
  journal = {The American Statistician},
  volume = {55},
  number = {3},
  pages = {244--254},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/000313001317098266},
  abstract = {Missing data frequently complicates data analysis for scientific investigations. The development of statistical methods to address missing data has been an active area of research in recent decades. Multiple imputation, originally proposed by Rubin in a public use dataset setting, is a general purpose method for analyzing datasets with missing data that is broadly applicable to a variety of missing data settings. We review multiple imputation as an analytic strategy formissing data. Wedescribe and evaluate a number of software packages that implement this procedure, and contrast the interface, features, and results. We compare the packages, and detail shortcomings and useful features. The comparisons are illustrated using examples from an artificial dataset and a study of child psychopathology. We suggest additional features as well as discuss limitations and cautions to consider when using multiple imputation as an analytic strategy for incomplete data settings.},
  keywords = {Generalized linear models,Incomplete data,Markov Chain Monte Carlo,Missing outcomes,Missing predictors,Unread},
  annotation = {\_eprint: https://doi.org/10.1198/000313001317098266},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Horton_Lipsitz2001\\Horton_Lipsitz_2001_Multiple Imputation in Practice.pdf}
}

@article{ibrahimIncompleteDataGeneralized1990,
  title = {Incomplete {{Data}} in {{Generalized Linear Models}}},
  author = {Ibrahim, Joseph G.},
  year = {1990},
  journal = {Journal of the American Statistical Association},
  volume = {85},
  number = {411},
  pages = {765--769},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290013},
  abstract = {This article examines incomplete data for the class of generalized linear models, in which incompleteness is due to partially missing covariates on some observations. Under the assumption that the missing data are missing at random, it is shown that the E step of the EM algorithm for any generalized linear model can be expressed as a weighted complete data log-likelihood when the unobserved covariates are assumed to come from a discrete distribution with finite range. Expressing the E step in this manner allows for a straightforward maximization in the M step, thus leading to maximum likelihood estimates (MLE's) for the parameters. Asymptotic variances of the MLE's are also derived, and results are illustrated with two examples.},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Ibrahim1990\\Ibrahim_1990_Incomplete Data in Generalized Linear Models.pdf}
}

@article{ibrahimMissingDataClinical2012,
  title = {Missing {{Data}} in {{Clinical Studies}}: {{Issues}} and {{Methods}}},
  shorttitle = {Missing {{Data}} in {{Clinical Studies}}},
  author = {Ibrahim, Joseph G. and Chu, Haitao and Chen, Ming-Hui},
  year = {2012},
  month = sep,
  journal = {Journal of Clinical Oncology},
  volume = {30},
  number = {26},
  pages = {3297--3303},
  issn = {0732-183X},
  doi = {10.1200/JCO.2011.38.7589},
  abstract = {Missing data are a prevailing problem in any type of data analyses. A participant variable is considered missing if the value of the variable (outcome or covariate) for the participant is not observed. In this article, various issues in analyzing studies with missing data are discussed. Particularly, we focus on missing response and/or covariate data for studies with discrete, continuous, or time-to-event end points in which generalized linear models, models for longitudinal data such as generalized linear mixed effects models, or Cox regression models are used. We discuss various classifications of missing data that may arise in a study and demonstrate in several situations that the commonly used method of throwing out all participants with any missing data may lead to incorrect results and conclusions. The methods described are applied to data from an Eastern Cooperative Oncology Group phase II clinical trial of liver cancer and a phase III clinical trial of advanced non\textendash small-cell lung cancer. Although the main area of application discussed here is cancer, the issues and methods we discuss apply to any type of study.},
  pmcid = {PMC3948388},
  pmid = {22649133},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Ibrahim et al2012\\Ibrahim et al_2012_Missing Data in Clinical Studies.pdf}
}

@article{kenwardControlledMultipleImputation2015,
  title = {Controlled Multiple Imputation Methods for Sensitivity Analyses in Longitudinal Clinical Trials with Dropout and Protocol Deviation.},
  author = {Kenward, M. G.},
  year = {2015},
  journal = {Clinical Investigation},
  volume = {5},
  number = {3},
  pages = {311--320},
  publisher = {{Future Science Ltd}},
  issn = {2041-6792},
  abstract = {Sensitivity analyses are commonly requested as part of the analysis of longitudinal clinical trials when data are missing. There are many ways in which such sensitivity analyses can be constructed. This article focuses on one particular approach, so-called controlled imputation. This combines two statistical ingredients, pattern-mixture models and multiple imputation. The aim is to assess...},
  langid = {english},
  keywords = {Sensitivity Analysis,Unread},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\H8DE9MBL\\20153149187.html}
}

@article{kingHaveRacialEthnic2019,
  title = {Have {{Racial}} and {{Ethnic Disparities}} in {{Sentencing Declined}}?},
  author = {King, Ryan D. and Light, Michael T.},
  year = {2019},
  month = may,
  journal = {Crime and Justice},
  volume = {48},
  pages = {365--437},
  publisher = {{The University of Chicago Press}},
  issn = {0192-3234},
  doi = {10.1086/701505},
  abstract = {Blacks and Hispanics convicted of felonies are more likely than whites to receive prison sentences for their crimes, and they receive slightly longer sentences if imprisoned. Yet the majority of prior research compares sentencing decisions at a single point in time and does not give explicit attention to whether and how racial and ethnic disparities have changed. Decades of sentencing data from Minnesota, the federal courts, and a sample of large urban counties are used to assess the degree of change in racial and ethnic sentencing disparities since the 1980s. There has been some decline in the magnitude of racial and ethnic disparities, with changes in drug laws aligning with some of the reduction in disparity at the federal level. This trend, along with the pattern of findings from related studies, poses a challenge to prominent theoretical explanations of sentencing disparities, including racial threat theory and the focal concerns perspective. Each of four influential theoretical explanations of racial and ethnic disparities in sentencing includes significant empirical or logical shortcomings. Advancing theoretical understanding of racial and ethnic disparity will require new data that follow cases from the point of arrest through to final disposition and include information about citizenship and victims.},
  keywords = {Unread},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/kingHaveRacialEthnic2019.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\King_Light2019\\King_Light_2019_Have Racial and Ethnic Disparities in Sentencing Declined.pdf;G\:\\My Drive\\Zotero\\Research\\Criminology\\King_Light2019\\King_Light_2019_Have Racial and Ethnic Disparities in Sentencing Declined2.pdf}
}

@article{kutateladzeCumulativeDisadvantageExamining2014,
  title = {Cumulative {{Disadvantage}}: {{Examining Racial}} and {{Ethnic Disparity}} in {{Prosecution}} and {{Sentencing}}},
  shorttitle = {Cumulative {{Disadvantage}}},
  author = {Kutateladze, Besiki L. and Andiloro, Nancy R. and Johnson, Brian D. and Spohn, Cassia C.},
  year = {2014},
  journal = {Criminology},
  volume = {52},
  number = {3},
  pages = {514--551},
  issn = {1745-9125},
  doi = {10.1111/1745-9125.12047},
  abstract = {Current research on criminal case processing typically examines a single decision-making point, so drawing reliable conclusions about the impact that factors such as defendants' race or ethnicity exert across successive stages of the justice system is difficult. Using data from the New York County District Attorney's Office that tracks 185,275 diverse criminal cases, this study assesses racial and ethnic disparity for multiple discretionary points of prosecution and sentencing. Findings from multivariate logistic regression analyses demonstrate that the effects of race and ethnicity vary by discretionary point and offense category. Black and Latino defendants were more likely than White defendants to be detained, to receive a custodial plea offer, and to be incarcerated\textemdash and they received especially punitive outcomes for person offenses\textemdash but were more likely to benefit from case dismissals. The findings for Asian defendants were less consistent but suggest they were the least likely to be detained, to receive custodial offers, and to be incarcerated. These findings are discussed in the context of contemporary theoretical perspectives on racial bias and cumulative disadvantage in the justice system.},
  langid = {english},
  keywords = {cumulative disadvantage,prosecution,punitiveness,race/ethnicity,sentencing,Unread},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1745-9125.12047},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/kutateladzeCumulativeDisadvantageExamining2014.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\Kutateladze et al2014\\Kutateladze et al_2014_Cumulative Disadvantage.pdf;G\:\\My Drive\\Zotero\\Research\\Criminology\\Kutateladze et al2014\\Kutateladze et al_2014_Cumulative Disadvantage2.pdf;C\:\\Users\\stocb\\Zotero\\storage\\NNIWV6PW\\1745-9125.html}
}

@article{lehmannRaceEthnicityCrime2020,
  title = {Race, {{Ethnicity}}, {{Crime Type}}, and the {{Sentencing}} of {{Violent Felony Offenders}}},
  author = {Lehmann, Peter S.},
  year = {2020},
  month = jun,
  journal = {Crime \& Delinquency},
  volume = {66},
  number = {6-7},
  pages = {770--805},
  issn = {0011-1287, 1552-387X},
  doi = {10.1177/0011128720902699},
  abstract = {Within the large body of literature on racial/ethnic disparities in criminal sentencing, some research has demonstrated that these relationships are conditional upon various legally relevant case characteristics, including the type of offense for which the defendants are sentenced. To date, however, few studies have explored the potential moderating effects of different violent crimes. Using data from Florida (N = 186,885), the findings from these analyses indicate that Black\textendash White sentencing disparities are particularly pronounced for manslaughter, robbery/carjacking, arson, and resisting arrest with violence. While Hispanic ethnicity exerts limited effects on sentencing outcomes generally, Hispanics are particularly disadvantaged in manslaughter cases. Relative to minority defendants, White offenders receive harsher sentences for sexual battery, other sex offenses, and abuse of children.},
  langid = {english},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Lehmann2020\\Lehmann_2020_Race, Ethnicity, Crime Type, and the Sentencing of Violent Felony Offenders.pdf;G\:\\My Drive\\Zotero\\Research\\Criminology\\Lehmann2020\\Lehmann_2020_Race, Ethnicity, Crime Type, and the Sentencing of Violent Felony Offenders2.pdf}
}

@article{littlePatternMixtureModelsMultivariate1993,
  title = {Pattern-{{Mixture Models}} for {{Multivariate Incomplete Data}}},
  author = {Little, Roderick J. A.},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {421},
  pages = {125--134},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290705},
  abstract = {Consider a random sample on variables X1, ..., XV with some values of XV missing. Selection models specify the distribution of X1, ..., XV over respondents and nonrespondents to XV, and the conditional distribution that XV is missing given X1, ..., XV. In contrast, pattern-mixture models specify the conditional distribution of X1, ..., XV given that XV is observed or missing respectively and the marginal distribution of the binary indicator for whether or not XV is missing. For multivariate data with a general pattern of missing values, the literature has tended to adopt the selection-modeling approach (see for example Little and Rubin); here, pattern-mixture models are proposed for this more general problem. Pattern-mixture models are chronically underidentified; in particular for the case of univariate nonresponse mentioned above, there are no data on the distribution of XV given X1, ..., XV-1 in the stratum with XV missing. Thus the models require restrictions or prior information to identify the parameters. Complete-case restrictions tie unidentified parameters to their (identified) analogs in the stratum of complete cases. Alternative types of restriction tie unidentified parameters to parameters in other missing-value patterns or sets of such patterns. This large set of possible identifying restrictions yields a rich class of missing-data models. Unlike ignorable selection models, which generally requires iterative methods except for special missing-data patterns, some pattern-mixture models yield explicit ML estimates for general patterns. Such models are readily amenable to Bayesian methods and form a convenient basis for multiple imputation. Some previously considered noniterative estimation methods are shown to be maximum likelihood (ML) under a pattern-mixture model. For example, Buck's method for continuous data, corrected as in Beale and Little (1975), and Brown's estimators for nonrandomly missing data are ML for pattern-mixture models with particular complete-case restrictions. Available-case analyses, where the mean and variance of Xj are computed using all cases with Xj observed and the correlation (or covariance) of Xj and Xk is computed using all cases with Xj and Xk observed, are also close to ML for another pattern-mixture model. Asymptotic theory for this class of estimators is outlined.},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/littlePatternMixtureModelsMultivariate1993.md;C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Notes\\Literature_Review_Notes\\littlePatternMixtureModelsMultivariate1993.md;G\:\\My Drive\\Zotero\\Research\\Missing Data\\Little1993\\Little_1993_Pattern-Mixture Models for Multivariate Incomplete Data.pdf}
}

@article{littlePatternMixtureModelsMultivariate1996,
  title = {Pattern-{{Mixture Models}} for {{Multivariate Incomplete Data}} with {{Covariates}}},
  author = {Little, Roderick J. A. and Wang, Yongxiao},
  year = {1996},
  journal = {Biometrics},
  volume = {52},
  number = {1},
  pages = {98--111},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2533148},
  abstract = {Pattern-mixture models stratify incomplete data by the pattern of missing values and formulate distinct models within each stratum. Pattern-mixture models are developed for analyzing a random sample on continuous variables y(1), y(2) when values of y(2) are nonrandomly missing. Methods for scalar y(1) and y(2) are here generalized to vector y(1) and y(2) with additional fixed covariates x. Parameters in these models are identified by alternative assumptions about the missing-data mechanism. Models may be underidentified (in which case additional assumptions are needed), just-identified, or overidentified. Maximum likelihood and Bayesian methods are developed for the latter two situations, using the EM and SEM algorithms, direct and iterative simulation methods. The methods are illustrated on a data set involving alternative dosage regimens for the treatment of schizophrenia using haloperidol and on a regression example. Sensitivity to alternative assumptions about the missing-data mechanism is assessed, and the new methods are compared with complete-case analysis and maximum likelihood for a probit selection model.},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Little_Wang1996\\Little_Wang_1996_Pattern-Mixture Models for Multivariate Incomplete Data with Covariates.pdf}
}

@article{littleRegressionMissingReview1992,
  title = {Regression with {{Missing X}}'s: {{A Review}}},
  shorttitle = {Regression with {{Missing X}}'s},
  author = {Little, Roderick J. A.},
  year = {1992},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {87},
  number = {420},
  pages = {1227--1237},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1992.10476282},
  abstract = {The literature of regression analysis with missing values of the independent variables is reviewed. Six classes of procedures are distinguished: complete case analysis, available case methods, least squares on imputed data, maximum likelihood, Bayesian methods, and multiple imputation. Methods are compared and illustrated when missing data are confined to one independent variable, and extensions to more general patterns are indicated. Attention is paid to the performance of methods when the missing data are not missing completely at random. Least squares methods that fill in missing X's using only data on the X's are contrasted with likelihood-based methods that use data on the X's and Y. The latter approach is preferred and provides methods for elaboration of the basic normal linear regression model. It is suggested that more widely distributed software is needed that advances beyond complete-case analysis, available-case analysis, and naive imputation methods. Bayesian simulation methods and multiple imputation are reviewed; these provide fruitful avenues for future research.},
  keywords = {Bayesian inference,Imputation,Incomplete data,Multiple imputation,Unread},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.1992.10476282},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Little1992\\Little_1992_Regression with Missing X's.pdf}
}

@article{perkinsPrincipledApproachesMissing2018,
  title = {Principled {{Approaches}} to {{Missing Data}} in {{Epidemiologic Studies}}},
  author = {Perkins, Neil J and Cole, Stephen R and Harel, Ofer and Tchetgen Tchetgen, Eric J and Sun, BaoLuo and Mitchell, Emily M and Schisterman, Enrique F},
  year = {2018},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {187},
  number = {3},
  pages = {568--575},
  issn = {0002-9262},
  doi = {10.1093/aje/kwx348},
  abstract = {Principled methods with which to appropriately analyze missing data have long existed; however, broad implementation of these methods remains challenging. In this and 2 companion papers (Am J Epidemiol. 2018;187(3):576\textendash 584 and Am J Epidemiol. 2018;187(3):585\textendash 591), we discuss issues pertaining to missing data in the epidemiologic literature. We provide details regarding missing-data mechanisms and nomenclature and encourage the conduct of principled analyses through a detailed comparison of multiple imputation and inverse probability weighting. Data from the Collaborative Perinatal Project, a multisite US study conducted from 1959 to 1974, are used to create a masked data-analytical challenge with missing data induced by known mechanisms. We illustrate the deleterious effects of missing data with naive methods and show how principled methods can sometimes mitigate such effects. For example, when data were missing at random, naive methods showed a spurious protective effect of smoking on the risk of spontaneous abortion (odds ratio (OR) = 0.43, 95\% confidence interval (CI): 0.19, 0.93), while implementation of principled methods multiple imputation (OR = 1.30, 95\% CI: 0.95, 1.77) or augmented inverse probability weighting (OR = 1.40, 95\% CI: 1.00, 1.97) provided estimates closer to the ``true'' full-data effect (OR = 1.31, 95\% CI: 1.05, 1.64). We call for greater acknowledgement of and attention to missing data and for the broad use of principled missing-data methods in epidemiologic research.},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/perkinsPrincipledApproachesMissing2018.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\Perkins et al2018\\Perkins et al_2018_Principled Approaches to Missing Data in Epidemiologic Studies.pdf;G\:\\My Drive\\Zotero\\Research\\Criminology\\Perkins et al2018\\Perkins et al_2018_Principled Approaches to Missing Data in Epidemiologic Studies2.pdf;C\:\\Users\\stocb\\Zotero\\storage\\I97Z3RYE\\4642951.html}
}

@article{raghunathanMultivariateTechniqueMultiply2001,
  title = {A {{Multivariate Technique}} for {{Multiply Imputing Missing Values Using}} a {{Sequence}} of {{Regression Models}}},
  author = {Raghunathan, Trivellore E and Lepkowski, James M and Hoewyk, John Van and Solenberger, Peter},
  year = {2001},
  month = jun,
  journal = {Statistics Canada},
  volume = {27},
  number = {1},
  pages = {85--95},
  abstract = {This article describes and evaluates a procedure for imputing missing values for a relatively complex data structure when the data are missing at random. The imputations are obtained by fitting a sequence of regression models and drawing values from the corresponding predictive distributions. The types of regression models used are linear, logistic, Poisson, generalized logit or a mixture of these depending on the type of variable being imputed. Two additional common features in the imputation process are incorporated: restriction to a relevant subpopulation for some variables and logical bounds or constraints for the imputed values. The restrictions involve subsetting the sample individuals that satisfy certain criteria while fitting the regression models. The bounds involve drawing values from a truncated predictive distribution. The development of this method was partly motivated by the analysis of two data sets which are used as illustrations. The sequential regression procedure is applied to perform multiple imputation analysis for the two applied problems. The sampling properties of inferences from multiply imputed data sets created using the sequential regression method are evaluated through simulated data sets.},
  langid = {english},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/raghunathanMultivariateTechniqueMultiply2001.md;C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Notes\\Literature_Review_Notes\\raghunathanMultivariateTechniqueMultiply2001.md;G\:\\My Drive\\UConn\\raghunathanMultivariateTechniqueMultiply2001.md;G\:\\My Drive\\Zotero\\Research\\Missing Data\\Raghunathan et al2001\\Raghunathan et al_2001_A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence.pdf}
}

@article{rubinMatchingRemoveBias1973,
  title = {Matching to {{Remove Bias}} in {{Observational Studies}}},
  author = {Rubin, Donald B.},
  year = {1973},
  journal = {Biometrics},
  volume = {29},
  number = {1},
  pages = {159--183},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529684},
  abstract = {Several matching methods that match all of one sample from another larger sample on a continuous matching variable are compared with respect to their ability to remove the bias of the matching variable. One method is a simple mean-matching method and three are nearest available pair-matching methods. The methods' abilities to remove bias are also compared with the theoretical maximum given fixed distributions and fixed sample sizes. A summary of advice to an investigator is included.},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Rubin1973\\Rubin_1973_Matching to Remove Bias in Observational Studies.pdf}
}

@book{rubinMultipleImputationNonresponse1987,
  title = {{Multiple Imputation for Nonresponse in Surveys | Wiley Series in Probability and Statistics}},
  author = {Rubin, Donald B},
  year = {1987},
  month = jun,
  series = {{Wiley series in probability and mathematical statistics : Applied probability and statistics}},
  publisher = {{Wiley}},
  address = {{New York}},
  abstract = {Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. Clearly illustrates the advantages of modern computing to such handle surveys, and demonstrates the benefit of this statistical technique for researchers who must analyze them. Also presents the background for Bayesian and frequentist theory. After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, the text evaluates procedures in general circumstances, outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. Examples and exercises reinforce ideas, and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.},
  isbn = {978-0-471-08705-2},
  langid = {English (United States), en-US},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\ESHP9QBJ\\9780470316696.html}
}

@book{rubinMultipleImputationNonresponse2004,
  title = {Multiple {{Imputation}} for {{Nonresponse}} in {{Surveys}}},
  author = {Rubin, Donald B.},
  year = {2004},
  series = {Wiley Series in Probability and Mathematical Statistics : {{Applied}} Probability and Statistics},
  edition = {Second},
  publisher = {{John Wiley \& Sons, Incorporated}},
  address = {{Hoboken, UNITED STATES}},
  abstract = {Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. Clearly illustrates the advantages of modern computing to such handle surveys, and demonstrates the benefit of this statistical technique for researchers who must analyze them. Also presents the background for Bayesian and frequentist theory. After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, the text evaluates procedures in general circumstances, outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. Examples and exercises reinforce ideas, and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.},
  isbn = {978-0-470-31736-5},
  keywords = {Multiple imputation (Statistics),Nonresponse (Statistics),Social surveys -- Response rate.},
  file = {/home/bsuconn/Documents/Textbooks/Rubin - 2004 - Multiple Imputation for Nonresponse in Surveys.pdf;C\:\\Users\\stocb\\Zotero\\storage\\A55VXHIR\\detail.html}
}

@article{rubinObjectiveCausalInference2008,
  title = {For Objective Causal Inference, Design Trumps Analysis},
  author = {Rubin, Donald B.},
  year = {2008},
  month = sep,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {3},
  pages = {808--840},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS187},
  abstract = {For obtaining causal inferences that are objective, and therefore have the best chance of revealing scientific truths, carefully designed and executed randomized experiments are generally considered to be the gold standard. Observational studies, in contrast, are generally fraught with problems that compromise any claim for objectivity of the resulting causal inferences. The thesis here is that observational studies have to be carefully designed to approximate randomized experiments, in particular, without examining any final outcome data. Often a candidate data set will have to be rejected as inadequate because of lack of data on key covariates, or because of lack of overlap in the distributions of key covariates between treatment and control groups, often revealed by careful propensity score analyses. Sometimes the template for the approximating randomized experiment will have to be altered, and the use of principal stratification can be helpful in doing this. These issues are discussed and illustrated using the framework of potential outcomes to define causal effects, which greatly clarifies critical issues.},
  keywords = {Average causal effect,causal effects,complier average causal effect,instrumental variables,noncompliance,observational studies,propensity scores,randomized experiments,Rubin Causal Model,Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Rubin2008\\Rubin_2008_For objective causal inference, design trumps analysis.pdf;C\:\\Users\\stocb\\Zotero\\storage\\7GLYCCYT\\08-AOAS187.html}
}

@article{schaferMultipleImputationMultivariate2003,
  title = {Multiple {{Imputation}} in {{Multivariate Problems When}} the {{Imputation}} and {{Analysis Models Differ}}},
  author = {Schafer, Joseph L.},
  year = {2003},
  journal = {Statistica Neerlandica},
  volume = {57},
  number = {1},
  pages = {19--35},
  issn = {1467-9574},
  doi = {10.1111/1467-9574.00218},
  abstract = {Bayesian multiple imputation (MI) has become a highly useful paradigm for handling missing values in many settings. In this paper, I compare Bayesian MI with other methods \textendash{} maximum likelihood, in particular\textemdash and point out some of its unique features. One key aspect of MI, the separation of the imputation phase from the analysis phase, can be advantageous in settings where the models underlying the two phases do not agree.},
  langid = {english},
  keywords = {missing data,nonresponse,Unread},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9574.00218},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Schafer2003\\Schafer_2003_Multiple Imputation in Multivariate Problems When the Imputation and Analysis.pdf;C\:\\Users\\stocb\\Zotero\\storage\\BQAN4CPY\\1467-9574.html}
}

@article{schaferMultipleImputationPrimer1999,
  title = {Multiple Imputation: A Primer},
  shorttitle = {Multiple Imputation},
  author = {Schafer, Joseph L},
  year = {1999},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {8},
  number = {1},
  pages = {3--15},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  doi = {10.1177/096228029900800102},
  abstract = {In recent years, multiple imputation has emerged as a convenient and flexible paradigm for analysing data with missing values. Essential features of multiple imputation are reviewed, with answers to frequently asked questions about using the method in practice.},
  langid = {english},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/schaferMultipleImputationPrimer1999.md;C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Notes\\Literature_Review_Notes\\schaferMultipleImputationPrimer1999.md;G\:\\My Drive\\Zotero\\Research\\Missing Data\\Schafer1999\\Schafer_1999_Multiple imputation.pdf}
}

@article{siddiqueAddressingMissingData2012,
  title = {Addressing {{Missing Data Mechanism Uncertainty}} Using {{Multiple-Model Multiple Imputation}}: {{Application}} to a {{Longitudinal Clinical Trial}}},
  shorttitle = {Addressing {{Missing Data Mechanism Uncertainty}} Using {{Multiple-Model Multiple Imputation}}},
  author = {Siddique, Juned and Harel, Ofer and Crespi, Catherine M.},
  year = {2012},
  month = dec,
  journal = {The annals of applied statistics},
  volume = {6},
  number = {4},
  pages = {1814--1837},
  issn = {1932-6157},
  doi = {10.1214/12-AOAS555},
  abstract = {We present a framework for generating multiple imputations for continuous data when the missing data mechanism is unknown. Imputations are generated from more than one imputation model in order to incorporate uncertainty regarding the missing data mechanism. Parameter estimates based on the different imputation models are combined using rules for nested multiple imputation. Through the use of simulation, we investigate the impact of missing data mechanism uncertainty on post-imputation inferences and show that incorporating this uncertainty can increase the coverage of parameter estimates. We apply our method to a longitudinal clinical trial of low-income women with depression where nonignorably missing data were a concern. We show that different assumptions regarding the missing data mechanism can have a substantial impact on inferences. Our method provides a simple approach for formalizing subjective notions regarding nonresponse so that they can be easily stated, communicated, and compared.},
  pmcid = {PMC3596844},
  pmid = {23503984},
  keywords = {Sensitivity Analysis,Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\Siddique et al2012\\Siddique et al_2012_Addressing Missing Data Mechanism Uncertainty using Multiple-Model Multiple.pdf}
}

@article{steffensmeierIntersectionalityRaceEthnicity2017,
  title = {Intersectionality of {{Race}}, {{Ethnicity}}, {{Gender}}, and {{Age}} on {{Criminal Punishment}}},
  author = {Steffensmeier, Darrell and {Painter-Davis}, Noah and Ulmer, Jeffery},
  year = {2017},
  month = aug,
  journal = {Sociological Perspectives},
  volume = {60},
  number = {4},
  pages = {810--833},
  publisher = {{SAGE Publications Inc}},
  issn = {0731-1214},
  doi = {10.1177/0731121416679371},
  abstract = {Race, ethnicity, gender, and age are core foci within sociology and law/criminology. Also prominent is how these statuses intersect to affect behavioral outcomes, but statistical studies of intersectionality are rare. In the area of criminal sentencing, an abundance of studies examine main and joint effects of race and gender but few investigate in detail how these effects are conditioned by defendant's age. Using recent Pennsylvania sentencing data and a novel method for analyzing statistical interactions, we examine the main and combined effects of these statuses on sentencing. We find strong evidence for intersectionality: Harsher sentences concentrate among young black males and Hispanic males of all ages, while the youngest females (regardless of race/ethnicity) and some older defendants receive leniency. The focal concerns model of sentencing that frames our study has strong affinity with intersectionality perspectives and can serve as a template for research examining the ways social statuses shape inequality.},
  langid = {english},
  keywords = {age,criminal punishment,ethnicity,gender,inequality,intersectionality,race,Read,theory},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/steffensmeierIntersectionalityRaceEthnicity2017.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\Steffensmeier et al2017\\Steffensmeier et al_2017_Intersectionality of Race, Ethnicity, Gender, and Age on Criminal Punishment.pdf;G\:\\My Drive\\Zotero\\Research\\Criminology\\Steffensmeier et al2017\\Steffensmeier et al_2017_Intersectionality of Race, Ethnicity, Gender, and Age on Criminal Punishment2.pdf}
}

@article{tannerCalculationPosteriorDistributions1987,
  title = {The {{Calculation}} of {{Posterior Distributions}} by {{Data Augmentation}}},
  author = {Tanner, Martin A. and Wong, Wing Hung},
  year = {1987},
  journal = {Journal of the American Statistical Association},
  volume = {82},
  number = {398},
  pages = {528--540},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2289457},
  abstract = {The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea is quite simple. The observed data y is augmented by the quantity z, which is referred to as the latent data. It is assumed that if y and z are both known, then the problem is straightforward to analyze, that is, the augmented data posterior p(\texttheta{} {$\mid$} y, z) can be calculated. But the posterior density that we want is p(\texttheta{} {$\mid$} y), which may be difficult to calculate directly. If, however, one can generate multiple values of z from the predictive distribution p(z {$\mid$} y) (i.e., multiple imputations of z), then p(\texttheta{} {$\mid$} y) can be approximately obtained as the average of p(\texttheta{} {$\mid$} y, z) over the imputed z's. However, p(z {$\mid$} y) depends, in turn, on p(\texttheta{} {$\mid$} y). Hence if p(\texttheta{} {$\mid$} y) was known, it could be used to calculate p(z {$\mid$} y). This mutual dependency between p(\texttheta{} {$\mid$} y) and p(z {$\mid$} y) leads to an iterative algorithm to calculate p(\texttheta{} {$\mid$} y). Analytically, this algorithm is essentially the method of successive substitution for solving an operator fixed point equation. We exploit this fact to prove convergence under mild regularity conditions. Typically, to implement the algorithm, one must be able to sample from two distributions, namely p(\texttheta{} {$\mid$} y, z) and p(z {$\mid$} \texttheta, y). In many cases, it is straightforward to sample from either distribution. In general, though, either sampling can be difficult, just as either the E or the M step can be difficult to implement in the EM algorithm. For p(\texttheta{} {$\mid$} y, z) arising from parametric submodels of the multinomial, we develop a primitive but generally applicable way to approximately sample \texttheta. The idea is first to sample from the posterior distribution of the cell probabilities and then to project to the parametric surface that is specified by the submodel, giving more weight to those observations lying closer to the surface. This procedure should cover many of the common models for categorical data. There are several examples given in this article. First, the algorithm is introduced and motivated in the context of a genetic linkage example. Second, we apply this algorithm to an example of inference from incomplete data regarding the correlation coefficient of the bivariate normal distribution. It is seen that the algorithm recovers the bimodal nature of the posterior distribution. Finally, the algorithm is used in the analysis of the traditional latent-class model as applied to data from the General Social Survey.},
  keywords = {bayesian data analysis,data augmentation,missing data,Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Tanner_Wong1987\\Tanner_Wong_1987_The Calculation of Posterior Distributions by Data Augmentation.pdf}
}

@article{ulmerDisproportionalImprisonmentBlack2016,
  title = {Disproportional {{Imprisonment}} of {{Black}} and {{Hispanic Males}}: {{Sentencing Discretion}}, {{Processing Outcomes}}, and {{Policy Structures}}},
  shorttitle = {Disproportional {{Imprisonment}} of {{Black}} and {{Hispanic Males}}},
  author = {Ulmer, Jeffery and {Painter-Davis}, Noah and Tinik, Leigh},
  year = {2016},
  month = jun,
  journal = {Justice Quarterly},
  volume = {33},
  number = {4},
  pages = {642--681},
  publisher = {{Routledge}},
  issn = {0741-8825},
  doi = {10.1080/07418825.2014.958186},
  abstract = {Disproportional incarceration of black and Hispanic men has been the subject of much critical commentary and empirical inquiry. Such disproportionality may be due to greater involvement of minority men in serious crime, to discretionary decisions by local justice officials, or to the differential impact of sentencing policies, such as mandatory minimums or sentencing guidelines, that differentially impact minority men. This study investigated the extent to which the disproportional punishment of black and Hispanic men, and local variation in such disproportionality, can be attributed to unexplained disparities in local sentencing decisions, as opposed to the extent to which such differences are mediated by sentencing policies, or case-processing and extralegal factors. We use 2005\textendash 2009 federal court and Pennsylvania state court data. Our findings suggest, particularly in Federal courts, that most disproportionality is determined by processes prior to sentencing, especially sentencing policies that differentially impact minority males.},
  keywords = {discretion,imprisonment,racial disparity,Read,sentencing},
  annotation = {\_eprint: https://doi.org/10.1080/07418825.2014.958186},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/ulmerDisproportionalImprisonmentBlack2016.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\Ulmer et al2016\\Ulmer et al_2016_Disproportional Imprisonment of Black and Hispanic Males.pdf;C\:\\Users\\stocb\\Zotero\\storage\\7CL7FP76\\07418825.2014.html}
}

@article{vachBiasedEstimationAdjusted1997,
  title = {Biased {{Estimation}} of {{Adjusted Odds Ratios}} from {{Incomplete Covariate Data Due}} to {{Violation}} of the {{Missing}} at {{Random Assumption}}},
  author = {Vach, Werner and Illi, Sabina},
  year = {1997},
  journal = {Biometrical Journal},
  volume = {39},
  number = {1},
  pages = {13--28},
  issn = {03233847, 15214036},
  doi = {10.1002/bimj.4710390103},
  langid = {english},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Vach_Illi1997\\Vach_Illi_1997_Biased Estimation of Adjusted Odds Ratios from Incomplete Covariate Data Due to.pdf}
}

@article{vachLogisticRegressionIncompletely1993,
  title = {Logistic {{Regression}} with {{Incompletely Observed Categorical Covariates}}: {{A Comparison}} of {{Three Approaches}}},
  shorttitle = {Logistic {{Regression}} with {{Incompletely Observed Categorical Covariates}}},
  author = {Vach, Werner and Schumacher, Martin},
  year = {1993},
  journal = {Biometrika},
  volume = {80},
  number = {2},
  pages = {353--362},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2337205},
  abstract = {A logistic regression analysis based on the complete cases neglects the information due to subjects with missing values in at least one but not in all covariates. Three different approaches allowing the use of this information are compared: maximum likelihood estimation, pseudo maximum likelihood estimation, and probability imputation. The first two yield consistent estimates of the regression coefficients and asymptotic normality allows the construction of asymptotically valid confidence intervals. The only necessary assumption is that the probability for the occurrence of missing values does not depend on the true value of the hidden covariates, whereas there may be a dependence on completely observed covariates and on the outcome variable. Their relative efficiency and the gain relative to a complete case analysis is investigated. Comparison with imputation methods shows that imputation of conditional probabilities can be regarded as a close approximation to the pseudo maximum likelihood estimation, whereas imputation of unconditional probabilities is associated with serious bias.},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Vach_Schumacher1993\\Vach_Schumacher_1993_Logistic Regression with Incompletely Observed Categorical Covariates.pdf}
}

@article{vanbuurenMultipleImputationMissing1999,
  title = {Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis},
  author = {{van Buuren}, S. and Boshuizen, H. C. and Knook, D. L.},
  year = {1999},
  journal = {Statistics in Medicine},
  volume = {18},
  number = {6},
  pages = {681--694},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19990330)18:6<681::AID-SIM71>3.0.CO;2-R},
  abstract = {This paper studies a non-response problem in survival analysis where the occurrence of missing data in the risk factor is related to mortality. In a study to determine the influence of blood pressure on survival in the very old (85+ years), blood pressure measurements are missing in about 12{$\cdot$}5 per cent of the sample. The available data suggest that the process that created the missing data depends jointly on survival and the unknown blood pressure, thereby distorting the relation of interest. Multiple imputation is used to impute missing blood pressure and then analyse the data under a variety of non-response models. One special modelling problem is treated in detail; the construction of a predictive model for drawing imputations if the number of variables is large. Risk estimates for these data appear robust to even large departures from the simplest non-response model, and are similar to those derived under deletion of the incomplete records. Copyright \textcopyright{} 1999 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Unread},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819990330\%2918\%3A6\%3C681\%3A\%3AAID-SIM71\%3E3.0.CO\%3B2-R},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\van Buuren et al1999\\van Buuren et al_1999_Multiple imputation of missing blood pressure covariates in survival analysis.pdf;C\:\\Users\\stocb\\Zotero\\storage\\LCPQUDAS\\(SICI)1097-0258(19990330)186681AID-SIM713.0.html}
}

@article{vanderheijdenImputationMissingValues2006,
  title = {Imputation of Missing Values Is Superior to Complete Case Analysis and the Missing-Indicator Method in Multivariable Diagnostic Research: {{A}} Clinical Example},
  shorttitle = {Imputation of Missing Values Is Superior to Complete Case Analysis and the Missing-Indicator Method in Multivariable Diagnostic Research},
  author = {{van der Heijden}, Geert J. M. G. and T. Donders, A. Rogier and Stijnen, Theo and Moons, Karel G. M.},
  year = {2006},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  number = {10},
  pages = {1102--1109},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2006.01.015},
  abstract = {Background and Objectives To illustrate the effects of different methods for handling missing data\textemdash complete case analysis, missing-indicator method, single imputation of unconditional and conditional mean, and multiple imputation (MI)\textemdash in the context of multivariable diagnostic research aiming to identify potential predictors (test results) that independently contribute to the prediction of disease presence or absence. Methods We used data from 398 subjects from a prospective study on the diagnosis of pulmonary embolism. Various diagnostic predictors or tests had (varying percentages of) missing values. Per method of handling these missing values, we fitted a diagnostic prediction model using multivariable logistic regression analysis. Results The receiver operating characteristic curve area for all diagnostic models was above 0.75. The predictors in the final models based on the complete case analysis, and after using the missing-indicator method, were very different compared to the other models. The models based on MI did not differ much from the models derived after using single conditional and unconditional mean imputation. Conclusion In multivariable diagnostic research complete case analysis and the use of the missing-indicator method should be avoided, even when data are missing completely at random. MI methods are known to be superior to single imputation methods. For our example study, the single imputation methods performed equally well, but this was most likely because of the low overall number of missing values.},
  langid = {english},
  keywords = {Bias,Complete case analysis,Indicator method,Missing data,Multiple imputation,Precision,Read,Single imputation},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\van der Heijden et al2006\\van der Heijden et al_2006_Imputation of missing values is superior to complete case analysis and the.pdf;C\:\\Users\\stocb\\Zotero\\storage\\4GYSJUIA\\S0895435606001983.html}
}

@article{vonhippelHowManyImputations2020,
  title = {How {{Many Imputations Do You Need}}? {{A Two-stage Calculation Using}} a {{Quadratic Rule}}},
  shorttitle = {How {{Many Imputations Do You Need}}?},
  author = {{von Hippel}, Paul T.},
  year = {2020},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {49},
  number = {3},
  pages = {699--718},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117747303},
  abstract = {When using multiple imputation, users often want to know how many imputations they need. An old answer is that 2?10 imputations usually suffice, but this recommendation only addresses the efficiency of point estimates. You may need more imputations if, in addition to efficient point estimates, you also want standard error (SE) estimates that would not change (much) if you imputed the data again. For replicable SE estimates, the required number of imputations increases quadratically with the fraction of missing information (not linearly, as previous studies have suggested). I recommend a two-stage procedure in which you conduct a pilot analysis using a small-to-moderate number of imputations, then use the results to calculate the number of imputations that are needed for a final analysis whose SE estimates will have the desired level of replicability. I implement the two-stage procedure using a new SAS macro called \%mi\_combine and a new Stata command called how\_many\_imputations.},
  langid = {english},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\von Hippel2020\\von Hippel_2020_How Many Imputations Do You Need.pdf}
}

@article{vonhippelRegressionMissingYs2007,
  title = {4. {{Regression}} with {{Missing Ys}}: {{An Improved Strategy}} for {{Analyzing Multiply Imputed Data}}},
  shorttitle = {4. {{Regression}} with {{Missing Ys}}},
  author = {{von Hippel}, Paul T.},
  year = {2007},
  month = aug,
  journal = {Sociological Methodology},
  volume = {37},
  number = {1},
  pages = {83--117},
  publisher = {{SAGE Publications Inc}},
  issn = {0081-1750},
  doi = {10.1111/j.1467-9531.2007.00180.x},
  abstract = {When fitting a generalized linear model?such as linear regression, logistic regression, or hierarchical linear modeling?analysts often wonder how to handle missing values of the dependent variable Y. If missing values have been filled in using multiple imputation, the usual advice is to use the imputed Y values in analysis. We show, however, that using imputed Ys can add needless noise to the estimates. Better estimates can usually be obtained using a modified strategy that we call multiple imputation, then deletion (MID). Under MID, all cases are used for imputation but, following imputation, cases with imputed Y values are excluded from the analysis. When there is something wrong with the imputed Y values, MID protects the estimates from the problematic imputations. And when the imputed Y values are acceptable, MID usually offers somewhat more efficient estimates than an ordinary MI strategy.},
  langid = {english},
  keywords = {Unread},
  file = {G\:\\My Drive\\Zotero\\Research\\Missing Data\\von Hippel2007\\von Hippel_2007_4.pdf}
}

@article{westreichBerksonBiasSelection2012,
  title = {Berkson's {{Bias}}, {{Selection Bias}}, and {{Missing Data}}},
  author = {Westreich, Daniel},
  year = {2012},
  journal = {Epidemiology},
  volume = {23},
  number = {1},
  pages = {159--164},
  publisher = {{Lippincott Williams \& Wilkins}},
  issn = {1044-3983},
  abstract = {Although Berkson's bias is widely recognized in the epidemiologic literature, it remains underappreciated as a model of both selection bias and bias due to missing data. Simple causal diagrams and 2 \texttimes{} 2 tables illustrate how Berkson's bias connects to collider bias and selection bias more generally, and show the strong analogies between Berksonian selection bias and bias due to missing data. In some situations, considerations of whether data are missing at random or missing not at random are less important than the causal structure of the missing data process. Although dealing with missing data always relies on strong assumptions about unobserved variables, the intuitions built with simple examples can provide a better understanding of approaches to missing data in real-world situations.},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\Westreich2012\\Westreich_2012_Berkson's Bias, Selection Bias, and Missing Data.pdf}
}

@article{whiteBiasEfficiencyMultiple2010,
  title = {Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values},
  author = {White, Ian R. and Carlin, John B.},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {28},
  pages = {2920--2931},
  issn = {1097-0258},
  doi = {10.1002/sim.3944},
  abstract = {When missing data occur in one or more covariates in a regression model, multiple imputation (MI) is widely advocated as an improvement over complete-case analysis (CC). We use theoretical arguments and simulation studies to compare these methods with MI implemented under a missing at random assumption. When data are missing completely at random, both methods have negligible bias, and MI is more efficient than CC across a wide range of scenarios. For other missing data mechanisms, bias arises in one or both methods. In our simulation setting, CC is biased towards the null when data are missing at random. However, when missingness is independent of the outcome given the covariates, CC has negligible bias and MI is biased away from the null. With more general missing data mechanisms, bias tends to be smaller for MI than for CC. Since MI is not always better than CC for missing covariate problems, the choice of method should take into account what is known about the missing data mechanism in a particular substantive application. Importantly, the choice of method should not be based on comparison of standard errors. We propose new ways to understand empirical differences between MI and CC, which may provide insights into the appropriateness of the assumptions underlying each method, and we propose a new index for assessing the likely gain in precision from MI: the fraction of incomplete cases among the observed values of a covariate (FICO). Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {complete-case analysis,missing covariates,missing data,multiple imputation,Read},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3944},
  file = {G\:\\My Drive\\Zotero\\Research\\Criminology\\White_Carlin2010\\White_Carlin_2010_Bias and efficiency of multiple imputation compared with complete-case analysis.pdf;C\:\\Users\\stocb\\Zotero\\storage\\FBS656A6\\sim.html}
}

@article{whiteMultipleImputationUsing2011,
  title = {Multiple Imputation Using Chained Equations: {{Issues}} and Guidance for Practice},
  shorttitle = {Multiple Imputation Using Chained Equations},
  author = {White, Ian R. and Royston, Patrick and Wood, Angela M.},
  year = {2011},
  journal = {Statistics in Medicine},
  volume = {30},
  number = {4},
  pages = {377--399},
  issn = {1097-0258},
  doi = {10.1002/sim.4067},
  abstract = {Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {fully conditional specification,missing data,multiple imputation,Read},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4067},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Notes\\Literature_Review_Notes\\whiteMultipleImputationUsing2011.md;G\:\\My Drive\\Zotero\\Research\\Criminology\\White et al2011\\White et al_2011_Multiple imputation using chained equations.pdf;C\:\\Users\\stocb\\Zotero\\storage\\42E2UFV5\\sim.html}
}

@article{woodComparisonImputationModelling2005,
  title = {Comparison of Imputation and Modelling Methods in the Analysis of a Physical Activity Trial with Missing Outcomes},
  author = {Wood, Angela M and White, Ian R and Hillsdon, Melvyn and Carpenter, James},
  year = {2005},
  month = feb,
  journal = {International Journal of Epidemiology},
  volume = {34},
  number = {1},
  pages = {89--99},
  issn = {0300-5771},
  doi = {10.1093/ije/dyh297},
  abstract = {Background Longitudinal studies almost always have some individuals with missing outcomes. Inappropriate handling of the missing data in the analysis can result in misleading conclusions. Here we review a wide range of methods to handle missing outcomes in single and repeated measures data and discuss which methods are most appropriate.Methods Using data from a randomized controlled trial to compare two interventions for increasing physical activity, we compare complete-case analysis; ad hoc imputation techniques such as last observation carried forward and worst-case; model-based imputation; longitudinal models with random effects; and recently proposed joint models for repeated measures data and non-ignorable dropout.Results Estimated intervention effects from ad hoc imputation methods vary widely. Standard multiple imputation and longitudinal modelling agree closely, as they should. Modifying the modelling method to allow for non-ignorable dropout had little effect on estimated intervention effects, but imputing using a common imputation model in both groups gave more conservative results.Conclusions Results from ad hoc imputation methods should be avoided in favour of methods with more plausible assumptions although they may be computationally more complex. Although standard multiple imputation methods and longitudinal modelling methods are equivalent for estimating the treatment effect, the two approaches suggest different ways of relaxing the assumptions, and the choice between them depends on contextual knowledge.},
  keywords = {Read},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Notes\\Literature_Review_Notes\\woodComparisonImputationModelling2005.md;G\:\\My Drive\\Zotero\\Research\\Missing Data\\Wood et al2005\\Wood et al_2005_Comparison of imputation and modelling methods in the analysis of a physical.pdf;C\:\\Users\\stocb\\Zotero\\storage\\FNVFB9MC\\638496.html}
}
