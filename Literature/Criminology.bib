
@article{azurMultipleImputationChained2011,
  title = {Multiple Imputation by Chained Equations: What Is It and How Does It Work?},
  shorttitle = {Multiple Imputation by Chained Equations},
  author = {Azur, Melissa J. and Stuart, Elizabeth A. and Frangakis, Constantine and Leaf, Philip J.},
  year = {2011},
  month = feb,
  journal = {International Journal of Methods in Psychiatric Research},
  volume = {20},
  number = {1},
  pages = {40--49},
  issn = {1049-8931},
  doi = {10.1002/mpr.329},
  abstract = {Multivariate imputation by chained equations (MICE) has emerged as a principled method of dealing with missing data. Despite properties that make MICE particularly useful for large imputation procedures and advances in software development that now make it accessible to many researchers, many psychiatric researchers have not been trained in these methods and few practical resources exist to guide researchers in the implementation of this technique. This paper provides an introduction to the MICE method with a focus on practical aspects and challenges in using this method. A brief review of software programs available to implement MICE and then analyze multiply imputed data is also provided. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  pmcid = {PMC3074241},
  pmid = {21499542},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/azurMultipleImputationChained2011.md;G\:\\My Drive\\Zotero\\Research\\Azur et al\\Multiple imputation by chained equations_Azur et al_2011.pdf}
}

@article{balesRacialEthnicDifferentials2012,
  title = {Racial/{{Ethnic Differentials}} in {{Sentencing}} to {{Incarceration}}},
  author = {Bales, William D. and Piquero, Alex R.},
  year = {2012},
  month = oct,
  journal = {Justice Quarterly},
  volume = {29},
  number = {5},
  pages = {742--773},
  publisher = {{Routledge}},
  issn = {0741-8825},
  doi = {10.1080/07418825.2012.659674},
  abstract = {Few criminological topics are as controversial as the relationships between race, ethnicity, crime, and criminal justice outcomes\textemdash especially incarceration. This paper assesses whether Blacks and Hispanics are disadvantaged at the sentencing phase of the justice system and whether the findings depend on the use of traditional regression-based methods to control for legally relevant variables vs. the use of precision matching methods, which attend to potential sample selection bias that occurs when there are not exact matches for those sentenced to incarceration and non-incarceration. Analysis of the population of Florida offenders from 1994 to 2006 using both methodologies indicates that Black offenders continue to be disproportionately incarcerated compared to White or Hispanic offenders, and that Hispanic offenders were slightly more likely than White offenders to be incarcerated.},
  keywords = {disparity,incarceration,minorities,over-representation,precision matching},
  annotation = {\_eprint: https://doi.org/10.1080/07418825.2012.659674},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/balesRacialEthnicDifferentials2012.md;G\:\\My Drive\\Zotero\\Criminology\\Bales_Piquero\\Racial-Ethnic Differentials in Sentencing to Incarceration_Bales_Piquero_2012.pdf;C\:\\Users\\stocb\\Zotero\\storage\\URGMJABA\\07418825.2012.html}
}

@article{bartlettAsymptoticallyUnbiasedEstimation2015,
  title = {Asymptotically {{Unbiased Estimation}} of {{Exposure Odds Ratios}} in {{Complete Records Logistic Regression}}},
  author = {Bartlett, Jonathan W. and Harel, Ofer and Carpenter, James R.},
  year = {2015},
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {182},
  number = {8},
  pages = {730--736},
  issn = {1476-6256},
  doi = {10.1093/aje/kwv114},
  abstract = {Missing data are a commonly occurring threat to the validity and efficiency of epidemiologic studies. Perhaps the most common approach to handling missing data is to simply drop those records with 1 or more missing values, in so-called "complete records" or "complete case" analysis. In this paper, we bring together earlier-derived yet perhaps now somewhat neglected results which show that a logistic regression complete records analysis can provide asymptotically unbiased estimates of the association of an exposure of interest with an outcome, adjusted for a number of confounders, under a surprisingly wide range of missing-data assumptions. We give detailed guidance describing how the observed data can be used to judge the plausibility of these assumptions. The results mean that in large epidemiologic studies which are affected by missing data and analyzed by logistic regression, exposure associations may be estimated without bias in a number of settings where researchers might otherwise assume that bias would occur.},
  langid = {english},
  pmcid = {PMC4597800},
  pmid = {26429998},
  keywords = {Aviation,Bias,Cohort Studies,complete case analysis,Data Interpretation; Statistical,Guidelines as Topic,Humans,Logistic Models,logistic regression,Medical Records Systems; Computerized,missing data,Mortality,Occupational Exposure,odds ratio,Odds Ratio,United Kingdom,Workforce},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Bartlett et al\\Asymptotically Unbiased Estimation of Exposure Odds Ratios in Complete Records_Bartlett et al_2015.pdf}
}

@article{berkWhatYouCan2010,
  title = {What {{You Can}} and {{Can}}'t {{Properly Do}} with {{Regression}}},
  author = {Berk, Richard},
  year = {2010},
  month = dec,
  journal = {Journal of Quantitative Criminology},
  volume = {26},
  number = {4},
  pages = {481--487},
  issn = {0748-4518, 1573-7799},
  doi = {10.1007/s10940-010-9116-4},
  langid = {english},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Berk\\What You Can and Can’t Properly Do with Regression_Berk_2010.pdf}
}

@article{cassidyDoesSentenceType2020,
  title = {Does {{Sentence Type}} and {{Length Matter}}?: {{Interactions}} of {{Age}}, {{Race}}, {{Ethnicity}}, and {{Gender}} on {{Jail}} and {{Prison Sentences}}},
  shorttitle = {Does {{Sentence Type}} and {{Length Matter}}?},
  author = {Cassidy, Michael and Rydberg, Jason},
  year = {2020},
  journal = {Criminal Justice and Behavior},
  volume = {47},
  pages = {61},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Cassidy_Rydberg\\Does Sentence Type and Length Matter_Cassidy_Rydberg_2020.pdf;C\:\\Users\\stocb\\Zotero\\storage\\YCB3YKIY\\LandingPage.html}
}

@article{dempsterMaximumLikelihoodIncomplete1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {39},
  number = {1},
  pages = {1--38},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  keywords = {Read},
  file = {G\:\\My Drive\\Zotero\\Research$a1977\\Dempster et al_1977_Maximum Likelihood from Incomplete Data via the EM Algorithm.pdf}
}

@article{grahamHowManyImputations2007,
  title = {How {{Many Imputations}} Are {{Really Needed}}? {{Some Practical Clarifications}} of {{Multiple Imputation Theory}}},
  shorttitle = {How {{Many Imputations}} Are {{Really Needed}}?},
  author = {Graham, John W. and Olchowski, Allison E. and Gilreath, Tamika D.},
  year = {2007},
  month = sep,
  journal = {Prevention Science},
  volume = {8},
  number = {3},
  pages = {206--213},
  issn = {1573-6695},
  doi = {10.1007/s11121-007-0070-9},
  abstract = {Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information ({$\gamma$}) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which {$\gamma$} and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on {$\gamma$}, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.},
  langid = {english},
  keywords = {Full information maximum likelihood,Missing data,Multiple imputation,Number of imputations,Read,Statistical power},
  file = {G\:\\My Drive\\Zotero\\Research\\Graham et al\\How Many Imputations are Really Needed_Graham et al_2007.pdf}
}

@article{harelInferencesMissingInformation2007,
  title = {Inferences on Missing Information under Multiple Imputation and Two-Stage Multiple Imputation},
  author = {Harel, Ofer},
  year = {2007},
  month = jan,
  journal = {Statistical Methodology},
  volume = {4},
  number = {1},
  pages = {75--89},
  issn = {15723127},
  doi = {10.1016/j.stamet.2006.03.002},
  abstract = {In the presence of missing values, researchers may be interested in the rates of missing information. The rates of missing information are (a) important for assessing how the missing information contributes to inferential uncertainty about, Q, the population quantity of interest, (b) are an important component in the decision of the number of imputations, and (c) can be used to test model uncertainty and model fitting. In this article I will derive the asymptotic distribution of the rates of missing information in two scenarios: the conventional multiple imputation (MI), and the two-stage MI. Numerically I will show that the proposed asymptotic distribution agrees with the simulated one. I will also suggest the number of imputations needed to obtain reliable missing information rate estimates for each method, based on the asymptotic distribution. c 2006 Elsevier B.V. All rights reserved.},
  langid = {english},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Research\\Literature_Review_Notes\\harelInferencesMissingInformation2007.md;G\:\\My Drive\\Zotero\\Criminology$a2007\\Harel_2007_Inferences on missing information under multiple imputation and two-stage.pdf}
}

@article{harelMultipleImputationIncomplete2018,
  title = {Multiple {{Imputation}} for {{Incomplete Data}} in {{Epidemiologic Studies}}},
  author = {Harel, Ofer and Mitchell, Emily M and Perkins, Neil J and Cole, Stephen R and Tchetgen Tchetgen, Eric J and Sun, BaoLuo and Schisterman, Enrique F},
  year = {2018},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {187},
  number = {3},
  pages = {576--584},
  issn = {0002-9262},
  doi = {10.1093/aje/kwx349},
  abstract = {Epidemiologic studies are frequently susceptible to missing information. Omitting observations with missing variables remains a common strategy in epidemiologic studies, yet this simple approach can often severely bias parameter estimates of interest if the values are not missing completely at random. Even when missingness is completely random, complete-case analysis can reduce the efficiency of estimated parameters, because large amounts of available data are simply tossed out with the incomplete observations. Alternative methods for mitigating the influence of missing information, such as multiple imputation, are becoming an increasing popular strategy in order to retain all available information, reduce potential bias, and improve efficiency in parameter estimation. In this paper, we describe the theoretical underpinnings of multiple imputation, and we illustrate application of this method as part of a collaborative challenge to assess the performance of various techniques for dealing with missing data (Am J Epidemiol. 2018;187(3):568\textendash 575). We detail the steps necessary to perform multiple imputation on a subset of data from the Collaborative Perinatal Project (1959\textendash 1974), where the goal is to estimate the odds of spontaneous abortion associated with smoking during pregnancy.},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Harel et al\\Multiple Imputation for Incomplete Data in Epidemiologic Studies_Harel et al_2018.pdf;C\:\\Users\\stocb\\Zotero\\storage\\J5U58K2L\\4642952.html}
}

@article{harelMultipleImputationReview2007,
  title = {Multiple Imputation: Review of Theory, Implementation and Software},
  shorttitle = {Multiple Imputation},
  author = {Harel, Ofer and Zhou, Xiao-Hua},
  year = {2007},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {26},
  number = {16},
  pages = {3057--3077},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.2787},
  abstract = {Missing data is a common complication in data analysis. In many medical settings missing data can cause difficulties in estimation, precision and inference. Multiple imputation (MI) (Multiple Imputation for Nonresponse in Surveys. Wiley: New York, 1987) is a simulation-based approach to deal with incomplete data. Although there are many different methods to deal with incomplete data, MI has become one of the leading methods. Since the late 1980s we observed a constant increase in the use and publication of MI-related research. This tutorial does not attempt to cover all the material concerning MI, but rather provides an overview and combines together the theory behind MI, the implementation of MI, and discusses increasing possibilities of the use of MI using commercial and free software. We illustrate some of the major points using an example from an Alzheimer disease (AD) study. In this AD study, while clinical data are available for all subjects, postmortem data are only available for the subset of those who died and underwent an autopsy. Analysis of incomplete data requires making unverifiable assumptions. These assumptions are discussed in detail in the text. Relevant S-Plus code is provided. Copyright q 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Read},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\SPL3FK4W\\Harel and Zhou - 2007 - Multiple imputation review of theory, implementat.pdf}
}

@article{ibrahimMissingDataClinical2012,
  title = {Missing {{Data}} in {{Clinical Studies}}: {{Issues}} and {{Methods}}},
  shorttitle = {Missing {{Data}} in {{Clinical Studies}}},
  author = {Ibrahim, Joseph G. and Chu, Haitao and Chen, Ming-Hui},
  year = {2012},
  month = sep,
  journal = {Journal of Clinical Oncology},
  volume = {30},
  number = {26},
  pages = {3297--3303},
  issn = {0732-183X},
  doi = {10.1200/JCO.2011.38.7589},
  abstract = {Missing data are a prevailing problem in any type of data analyses. A participant variable is considered missing if the value of the variable (outcome or covariate) for the participant is not observed. In this article, various issues in analyzing studies with missing data are discussed. Particularly, we focus on missing response and/or covariate data for studies with discrete, continuous, or time-to-event end points in which generalized linear models, models for longitudinal data such as generalized linear mixed effects models, or Cox regression models are used. We discuss various classifications of missing data that may arise in a study and demonstrate in several situations that the commonly used method of throwing out all participants with any missing data may lead to incorrect results and conclusions. The methods described are applied to data from an Eastern Cooperative Oncology Group phase II clinical trial of liver cancer and a phase III clinical trial of advanced non\textendash small-cell lung cancer. Although the main area of application discussed here is cancer, the issues and methods we discuss apply to any type of study.},
  pmcid = {PMC3948388},
  pmid = {22649133},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Ibrahim et al\\Missing Data in Clinical Studies_Ibrahim et al_2012.pdf}
}

@article{kingHaveRacialEthnic2019,
  title = {Have {{Racial}} and {{Ethnic Disparities}} in {{Sentencing Declined}}?},
  author = {King, Ryan D. and Light, Michael T.},
  year = {2019},
  month = may,
  journal = {Crime and Justice},
  volume = {48},
  pages = {365--437},
  publisher = {{The University of Chicago Press}},
  issn = {0192-3234},
  doi = {10.1086/701505},
  abstract = {Blacks and Hispanics convicted of felonies are more likely than whites to receive prison sentences for their crimes, and they receive slightly longer sentences if imprisoned. Yet the majority of prior research compares sentencing decisions at a single point in time and does not give explicit attention to whether and how racial and ethnic disparities have changed. Decades of sentencing data from Minnesota, the federal courts, and a sample of large urban counties are used to assess the degree of change in racial and ethnic sentencing disparities since the 1980s. There has been some decline in the magnitude of racial and ethnic disparities, with changes in drug laws aligning with some of the reduction in disparity at the federal level. This trend, along with the pattern of findings from related studies, poses a challenge to prominent theoretical explanations of sentencing disparities, including racial threat theory and the focal concerns perspective. Each of four influential theoretical explanations of racial and ethnic disparities in sentencing includes significant empirical or logical shortcomings. Advancing theoretical understanding of racial and ethnic disparity will require new data that follow cases from the point of arrest through to final disposition and include information about citizenship and victims.},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/kingHaveRacialEthnic2019.md;G\:\\My Drive\\Zotero\\Criminology\\King_Light\\Have Racial and Ethnic Disparities in Sentencing Declined_King_Light_2019.pdf;G\:\\My Drive\\Zotero\\Criminology\\King_Light\\Have Racial and Ethnic Disparities in Sentencing Declined_King_Light_22.pdf}
}

@article{kutateladzeCumulativeDisadvantageExamining2014,
  title = {Cumulative {{Disadvantage}}: {{Examining Racial}} and {{Ethnic Disparity}} in {{Prosecution}} and {{Sentencing}}},
  shorttitle = {Cumulative {{Disadvantage}}},
  author = {KutaTELadze, Besiki L. and Andiloro, Nancy R. and Johnson, Brian D. and Spohn, Cassia C.},
  year = {2014},
  journal = {Criminology},
  volume = {52},
  number = {3},
  pages = {514--551},
  issn = {1745-9125},
  doi = {10.1111/1745-9125.12047},
  abstract = {Current research on criminal case processing typically examines a single decision-making point, so drawing reliable conclusions about the impact that factors such as defendants' race or ethnicity exert across successive stages of the justice system is difficult. Using data from the New York County District Attorney's Office that tracks 185,275 diverse criminal cases, this study assesses racial and ethnic disparity for multiple discretionary points of prosecution and sentencing. Findings from multivariate logistic regression analyses demonstrate that the effects of race and ethnicity vary by discretionary point and offense category. Black and Latino defendants were more likely than White defendants to be detained, to receive a custodial plea offer, and to be incarcerated\textemdash and they received especially punitive outcomes for person offenses\textemdash but were more likely to benefit from case dismissals. The findings for Asian defendants were less consistent but suggest they were the least likely to be detained, to receive custodial offers, and to be incarcerated. These findings are discussed in the context of contemporary theoretical perspectives on racial bias and cumulative disadvantage in the justice system.},
  langid = {english},
  keywords = {cumulative disadvantage,prosecution,punitiveness,race/ethnicity,sentencing},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1745-9125.12047},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/kutateladzeCumulativeDisadvantageExamining2014.md;G\:\\My Drive\\Zotero\\Criminology\\Kuta℡adze et al\\Cumulative Disadvantage_Kuta℡adze et al_2014.pdf;G\:\\My Drive\\Zotero\\Criminology\\Kuta℡adze et al\\Cumulative Disadvantage_Kuta℡adze et al_22.pdf;C\:\\Users\\stocb\\Zotero\\storage\\NNIWV6PW\\1745-9125.html}
}

@article{lehmannRaceEthnicityCrime2020,
  title = {Race, {{Ethnicity}}, {{Crime Type}}, and the {{Sentencing}} of {{Violent Felony Offenders}}},
  author = {Lehmann, Peter S.},
  year = {2020},
  month = jun,
  journal = {Crime \& Delinquency},
  volume = {66},
  number = {6-7},
  pages = {770--805},
  issn = {0011-1287, 1552-387X},
  doi = {10.1177/0011128720902699},
  abstract = {Within the large body of literature on racial/ethnic disparities in criminal sentencing, some research has demonstrated that these relationships are conditional upon various legally relevant case characteristics, including the type of offense for which the defendants are sentenced. To date, however, few studies have explored the potential moderating effects of different violent crimes. Using data from Florida (N = 186,885), the findings from these analyses indicate that Black\textendash White sentencing disparities are particularly pronounced for manslaughter, robbery/carjacking, arson, and resisting arrest with violence. While Hispanic ethnicity exerts limited effects on sentencing outcomes generally, Hispanics are particularly disadvantaged in manslaughter cases. Relative to minority defendants, White offenders receive harsher sentences for sexual battery, other sex offenses, and abuse of children.},
  langid = {english},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Lehmann\\Race, Ethnicity, Crime Type, and the Sentencing of Violent Felony Offenders_Lehmann_2020.pdf;G\:\\My Drive\\Zotero\\Criminology\\Lehmann\\Race, Ethnicity, Crime Type, and the Sentencing of Violent Felony Offenders_Lehmann_22.pdf}
}

@article{perkinsPrincipledApproachesMissing2018,
  title = {Principled {{Approaches}} to {{Missing Data}} in {{Epidemiologic Studies}}},
  author = {Perkins, Neil J and Cole, Stephen R and Harel, Ofer and Tchetgen Tchetgen, Eric J and Sun, BaoLuo and Mitchell, Emily M and Schisterman, Enrique F},
  year = {2018},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {187},
  number = {3},
  pages = {568--575},
  issn = {0002-9262},
  doi = {10.1093/aje/kwx348},
  abstract = {Principled methods with which to appropriately analyze missing data have long existed; however, broad implementation of these methods remains challenging. In this and 2 companion papers (Am J Epidemiol. 2018;187(3):576\textendash 584 and Am J Epidemiol. 2018;187(3):585\textendash 591), we discuss issues pertaining to missing data in the epidemiologic literature. We provide details regarding missing-data mechanisms and nomenclature and encourage the conduct of principled analyses through a detailed comparison of multiple imputation and inverse probability weighting. Data from the Collaborative Perinatal Project, a multisite US study conducted from 1959 to 1974, are used to create a masked data-analytical challenge with missing data induced by known mechanisms. We illustrate the deleterious effects of missing data with naive methods and show how principled methods can sometimes mitigate such effects. For example, when data were missing at random, naive methods showed a spurious protective effect of smoking on the risk of spontaneous abortion (odds ratio (OR) = 0.43, 95\% confidence interval (CI): 0.19, 0.93), while implementation of principled methods multiple imputation (OR = 1.30, 95\% CI: 0.95, 1.77) or augmented inverse probability weighting (OR = 1.40, 95\% CI: 1.00, 1.97) provided estimates closer to the ``true'' full-data effect (OR = 1.31, 95\% CI: 1.05, 1.64). We call for greater acknowledgement of and attention to missing data and for the broad use of principled missing-data methods in epidemiologic research.},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/perkinsPrincipledApproachesMissing2018.md;G\:\\My Drive\\Zotero\\Criminology\\Perkins et al\\Principled Approaches to Missing Data in Epidemiologic Studies_Perkins et al_2018.pdf;G\:\\My Drive\\Zotero\\Criminology\\Perkins et al\\Principled Approaches to Missing Data in Epidemiologic Studies_Perkins et al_22.pdf;C\:\\Users\\stocb\\Zotero\\storage\\I97Z3RYE\\4642951.html}
}

@article{raghunathanMultivariateTechniqueMultiply2001,
  title = {A {{Multivariate Technique}} for {{Multiply Imputing Missing Values Using}} a {{Sequence}} of {{Regression Models}}},
  author = {Raghunathan, Trivellore E and Lepkowski, James M and Hoewyk, John Van and Solenberger, Peter},
  year = {2001},
  month = jun,
  volume = {27},
  number = {1},
  pages = {85--95},
  abstract = {This article describes and evaluates a procedure for imputing missing values for a relatively complex data structure when the data are missing at random. The imputations are obtained by fitting a sequence of regression models and drawing values from the corresponding predictive distributions. The types of regression models used are linear, logistic, Poisson, generalized logit or a mixture of these depending on the type of variable being imputed. Two additional common features in the imputation process are incorporated: restriction to a relevant subpopulation for some variables and logical bounds or constraints for the imputed values. The restrictions involve subsetting the sample individuals that satisfy certain criteria while fitting the regression models. The bounds involve drawing values from a truncated predictive distribution. The development of this method was partly motivated by the analysis of two data sets which are used as illustrations. The sequential regression procedure is applied to perform multiple imputation analysis for the two applied problems. The sampling properties of inferences from multiply imputed data sets created using the sequential regression method are evaluated through simulated data sets.},
  langid = {english},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/raghunathanMultivariateTechniqueMultiply2001.md;G\:\\My Drive\\UConn\\raghunathanMultivariateTechniqueMultiply2001.md;G\:\\My Drive\\Zotero\\Research$a2001\\Raghunathan et al_2001_A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence.pdf}
}

@article{rubinMatchingRemoveBias1973,
  title = {Matching to {{Remove Bias}} in {{Observational Studies}}},
  author = {Rubin, Donald B.},
  year = {1973},
  journal = {Biometrics},
  volume = {29},
  number = {1},
  pages = {159--183},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529684},
  abstract = {Several matching methods that match all of one sample from another larger sample on a continuous matching variable are compared with respect to their ability to remove the bias of the matching variable. One method is a simple mean-matching method and three are nearest available pair-matching methods. The methods' abilities to remove bias are also compared with the theoretical maximum given fixed distributions and fixed sample sizes. A summary of advice to an investigator is included.},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Rubin\\Matching to Remove Bias in Observational Studies_Rubin_12.pdf}
}

@book{rubinMultipleImputationNonresponse1987,
  title = {{Multiple Imputation for Nonresponse in Surveys | Wiley Series in Probability and Statistics}},
  author = {Rubin, Donald B},
  year = {1987},
  month = jun,
  series = {{Wiley Series in Probability and Statistics}},
  publisher = {{Wiley}},
  abstract = {Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. Clearly illustrates the advantages of modern computing to such handle surveys, and demonstrates the benefit of this statistical technique for researchers who must analyze them. Also presents the background for Bayesian and frequentist theory. After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, the text evaluates procedures in general circumstances, outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. Examples and exercises reinforce ideas, and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.},
  isbn = {978-0-471-08705-2},
  langid = {English (United States), en-US},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\ESHP9QBJ\\9780470316696.html}
}

@book{rubinMultipleImputationNonresponse2004,
  title = {Multiple {{Imputation}} for {{Nonresponse}} in {{Surveys}}},
  author = {Rubin, Donald B.},
  year = {2004},
  publisher = {{John Wiley \& Sons, Incorporated}},
  address = {{Hoboken, UNITED STATES}},
  abstract = {Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. Clearly illustrates the advantages of modern computing to such handle surveys, and demonstrates the benefit of this statistical technique for researchers who must analyze them. Also presents the background for Bayesian and frequentist theory. After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, the text evaluates procedures in general circumstances, outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. Examples and exercises reinforce ideas, and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.},
  isbn = {978-0-470-31736-5},
  keywords = {Multiple imputation (Statistics),Nonresponse (Statistics),Social surveys -- Response rate.},
  file = {/home/bsuconn/Documents/Textbooks/Rubin - 2004 - Multiple Imputation for Nonresponse in Surveys.pdf;C\:\\Users\\stocb\\Zotero\\storage\\A55VXHIR\\detail.html}
}

@article{rubinObjectiveCausalInference2008,
  title = {For Objective Causal Inference, Design Trumps Analysis},
  author = {Rubin, Donald B.},
  year = {2008},
  month = sep,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {3},
  pages = {808--840},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS187},
  abstract = {For obtaining causal inferences that are objective, and therefore have the best chance of revealing scientific truths, carefully designed and executed randomized experiments are generally considered to be the gold standard. Observational studies, in contrast, are generally fraught with problems that compromise any claim for objectivity of the resulting causal inferences. The thesis here is that observational studies have to be carefully designed to approximate randomized experiments, in particular, without examining any final outcome data. Often a candidate data set will have to be rejected as inadequate because of lack of data on key covariates, or because of lack of overlap in the distributions of key covariates between treatment and control groups, often revealed by careful propensity score analyses. Sometimes the template for the approximating randomized experiment will have to be altered, and the use of principal stratification can be helpful in doing this. These issues are discussed and illustrated using the framework of potential outcomes to define causal effects, which greatly clarifies critical issues.},
  keywords = {Average causal effect,causal effects,complier average causal effect,instrumental variables,noncompliance,observational studies,propensity scores,randomized experiments,Rubin Causal Model},
  file = {G\:\\My Drive\\Zotero\\Criminology\\Rubin\\For objective causal inference, design trumps analysis_Rubin_22.pdf;C\:\\Users\\stocb\\Zotero\\storage\\7GLYCCYT\\08-AOAS187.html}
}

@article{schaferMultipleImputationMultivariate2003,
  title = {Multiple {{Imputation}} in {{Multivariate Problems When}} the {{Imputation}} and {{Analysis Models Differ}}},
  author = {Schafer, Joseph L.},
  year = {2003},
  journal = {Statistica Neerlandica},
  volume = {57},
  number = {1},
  pages = {19--35},
  issn = {1467-9574},
  doi = {10.1111/1467-9574.00218},
  abstract = {Bayesian multiple imputation (MI) has become a highly useful paradigm for handling missing values in many settings. In this paper, I compare Bayesian MI with other methods \textendash{} maximum likelihood, in particular\textemdash and point out some of its unique features. One key aspect of MI, the separation of the imputation phase from the analysis phase, can be advantageous in settings where the models underlying the two phases do not agree.},
  langid = {english},
  keywords = {missing data,nonresponse,Unread},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9574.00218},
  file = {G\:\\My Drive\\Zotero\\Research\\Schafer\\Multiple Imputation in Multivariate Problems When the Imputation and Analysis_Schafer_2003.pdf;C\:\\Users\\stocb\\Zotero\\storage\\BQAN4CPY\\1467-9574.html}
}

@article{schaferMultipleImputationPrimer1999,
  title = {Multiple Imputation: A Primer},
  shorttitle = {Multiple Imputation},
  author = {Schafer, Joseph L},
  year = {1999},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {8},
  number = {1},
  pages = {3--15},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  doi = {10.1177/096228029900800102},
  abstract = {In recent years, multiple imputation has emerged as a convenient and flexible paradigm for analysing data with missing values. Essential features of multiple imputation are reviewed, with answers to frequently asked questions about using the method in practice.},
  langid = {english},
  keywords = {Read},
  file = {/home/bsuconn/Documents/Research/Literature_Review_Notes/schaferMultipleImputationPrimer1999.md;G\:\\My Drive\\Zotero\\Research\\Schafer\\Multiple imputation_Schafer_1999.pdf}
}

@article{steffensmeierIntersectionalityRaceEthnicity2017,
  title = {Intersectionality of {{Race}}, {{Ethnicity}}, {{Gender}}, and {{Age}} on {{Criminal Punishment}}},
  author = {Steffensmeier, Darrell and {Painter-Davis}, Noah and Ulmer, Jeffery},
  year = {2017},
  month = aug,
  journal = {Sociological Perspectives},
  volume = {60},
  number = {4},
  pages = {810--833},
  publisher = {{SAGE Publications Inc}},
  issn = {0731-1214},
  doi = {10.1177/0731121416679371},
  abstract = {Race, ethnicity, gender, and age are core foci within sociology and law/criminology. Also prominent is how these statuses intersect to affect behavioral outcomes, but statistical studies of intersectionality are rare. In the area of criminal sentencing, an abundance of studies examine main and joint effects of race and gender but few investigate in detail how these effects are conditioned by defendant's age. Using recent Pennsylvania sentencing data and a novel method for analyzing statistical interactions, we examine the main and combined effects of these statuses on sentencing. We find strong evidence for intersectionality: Harsher sentences concentrate among young black males and Hispanic males of all ages, while the youngest females (regardless of race/ethnicity) and some older defendants receive leniency. The focal concerns model of sentencing that frames our study has strong affinity with intersectionality perspectives and can serve as a template for research examining the ways social statuses shape inequality.},
  langid = {english},
  keywords = {age,criminal punishment,ethnicity,gender,inequality,intersectionality,race,theory},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/steffensmeierIntersectionalityRaceEthnicity2017.md;G\:\\My Drive\\Zotero\\Criminology\\Steffensmeier et al\\Intersectionality of Race, Ethnicity, Gender, and Age on Criminal Punishment_Steffensmeier et al_2017.pdf;G\:\\My Drive\\Zotero\\Criminology\\Steffensmeier et al\\Intersectionality of Race, Ethnicity, Gender, and Age on Criminal Punishment_Steffensmeier et al_22.pdf}
}

@article{tannerCalculationPosteriorDistributions1987,
  title = {The {{Calculation}} of {{Posterior Distributions}} by {{Data Augmentation}}},
  author = {Tanner, Martin A. and Wong, Wing Hung},
  year = {1987},
  journal = {Journal of the American Statistical Association},
  volume = {82},
  number = {398},
  pages = {528--540},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2289457},
  abstract = {The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea is quite simple. The observed data y is augmented by the quantity z, which is referred to as the latent data. It is assumed that if y and z are both known, then the problem is straightforward to analyze, that is, the augmented data posterior p(\texttheta{} {$\mid$} y, z) can be calculated. But the posterior density that we want is p(\texttheta{} {$\mid$} y), which may be difficult to calculate directly. If, however, one can generate multiple values of z from the predictive distribution p(z {$\mid$} y) (i.e., multiple imputations of z), then p(\texttheta{} {$\mid$} y) can be approximately obtained as the average of p(\texttheta{} {$\mid$} y, z) over the imputed z's. However, p(z {$\mid$} y) depends, in turn, on p(\texttheta{} {$\mid$} y). Hence if p(\texttheta{} {$\mid$} y) was known, it could be used to calculate p(z {$\mid$} y). This mutual dependency between p(\texttheta{} {$\mid$} y) and p(z {$\mid$} y) leads to an iterative algorithm to calculate p(\texttheta{} {$\mid$} y). Analytically, this algorithm is essentially the method of successive substitution for solving an operator fixed point equation. We exploit this fact to prove convergence under mild regularity conditions. Typically, to implement the algorithm, one must be able to sample from two distributions, namely p(\texttheta{} {$\mid$} y, z) and p(z {$\mid$} \texttheta, y). In many cases, it is straightforward to sample from either distribution. In general, though, either sampling can be difficult, just as either the E or the M step can be difficult to implement in the EM algorithm. For p(\texttheta{} {$\mid$} y, z) arising from parametric submodels of the multinomial, we develop a primitive but generally applicable way to approximately sample \texttheta. The idea is first to sample from the posterior distribution of the cell probabilities and then to project to the parametric surface that is specified by the submodel, giving more weight to those observations lying closer to the surface. This procedure should cover many of the common models for categorical data. There are several examples given in this article. First, the algorithm is introduced and motivated in the context of a genetic linkage example. Second, we apply this algorithm to an example of inference from incomplete data regarding the correlation coefficient of the bivariate normal distribution. It is seen that the algorithm recovers the bimodal nature of the posterior distribution. Finally, the algorithm is used in the analysis of the traditional latent-class model as applied to data from the General Social Survey.},
  keywords = {bayesian data analysis,data augmentation,missing data,Unread},
  file = {G\:\\My Drive\\Zotero\\Generally Important Stats Papers$a1987\\Tanner_Wong_1987_The Calculation of Posterior Distributions by Data Augmentation.pdf}
}

@article{ulmerDisproportionalImprisonmentBlack2016,
  title = {Disproportional {{Imprisonment}} of {{Black}} and {{Hispanic Males}}: {{Sentencing Discretion}}, {{Processing Outcomes}}, and {{Policy Structures}}},
  shorttitle = {Disproportional {{Imprisonment}} of {{Black}} and {{Hispanic Males}}},
  author = {Ulmer, Jeffery and {Painter-Davis}, Noah and Tinik, Leigh},
  year = {2016},
  month = jun,
  journal = {Justice Quarterly},
  volume = {33},
  number = {4},
  pages = {642--681},
  publisher = {{Routledge}},
  issn = {0741-8825},
  doi = {10.1080/07418825.2014.958186},
  abstract = {Disproportional incarceration of black and Hispanic men has been the subject of much critical commentary and empirical inquiry. Such disproportionality may be due to greater involvement of minority men in serious crime, to discretionary decisions by local justice officials, or to the differential impact of sentencing policies, such as mandatory minimums or sentencing guidelines, that differentially impact minority men. This study investigated the extent to which the disproportional punishment of black and Hispanic men, and local variation in such disproportionality, can be attributed to unexplained disparities in local sentencing decisions, as opposed to the extent to which such differences are mediated by sentencing policies, or case-processing and extralegal factors. We use 2005\textendash 2009 federal court and Pennsylvania state court data. Our findings suggest, particularly in Federal courts, that most disproportionality is determined by processes prior to sentencing, especially sentencing policies that differentially impact minority males.},
  keywords = {discretion,imprisonment,racial disparity,sentencing},
  annotation = {\_eprint: https://doi.org/10.1080/07418825.2014.958186},
  file = {/home/bsuconn/Documents/Research/Criminology/Literature/ulmerDisproportionalImprisonmentBlack2016.md;G\:\\My Drive\\Zotero\\Criminology\\Ulmer et al\\Disproportional Imprisonment of Black and Hispanic Males_Ulmer et al_2016.pdf;C\:\\Users\\stocb\\Zotero\\storage\\7CL7FP76\\07418825.2014.html}
}


