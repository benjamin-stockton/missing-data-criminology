% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=30mm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tablesignature}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Entropy Balancing},
  pdfauthor={Benjamin Stockton},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Entropy Balancing}
\author{Benjamin Stockton}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, sharp corners, breakable, frame hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, enhanced]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{entropy-balancing}{%
\section{Entropy Balancing}\label{entropy-balancing}}

To make causal inferences in observational data, the observed
Y(1)\textbar D=1 and counterfactual Y(0)\textbar D=1 are compared to get
the Population Average Treatment Effect on Treated (PATT) defined as
\(\tau = E(Y(1)|D=1) - E(Y(0) | D=1)\). In experimental studies where
treatment assignment is independent of the potential outcomes,
approximate the second expectation by \(E(Y(0) | D=0)\) i.e.~the mean of
the control group. In observational studies, if we can ``assume
ignorable treatment assignment and overlap'', then \(Y(0) \perp D|X\)
and \(P(D = 1| X=x) \leq 1\) for all \(x\) in the support of
\(f_{X|D=1}\). That means that if the confounding covariates are similar
on both the treatment and control groups, we can estimate the
counterfactual average outcome by
\(\tau = E(Y|D=1) = \int E(Y|X=x, D=0) f_{X|D=1}(x) dx.\)

To make causal inferences in observational data, the observed
\(Y(1)|D=1\) and counterfactual \(Y(0)|D=1\) are compared to get the
Population Average Treatment Effect on Treated (PATT) defined as
\(\tau = E(Y(1)|D=1) - E(Y(0) | D=1)\). In experimental studies where
treatment assignment is independent of the potential outcomes,
approximate the second expectation by \(E(Y(0) | D=0)\) i.e.~the mean of
the control group. In observational studies, if we can ``assume
ignorable treatment assignment and overlap'', then \(Y(0) \perp D|X\)
and \(P(D = 1| X=x) \leq 1\) for all \(x\) in the support of
\(f_{X|D=1}\). That means that if the confounding covariates are similar
on both the treatment and control groups, we can estimate the
counterfactual average outcome by
\(\tau = E(Y|D=1) = \int E(Y|X=x, D=0) f_{X|D=1}(x) dx.\)

\begin{quote}
Notice that the last term in this expression is equal to the covariate
adjusted mean, that is, the estimated mean of \(Y\) in the source
population if its covariates were distributed as in the target
population.
\end{quote}

\begin{itemize}
\tightlist
\item
  p. 28 (\textbf{hainmueller2012?})
\end{itemize}

\begin{quote}
{[}Rosenbaum and Rubin (1983){]} showed that the multivariate matching
pre-processing problem{]} can be reduced to a single dimension if the
counterfactual mean can be identified as
\(E(Y(0)|D=1) = \int E(Y|p(X) = \rho, D=0) f_{p|D=1}(\rho)d\rho\) where
\(f_{p|D=1}\) is the dist. of the propensity score
\(p(x) = P(D=1 | X=x)\) in the target population.
\end{quote}

\begin{itemize}
\tightlist
\item
  p. 28 (\textbf{hainmueller2012?})
\end{itemize}

Propensity score weighting is performed by using a binary response
(logit/probit) regression to estimate a probability \(p_i\) to be in the
treatment given the covariates. These are converted to weights \(d_i\)
by the inverse of the link function \(d_i = p_i / (1-p_i)\) so that the
counterfactual mean is estimated as
\(\widehat{E(Y(0)|D=1)} = \frac{\sum_{i|D=0}Y_i d_i}{\sum_{i|D=0}d_i}.\)

Drawbacks of PSW:

\begin{itemize}
\item
  The true propensity score is valuable because it balances the
  covariate distributions of the two groups, but is unknown and
  difficult to accurately estimate.
\item
  Mis-specified scores can lead to biased estimation of TE
\item
  Practitioners often iterate between weighting and matching, modeling
  the PS, and then evaluating the balance until a suitable balance is
  achieved. Imai, King and Stuart (2008) call this the ``propensity
  score tautology''.

  \begin{itemize}
  \tightlist
  \item
    Despite this balance often isn't achieved and can make balance worse
    among confounders.
  \end{itemize}
\end{itemize}

\hypertarget{entropy-balancing-procedure}{%
\subsection{Entropy Balancing
Procedure:}\label{entropy-balancing-procedure}}

\textbf{Goal:} Estimate \(\tau = E(Y(1) |D=1) - E(Y(0) | D=1).\)

The counterfactual mean could be estimated by
\(\widehat{E(Y(0)|D=1)}=\frac{\sum_{i|D=0} Y_i w_i}{\sum_{i|D=0} w_i}.\)
The weights are found by minimizing \(H(w) = \sum_{i|D=0} h(w_i)\)
subject to \(\sum_{i|D=0} w_i c_{ri}(X_i) = m_r\) for \(r =1,…,R\) and
\(\sum_{i|D=0} w_i = 1\) and \(w_i \geq 0\) for all \(i\) given
\(D_i = 0\) where \(h(.)\) is a distance metric and \(c_{ri}(X_i)=m_r\)
describes the set of \(R\) balance constraints imposed by the covariate
moments of the re-weighted control group.

Authors choose to use \(h(w_i)=w_i\log(w_i/q_i)\) the Kullback entropy
divergence.

In conventional PSW, the researcher (1) estimates \(d_i\) then (2)
checks if the weights balance the covariate distribution. Entropy
balancing reverses the approach by obtaining weights by minimizing a
linear equation with constraints that guarantee balance while remaining
close to the uniform weights to guarantee efficiency.

Optimization is performed using Lagrangian Multipliers.

\hypertarget{main-issues-with-entropy-balancing}{%
\subsubsection{3 Main Issues with Entropy
Balancing:}\label{main-issues-with-entropy-balancing}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``No weighting solution exists if the balance constraints are
  inconsistent.'' Easy to avoid as constraints are researcher imposed.
\item
  ``Balance constraints are consistent but there exists no set of
  positive weights to actually satisfy the constraints.'' For example,
  there's heavy imbalance in a binary categorical variable between the
  treatment and control groups. ``If there aren't enough controls that
  look anything like the treated units, then the existing data do not
  contain sufficient information to reliably infer the counterfactual.''
\item
  ``A solution exists, but due to limited overlap, the solution involves
  an extreme adjustment to the weights of some control units.'' Only a
  few units receive relatively large weights and all others are set near
  0, then the variance will increase and effective sample size is small.
\end{enumerate}

Issues 2 and 3 are also relevant to most pre-processing that involves
balancing including propensity score weighting.

\hypertarget{entropy-balancing-in-sentencing-research}{%
\subsection{Entropy Balancing in Sentencing
Research}\label{entropy-balancing-in-sentencing-research}}

These notes are largely based on and quotions from (MacDonald and
Donnelly 2019). The authors note that a cetnral goal of contemporary
criminal justice reform is to ``reduce racial disparities in prisons''
in part by building an ``understanding of the sources of these
inequalities.'' Regression has been the primary tool used to estimate
the race effect (note that the authors make causal claims without
specifying the use of causal methods). MacDonald and Donnelly motivate
entropy balancing by noting that

\begin{quote}
``{[}M{]}ultivariate regression approaches to estimating the effect of
race on sentencing may produce biased estimates if there are important
subgroup differences across covariates that are not adequately removed
through mean adjustment.''
\end{quote}

Given that is the extent of the motivation and the lack of theoretical
basis for entropy balancing beyond making the covariate distributions
identical on the moments, this doesn't seem to be the best path forward
for causal methods in criminology or other social sciences. MacDonald
and Donnelly engage with propensity score matching and weighting, but
dismiss it for not producing well-balanced covariate data sets in every
case. The balancing seems to me to be a side-effect of constructing
suitable counterfactuals, and not the direct goal of propensity score
methods. In other words, propensity scores (and resulting weights) have
the theoretical meaning of being the likelihood of belonging to the
treatment group given other characterisitcs. Entropy weights have no
such meaning because they work backward from the assumption that the
moments are matched.

The authors also claim entropy balancing as doubly robust, while other
papers have called the doubly robust claims into question
(\textbf{freedman2008?}) (notably the co-author, Berk, is a
criminology/statistics professor who has been critical of the way
statistical methods have been used in criminology in other papers as
well (Berk 2010)). MacDonald and Donnelly seem to define doubly robust
estimation as ``assum{[}ing{]} consistent measurement of treatment
effects if either the weights from the \textbf{balancing approaches} or
\textbf{regression model} are correctly specified
(\textbf{bang2005?}).'' (MacDonald and Donnelly 2019, p.
662).\footnote{When actually checking Bang and Robins'
  (\textbf{bang2005?}) definition they say ``In a missing data model, an
  estimator is DR if it remains consistent when either (but not
  necessarily both) a model for the missingness mechanism or a model for
  the distribution of the complete data is correctly specified.'' which
  is consistent with Funk et al's definition (\textbf{funk2011?}) .
  These are markedly different definitions in that Bang and Robins are
  discussing the model specification as being the source of biasedness
  while MacDonald and Donnelly seem to be paraphrasing the definition as
  caring only about whether the estimates are biased regardless of the
  cause.} This does not seem to be the correct definition.

The central claim by the authors is that

\begin{quote}
``Although the coefficients produced from the linear regression and the
other approaches are not statistically different from each other
(e.g.~all coefficients are within 1 standard deviation of each other),
the change in the size of the estimated mean sentence length difference
between Blacks and Whites suggests that the regression model is
downwardly biased. Specifically, race coefficients have similar
precision, but the parameter estimated from the linear regression is
one-half the size of the coefficients estimated from entropy or
propensity score weighting.'' (MacDonald and Donnelly 2019, p. 675)
\end{quote}

But there's no justification for any of these claims. No engagement with
the uncertainty measures; in fact saying they're all within one standard
error of each other should be the end of the conversation. That means
there are no real distinctions between results so trying to draw any is
pointless.

A central issue with entropy weights is the lack of a clear causal
estimand resulting from the weighting. IPW results in ATE (total
population), ``treated'' results in ATT (target population is only the
treated individuals), and ``overlap'' weights result in ATO (overlap of
treatment and control in population) (\textbf{zhou2022?}). Generally,
entropy weighting will also make comparisons on the portion of the total
population where there's significant overlap between the treatment and
control groups, but without theoretical optimality properties.

\hypertarget{implemented-by-the-ebal-package}{%
\section{Implemented by the ebal
Package}\label{implemented-by-the-ebal-package}}

\hypertarget{simulated-data}{%
\subsection{Simulated Data}\label{simulated-data}}

First, we'll demonstrate entropy balancing in a logistic regression
setting to estimate the race effect on the simulated data. This data
contains the same number of observations as the PCS data on the same
variables which have been simulated independently, so a priori we should
expect the marginal means of the variables to be the same for ``Black''
and ``Non-Black'' simulated defendants. The simulated data set is
complete.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ebal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##
## ebal Package: Implements Entropy Balancing.
\end{verbatim}

\begin{verbatim}
## See http://www.stanford.edu/~jhain/ for additional information.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    filter, lag
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../Data/simulated\_data.csv"}\NormalTok{)}

\NormalTok{dat}\SpecialCharTok{$}\NormalTok{YEAR }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{YEAR)}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{COUNTY }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{COUNTY)}

\NormalTok{dat}\SpecialCharTok{$}\NormalTok{OFF\_RACER }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{OFF\_RACER, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"WHITE"}\NormalTok{, }\StringTok{"BLACK"}\NormalTok{, }\StringTok{"LATINO"}\NormalTok{, }\StringTok{"OTHER"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Entropy balancing seems to be performed very quickly. Next we check the
marginal means for each of the covariates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(INCAR, YEAR, COUNTY, OFF\_RACER)) }\OtherTok{{-}\textgreater{}}\NormalTok{ X}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ X)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{treatment }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{OFF\_RACER }\SpecialCharTok{==} \StringTok{"BLACK"}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)}

\NormalTok{eb.out }\OtherTok{\textless{}{-}} \FunctionTok{ebalance}\NormalTok{(}\AttributeTok{Treatment =}\NormalTok{ treatment, }
                   \AttributeTok{X =}\NormalTok{ X, }\AttributeTok{print.level =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Iteration 1 maximum deviation is = 211656 
Iteration 2 maximum deviation is = 9074 
Converged within tolerance 
Converged within tolerance 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[treatment,], }\DecValTok{2}\NormalTok{, mean)}


\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[}\SpecialCharTok{!}\NormalTok{treatment,], }\DecValTok{2}\NormalTok{, weighted.mean, }\AttributeTok{w =}\NormalTok{ eb.out}\SpecialCharTok{$}\NormalTok{w)}


\NormalTok{c3 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[}\SpecialCharTok{!}\NormalTok{treatment,], }\DecValTok{2}\NormalTok{, mean)}

\NormalTok{X.means }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(c1, c2, c3) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.data.frame}\NormalTok{()}
\FunctionTok{rownames}\NormalTok{(X.means) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\StringTok{"Non{-}Black {-} EB"}\NormalTok{, }\StringTok{"Non{-}Black {-} Unbalanced"}\NormalTok{)}
\FunctionTok{round}\NormalTok{(}\FunctionTok{t}\NormalTok{(X.means), }\DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    kableExtra}\SpecialCharTok{::}\FunctionTok{kbl}\NormalTok{(}\AttributeTok{format =} \StringTok{"latex"}\NormalTok{,}
                    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{digits =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

\begin{tabular}[t]{lrrr}
\toprule
  & Black & Non-Black - EB & Non-Black - Unbalanced\\
\midrule
CRIMETYPEDUI & 0.237 & 0.237 & 0.238\\
CRIMETYPEOTHER & 0.124 & 0.124 & 0.123\\
CRIMETYPEPERSONS & 0.156 & 0.156 & 0.157\\
CRIMETYPEPROPERTY & 0.256 & 0.256 & 0.255\\
OGS & 3.428 & 3.428 & 3.418\\
\addlinespace
TRIALYes & 0.978 & 0.978 & 0.979\\
MALEMale & 0.773 & 0.773 & 0.774\\
PRS4/5 & 0.181 & 0.181 & 0.182\\
PRSNone & 0.486 & 0.486 & 0.486\\
PRSREVOC/RFEL & 0.030 & 0.030 & 0.030\\
\addlinespace
DOSAGE & 34.267 & 34.267 & 34.276\\
RECMINYes & 0.670 & 0.670 & 0.671\\
OGSQ & 17.944 & 17.944 & 17.850\\
DOSAGEQ & 1306.290 & 1306.290 & 1307.200\\
\bottomrule
\end{tabular}
\end{verbatim}

The means are essentially the same between the covariates before
balancing, so it won't have any effect on the actual analysis. This is a
key point regarding use of the modal approach in the simulations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eb.out}\SpecialCharTok{$}\NormalTok{w), }\StringTok{"Std. Dev"} \OtherTok{=} \FunctionTok{sd}\NormalTok{(eb.out}\SpecialCharTok{$}\NormalTok{w))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. 
0.353880254 0.366262566 0.368087992 0.368239110 0.369965186 0.385803692 
   Std. Dev 
0.002779261 
\end{verbatim}

The weights are essentially uniform still indicating little needs to be
done to balance the covariates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(dat))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(dat)) \{}
    \ControlFlowTok{if}\NormalTok{ (treatment[i]) \{}
\NormalTok{        weights[i] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{    \}}
    \ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        weights[i] }\OtherTok{\textless{}{-}}\NormalTok{ eb.out}\SpecialCharTok{$}\NormalTok{w[i]}
\NormalTok{    \}}
\NormalTok{\}}

\NormalTok{ptime }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{(\{}
\NormalTok{    fit\_sim\_unweighted }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{glm}\NormalTok{(INCAR }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                              \AttributeTok{data =}\NormalTok{ dat, }
                              \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)))}
    
\NormalTok{    fit\_sim\_weighted }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{glm}\NormalTok{(INCAR }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                              \AttributeTok{data =}\NormalTok{ dat, }
                              \AttributeTok{weights =}\NormalTok{ weights,}
                              \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)))}
\NormalTok{\})[}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in eval(family$initialize): non-integer #successes in a binomial glm!
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(ptime)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
elapsed 
 41.589 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Unweighted Race Effect Estimate:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Unweighted Race Effect Estimate:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_sim\_unweighted}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"OFF\_RACERBLACK"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      z value     Pr(>|z|) 
 0.225326688  0.005659322 39.815138611  0.000000000 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Weighted Race Effect Estimate:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Weighted Race Effect Estimate:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_sim\_weighted}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"OFF\_RACERBLACK"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Estimate    Std. Error       z value      Pr(>|z|) 
 2.246423e-01  7.484070e-03  3.001606e+01 6.057134e-198 
\end{verbatim}

\hypertarget{real-pcs-data}{%
\subsection{Real PCS Data}\label{real-pcs-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcs }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../Data/most\_serious\_sentence\_2010{-}2019\_slim.csv"}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{7822}\NormalTok{)}
\NormalTok{pcs }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(COUNTY) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# sample\_n(size = 300) \%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{YEAR =} \FunctionTok{as.factor}\NormalTok{(YEAR),}
        \AttributeTok{COUNTY =} \FunctionTok{as.factor}\NormalTok{(COUNTY),}
        \AttributeTok{OFF\_RACER =} \FunctionTok{factor}\NormalTok{(OFF\_RACER, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"WHITE"}\NormalTok{, }\StringTok{"BLACK"}\NormalTok{, }\StringTok{"LATINO"}\NormalTok{, }\StringTok{"OTHER"}\NormalTok{))}
\NormalTok{        ) }\OtherTok{{-}\textgreater{}}\NormalTok{ pcs.sub}

\NormalTok{mice}\SpecialCharTok{::}\FunctionTok{md.pattern}\NormalTok{(pcs.sub, }\AttributeTok{rotate.names =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{entropy_balancing_files/figure-pdf/PCS-set-up-1.pdf}

}

\end{figure}

\begin{verbatim}
       INCAR CRIMETYPE OGS OGSQ TRIAL MALE COUNTY YEAR PRS DOSAGE DOSAGEQ
834546     1         1   1    1     1    1      1    1   1      1       1
26206      1         1   1    1     1    1      1    1   1      1       1
2076       1         1   1    1     1    1      1    1   1      1       1
34         1         1   1    1     1    1      1    1   1      1       1
1465       1         1   1    1     1    1      1    1   1      0       0
92         1         1   1    1     1    1      1    1   1      0       0
3          1         1   1    1     1    1      1    1   0      1       1
           0         0   0    0     0    0      0    0   3   1557    1557
       RECMIN OFF_RACER      
834546      1         1     0
26206       1         0     1
2076        0         1     1
34          0         0     2
1465        1         1     2
92          1         0     3
3           0         1     2
         2113     26332 31562
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcs.sub }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(OFF\_RACER),}
           \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(DOSAGE),}
           \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(RECMIN),}
           \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(PRS)) }\OtherTok{{-}\textgreater{}}\NormalTok{ pcs.cc}
\NormalTok{pcs.cc }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(INCAR, YEAR, COUNTY, OFF\_RACER)) }\OtherTok{{-}\textgreater{}}\NormalTok{ X}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ X)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{treatment }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(pcs.cc}\SpecialCharTok{$}\NormalTok{OFF\_RACER }\SpecialCharTok{==} \StringTok{"BLACK"}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)}

\NormalTok{eb.pcs }\OtherTok{\textless{}{-}} \FunctionTok{ebalance}\NormalTok{(}\AttributeTok{Treatment =}\NormalTok{ treatment, }
                   \AttributeTok{X =}\NormalTok{ X, }\AttributeTok{print.level =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Iteration 1 maximum deviation is = 28887568 
Iteration 2 maximum deviation is = 26506872 
Iteration 3 maximum deviation is = 23179050 
Iteration 4 maximum deviation is = 18192459 
Iteration 5 maximum deviation is = 8447031 
Iteration 6 maximum deviation is = 222757 
Iteration 7 maximum deviation is = 315.2 
Converged within tolerance 
Converged within tolerance 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[treatment,], }\DecValTok{2}\NormalTok{, mean)}


\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[}\SpecialCharTok{!}\NormalTok{treatment,], }\DecValTok{2}\NormalTok{, weighted.mean, }\AttributeTok{w =}\NormalTok{ eb.pcs}\SpecialCharTok{$}\NormalTok{w)}


\NormalTok{c3 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[}\SpecialCharTok{!}\NormalTok{treatment,], }\DecValTok{2}\NormalTok{, mean)}

\NormalTok{X.means }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(c1, c2, c3) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.data.frame}\NormalTok{()}
\FunctionTok{rownames}\NormalTok{(X.means) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\StringTok{"Non{-}Black {-} EB"}\NormalTok{, }\StringTok{"Non{-}Black {-} Unbalanced"}\NormalTok{)}
\FunctionTok{round}\NormalTok{(}\FunctionTok{t}\NormalTok{(X.means), }\DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    kableExtra}\SpecialCharTok{::}\FunctionTok{kbl}\NormalTok{(}\AttributeTok{format =} \StringTok{"latex"}\NormalTok{,}
                    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{digits =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

\begin{tabular}[t]{lrrr}
\toprule
  & Black & Non-Black - EB & Non-Black - Unbalanced\\
\midrule
CRIMETYPEDUI & 0.127 & 0.127 & 0.279\\
CRIMETYPEOTHER & 0.163 & 0.163 & 0.108\\
CRIMETYPEPERSONS & 0.184 & 0.184 & 0.146\\
CRIMETYPEPROPERTY & 0.244 & 0.244 & 0.259\\
OGS & 4.060 & 4.060 & 3.183\\
\addlinespace
OGSQ & 24.727 & 24.727 & 15.321\\
RECMINYes & 0.530 & 0.530 & 0.720\\
TRIALYes & 0.959 & 0.959 & 0.986\\
PRS4/5 & 0.272 & 0.272 & 0.152\\
PRSNone & 0.386 & 0.386 & 0.515\\
\addlinespace
PRSREVOC/RFEL & 0.051 & 0.051 & 0.023\\
MALEMale & 0.827 & 0.827 & 0.754\\
DOSAGE & 32.912 & 32.912 & 34.789\\
DOSAGEQ & 1213.208 & 1213.208 & 1342.119\\
\bottomrule
\end{tabular}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eb.pcs}\SpecialCharTok{$}\NormalTok{w), }\StringTok{"Std. Dev"} \OtherTok{=} \FunctionTok{sd}\NormalTok{(eb.pcs}\SpecialCharTok{$}\NormalTok{w))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. 
 0.07609185  0.18832187  0.29698135  0.36708400  0.45848952 16.65560610 
   Std. Dev 
 0.28137031 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(pcs.cc))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(pcs.cc)) \{}
    \ControlFlowTok{if}\NormalTok{ (treatment[i]) \{}
\NormalTok{        weights[i] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{    \}}
    \ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        weights[i] }\OtherTok{\textless{}{-}}\NormalTok{ eb.pcs}\SpecialCharTok{$}\NormalTok{w[i]}
\NormalTok{    \}}
\NormalTok{\}}

\NormalTok{fit\_pcs\_unweighted }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{glm}\NormalTok{(INCAR }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                          \AttributeTok{data =}\NormalTok{ pcs.cc, }
                          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                          \AttributeTok{model =} \ConstantTok{FALSE}\NormalTok{, }
                          \AttributeTok{y =} \ConstantTok{FALSE}\NormalTok{))}

\NormalTok{fit\_pcs\_weighted }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{glm}\NormalTok{(INCAR }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                          \AttributeTok{data =}\NormalTok{ pcs.cc, }
                          \AttributeTok{weights =}\NormalTok{ weights,}
                          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                          \AttributeTok{model =} \ConstantTok{FALSE}\NormalTok{, }
                          \AttributeTok{y =} \ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in eval(family$initialize): non-integer #successes in a binomial glm!
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Unweighted Race Effect Estimate:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Unweighted Race Effect Estimate:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_pcs\_unweighted}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"OFF\_RACERBLACK"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      z value     Pr(>|z|) 
3.332389e-01 5.358802e-02 6.218533e+00 5.018254e-10 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Weighted Race Effect Estimate:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Weighted Race Effect Estimate:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_pcs\_weighted}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"OFF\_RACERBLACK"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      z value     Pr(>|z|) 
0.3464378239 0.0799957463 4.3307030664 0.0000148634 
\end{verbatim}

\hypertarget{incomplete-data}{%
\subsection{Incomplete Data}\label{incomplete-data}}

The impact of incomplete data is the next topic to tackle. We know that
CCA can be biased under several (untestable) assumptions for the
missingness mechanisms for logistic regression. Need to show that
missingness also has an impact on weights doubly impacting the
inferential results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{9} \SpecialCharTok{+}\NormalTok{ .}\DecValTok{5} \SpecialCharTok{*}\NormalTok{ pcs.cc}\SpecialCharTok{$}\NormalTok{DOSAGE }\SpecialCharTok{+}\NormalTok{ .}\DecValTok{5} \SpecialCharTok{*}\NormalTok{ pcs.cc}\SpecialCharTok{$}\NormalTok{OGS)))}\SpecialCharTok{\^{}{-}}\DecValTok{1}
\FunctionTok{boxplot}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{entropy_balancing_files/figure-pdf/missing-model-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12948}\NormalTok{)}
\NormalTok{mis }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(pcs.cc), }\AttributeTok{size =} \FunctionTok{ceiling}\NormalTok{(}\FloatTok{0.01} \SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(pcs.cc)), }\AttributeTok{prob =}\NormalTok{ p)}
\FunctionTok{str}\NormalTok{(mis)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 int [1:194] 11771 6050 2603 15054 2490 11909 13617 2250 14735 12330 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcs.inc }\OtherTok{\textless{}{-}}\NormalTok{ pcs.cc}
\NormalTok{pcs.inc[mis, }\StringTok{"OFF\_RACER"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}


\NormalTok{pcs.inc }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(OFF\_RACER),}
           \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(DOSAGE),}
           \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(RECMIN),}
           \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(PRS)) }\OtherTok{{-}\textgreater{}}\NormalTok{ pcs.cc2}
\NormalTok{pcs.cc2 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(INCAR, YEAR, COUNTY, OFF\_RACER)) }\OtherTok{{-}\textgreater{}}\NormalTok{ X}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ X)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{treatment }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(pcs.cc2}\SpecialCharTok{$}\NormalTok{OFF\_RACER }\SpecialCharTok{==} \StringTok{"BLACK"}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)}

\NormalTok{eb.pcs2 }\OtherTok{\textless{}{-}} \FunctionTok{ebalance}\NormalTok{(}\AttributeTok{Treatment =}\NormalTok{ treatment, }
                   \AttributeTok{X =}\NormalTok{ X, }\AttributeTok{print.level =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Iteration 1 maximum deviation is = 28904124 
Iteration 2 maximum deviation is = 26520140 
Iteration 3 maximum deviation is = 23186669 
Iteration 4 maximum deviation is = 18189093 
Iteration 5 maximum deviation is = 8411645 
Iteration 6 maximum deviation is = 221142 
Iteration 7 maximum deviation is = 310.9 
Converged within tolerance 
Converged within tolerance 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[treatment,], }\DecValTok{2}\NormalTok{, mean)}


\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[}\SpecialCharTok{!}\NormalTok{treatment,], }\DecValTok{2}\NormalTok{, weighted.mean, }\AttributeTok{w =}\NormalTok{ eb.pcs2}\SpecialCharTok{$}\NormalTok{w)}


\NormalTok{c3 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X[}\SpecialCharTok{!}\NormalTok{treatment,], }\DecValTok{2}\NormalTok{, mean)}

\NormalTok{X.means }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(c1, c2, c3) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.data.frame}\NormalTok{()}
\FunctionTok{rownames}\NormalTok{(X.means) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\StringTok{"Non{-}Black {-} EB"}\NormalTok{, }\StringTok{"Non{-}Black {-} Unbalanced"}\NormalTok{)}
\FunctionTok{round}\NormalTok{(}\FunctionTok{t}\NormalTok{(X.means), }\DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    kableExtra}\SpecialCharTok{::}\FunctionTok{kbl}\NormalTok{(}\AttributeTok{format =} \StringTok{"latex"}\NormalTok{,}
                    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{digits =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

\begin{tabular}[t]{lrrr}
\toprule
  & Black & Non-Black - EB & Non-Black - Unbalanced\\
\midrule
CRIMETYPEDUI & 0.127 & 0.127 & 0.279\\
CRIMETYPEOTHER & 0.163 & 0.163 & 0.108\\
CRIMETYPEPERSONS & 0.184 & 0.184 & 0.146\\
CRIMETYPEPROPERTY & 0.244 & 0.244 & 0.259\\
OGS & 4.060 & 4.060 & 3.183\\
\addlinespace
OGSQ & 24.726 & 24.726 & 15.321\\
RECMINYes & 0.530 & 0.530 & 0.720\\
TRIALYes & 0.959 & 0.959 & 0.986\\
PRS4/5 & 0.272 & 0.272 & 0.152\\
PRSNone & 0.386 & 0.386 & 0.515\\
\addlinespace
PRSREVOC/RFEL & 0.051 & 0.051 & 0.023\\
MALEMale & 0.827 & 0.827 & 0.754\\
DOSAGE & 32.909 & 32.909 & 34.787\\
DOSAGEQ & 1212.969 & 1212.969 & 1341.995\\
\bottomrule
\end{tabular}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eb.pcs2}\SpecialCharTok{$}\NormalTok{w), }\StringTok{"Std. Dev"} \OtherTok{=} \FunctionTok{sd}\NormalTok{(eb.pcs2}\SpecialCharTok{$}\NormalTok{w))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. 
 0.07608166  0.18827935  0.29694217  0.36704165  0.45845299 16.66036196 
   Std. Dev 
 0.28137440 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights2 }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(pcs.cc2))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(pcs.cc2)) \{}
    \ControlFlowTok{if}\NormalTok{ (treatment[i]) \{}
\NormalTok{        weights2[i] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{    \}}
    \ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        weights2[i] }\OtherTok{\textless{}{-}}\NormalTok{ eb.pcs2}\SpecialCharTok{$}\NormalTok{w[i]}
\NormalTok{    \}}
\NormalTok{\}}

\NormalTok{fit\_pcs\_unweighted2 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{glm}\NormalTok{(INCAR }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                          \AttributeTok{data =}\NormalTok{ pcs.cc2, }
                          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                          \AttributeTok{model =} \ConstantTok{FALSE}\NormalTok{, }
                          \AttributeTok{y =} \ConstantTok{FALSE}\NormalTok{))}

\NormalTok{fit\_pcs\_weighted2 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{glm}\NormalTok{(INCAR }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                          \AttributeTok{data =}\NormalTok{ pcs.cc2, }
                          \AttributeTok{weights =}\NormalTok{ weights2,}
                          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                          \AttributeTok{model =} \ConstantTok{FALSE}\NormalTok{, }
                          \AttributeTok{y =} \ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in eval(family$initialize): non-integer #successes in a binomial glm!
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Unweighted Race Effect Estimate with Incomplete Data:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Unweighted Race Effect Estimate with Incomplete Data:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_pcs\_unweighted2}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"OFF\_RACERBLACK"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      z value     Pr(>|z|) 
3.259340e-01 5.390277e-02 6.046702e+00 1.478410e-09 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Weighted Race Effect Estimate with Incomplete Data:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Weighted Race Effect Estimate with Incomplete Data:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_pcs\_weighted2}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"OFF\_RACERBLACK"}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      z value     Pr(>|z|) 
3.640781e-01 8.093915e-02 4.498171e+00 6.854065e-06 
\end{verbatim}

\hypertarget{implemented-by-the-psweight-package}{%
\section{Implemented by the PSweight
Package}\label{implemented-by-the-psweight-package}}

Following the
\href{https://cran.r-project.org/web/packages/PSweight/vignettes/vignette.pdf}{vignette}
from the PSweight package, I will implement the estimation of the ATE,
ATO, and ATT by using overlap weights (OW) and entropy balancing (EB).
The vignette includes formulae for how to calculate causal estimands
like the ATE, ATO, ATT, ATEN and how those estimands relate to the
choice of weights. They also note that the entropy balancing has no
theoretical foundation is not as frequently used as inverse propensity
weighting (IPW) or OW.

Following the steps in this framework, I will obtain causal estimands
and be able to demonstrate that these inferences are also subject to the
influence of missingness.

\hypertarget{binary-treatment}{%
\subsection{Binary Treatment}\label{binary-treatment}}

As an initial step, I'll reduce the comparison to just White and Black
defendants being Black set to be the ``treatment''. Later I will
consider the same analysis with multiple treatment levels where White is
the control and Black, Latino, and Other are treatments.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(PSweight)}
\CommentTok{\# }
\CommentTok{\# pcs.cc \%\textgreater{}\% }
\CommentTok{\#     filter(OFF\_RACER \%in\% c("WHITE", "BLACK")) \%\textgreater{}\% }
\CommentTok{\#     mutate(BLACK = ifelse(OFF\_RACER == "BLACK", 1, 0)) \%\textgreater{}\% }
\CommentTok{\#     select({-}c("OFF\_RACER")) {-}\textgreater{} pcs.bw}
\CommentTok{\# }
\CommentTok{\# ps.any \textless{}{-} BLACK \textasciitilde{} OGS + OGSQ + as.factor(CRIMETYPE) + as.factor(RECMIN) + as.factor(TRIAL) + as.factor(PRS) + as.factor(MALE) + DOSAGE + DOSAGEQ}
\CommentTok{\# }
\CommentTok{\# bal.any \textless{}{-} SumStat(ps.formula = ps.any, data = pcs.bw,}
\CommentTok{\#                    weight = c("IPW", "overlap", "treated", "entropy"))}
\CommentTok{\# bal.any}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot(bal.any, type = "density")}
\CommentTok{\# plot(bal.any, type = "balance", metric = "PSD")}
\end{Highlighting}
\end{Shaded}

Now we continue to the analysis step, first without including
confounders.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ate.any \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                     yname = "INCAR", }
\CommentTok{\#                     data = pcs.bw,}
\CommentTok{\#                     weight = "IPW",}
\CommentTok{\#                     family = "binomial")}
\CommentTok{\# }
\CommentTok{\# att.any \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                     yname = "INCAR", }
\CommentTok{\#                     data = pcs.bw,}
\CommentTok{\#                     weight = "treated",}
\CommentTok{\#                     family = "binomial")}
\CommentTok{\# }
\CommentTok{\# ato.any \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                     yname = "INCAR", }
\CommentTok{\#                     data = pcs.bw,}
\CommentTok{\#                     weight = "overlap",}
\CommentTok{\#                     family = "binomial")}
\CommentTok{\# }
\CommentTok{\# aten.any \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                     yname = "INCAR", }
\CommentTok{\#                     data = pcs.bw,}
\CommentTok{\#                     weight = "entropy",}
\CommentTok{\#                     family = "binomial")}
\CommentTok{\# }
\CommentTok{\# summary(ate.any, type = "OR")}
\CommentTok{\# summary(att.any, type = "OR")}
\CommentTok{\# summary(ato.any, type = "OR")}
\CommentTok{\# summary(aten.any, type = "OR")}
\end{Highlighting}
\end{Shaded}

And including confounders.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# out.incar \textless{}{-} INCAR \textasciitilde{} as.factor(CRIMETYPE) + OGS + OGSQ + as.factor(RECMIN) + as.factor(TRIAL) + as.factor(PRS) + as.factor(MALE) + DOSAGE + DOSAGEQ + as.factor(YEAR) + as.factor(COUNTY)}
\CommentTok{\# }
\CommentTok{\# ate.any.aug \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                         out.formula = out.incar,}
\CommentTok{\#                         augmentation = TRUE,}
\CommentTok{\#                         yname = "INCAR", }
\CommentTok{\#                         data = pcs.sub,}
\CommentTok{\#                         weight = "IPW",}
\CommentTok{\#                         family = "binomial")}
\CommentTok{\# }
\CommentTok{\# summary(ate.any.aug, type = "OR")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ato.any.aug \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                         out.formula = out.incar,}
\CommentTok{\#                         augmentation = TRUE,}
\CommentTok{\#                         yname = "INCAR", }
\CommentTok{\#                         data = pcs.bw[s,],}
\CommentTok{\#                         weight = "overlap",}
\CommentTok{\#                         family = "binomial")}
\CommentTok{\# }
\CommentTok{\# summary(ato.any.aug, type = "OR")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# aten.any.aug \textless{}{-} PSweight(ps.formula = ps.any, }
\CommentTok{\#                         out.formula = out.incar,}
\CommentTok{\#                         augmentation = TRUE,}
\CommentTok{\#                         yname = "INCAR", }
\CommentTok{\#                         data = pcs.bw[s,],}
\CommentTok{\#                         weight = "entropy",}
\CommentTok{\#                         family = "binomial")}
\CommentTok{\# }
\CommentTok{\# summary(aten.any.aug, type = "OR")}
\end{Highlighting}
\end{Shaded}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-berkWhatYouCan2010}{}}%
Berk R (2010) What {You Can} and {Can}'t {Properly Do} with
{Regression}. Journal of Quantitative Criminology 26:481--487.
\url{https://doi.org/10.1007/s10940-010-9116-4}

\leavevmode\vadjust pre{\hypertarget{ref-macdonaldEvaluatingRoleRace2019}{}}%
MacDonald JM, Donnelly EA (2019) Evaluating the {Role} of {Race} in
{Sentencing}: {An Entropy Weighting Analysis}. Justice Quarterly
36:656--681. \url{https://doi.org/10.1080/07418825.2017.1415368}

\end{CSLReferences}



\end{document}
