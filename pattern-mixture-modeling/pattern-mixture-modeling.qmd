---
title: "Multiple Imputation and Pattern-mixture Modeling to Assess the Sensitivity of Race/Ethnicity Effect Estimates Given Incomplete Data"
author: 
    - name: C. Clare Strange
      email: cs3846@drexel.edu
      affiliations:
          - name: Department of Criminology and Justice Studies, Drexel University
    - name: Benjamin Stockton
      email: benjamin.stockton@uconn.edu
      orcid: 0000-0002-3820-5293
      affiliations: 
          - name: Department of Statistics, University of Connecticut
    - name: Jordan Zvonkovich
      email: jtz5015@psu.edu
      affiliations:
        - name: Department of Criminology, Penn State University
    - name: Ofer Harel
      affiliations: 
          - name: Department of Statistics, University of Connecticut
date: today
number-sections: false 
number-depth: 2
toc: false
format:
  pdf:
    keep-tex: true
  docx: 
    number-sections: true
  html: 
    notebook-view: false
execute: 
  cache: true
  echo: false
  warning: false
bibliography: ../Literature/Criminology.bib
csl: apa.csl
fig-height: 5
fig-width: 8
abstract: "Incomplete data are common in criminological research. The treatment of incomplete data may alter effect estimates, including the observed impacts of race/ethnicity on an outcome of interest. Knowing the degree of certainty and robustness of effects is paramount importance considering such research is often cited in policy discussions. We assess the impacts of race/ethnicity on sentencing using data from Pennsylvania’s Court of Common Pleas (2010 – 2019, N = ???). In doing so, we present a novel strategy for sensitivity testing given incomplete race/ethnicity data: multiple imputation in combination with pattern-mixture models (PMMs). PMMs were developed for use with incomplete data with non-ignorable missingness and are particularly applicable to criminological research. Our results suggest that Black offenders receive longer jail/prison sentences [Ben – more detail here?], and that race/ethnicity effect estimates are relatively robust against violations of the MAR assumption on the race or ethnicity of offenders. We conclude that multiple imputation in combination with pattern mixture models are a useful tool for criminologists to assess the sensitivity of any effect of interest, and can be applied in any criminological research context in which non-ignorable missingness is a concern."
keywords: pattern-mixture/shared parameter modeling, incomplete data, racial/ethnic disparities, sentencing
editor: visual
---

```{r}
#| label: set-up
library(dplyr, warn.conflicts = FALSE, quietly = TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly = TRUE)
library(mice, warn.conflicts = FALSE, quietly = TRUE)
library(GLMMadaptive)
library(broom.mixed)
library(brms)
library(VIM)
library(bayesplot)

theme_set(theme_bw())
color_scheme_set("brightblue")
```

## Introduction and Background

Criminal justice data are often “incomplete,” or have missing observations. Incomplete data can stem from several, sometimes interconnected sources, including (but not limited to) sample attrition (e.g., observations exiting a study over time), selection (e.g., certain individuals being more likely included in a study or an analysis), and lapses in administrative data entry or recordkeeping (e.g., failure to consistently record and report information). Regardless of the source, the pattern of incomplete data will take one of several forms: missing not at random (MNAR; otherwise known as nonignorable missingness), missing at random (MAR), or missing completely at random (MCAR), which has implications for the appropriate assumptions and method of treatment (Rubin, 1987).

Discussed in depth elsewhere (see Stockton et al., 2023), administrative criminal justice data have differing rates of completeness, with larger ones tending to have higher rates (e.g., Federal or state-level databases) than smaller, more localized ones. As demonstrated in a data simulation by Stockton and colleagues (2023), the rate, pattern, and treatment of incomplete data (even in moderate or small quantities) can meaningfully alter effect estimates. Without appropriate treatment, the analysis of incomplete data may lead to biased model estimates and contribute to inaccurate conclusions about an effect of interest (Breen, 2015; Bushway et al., 2007).

The treatment of incomplete data and the robustness of effect estimates are longstanding concerns of the criminological community. Brame & Paternoster (2003) noted over two decades ago the importance of these concerns, which have since been addressed in various ways (see, e.g., the use of minimal assumptions and middle-ground estimators by Brame et al. 2014). Despite these advancements, however, criminologists have failed to consistently treat incomplete data and assess and report the robustness of their findings (Stockton et al., 2023). The latter is paramount for understanding the potential impact on effect estimates if our assumptions about incomplete data do not hold.

Given the prevalence, inconsistent treatment, and potential impacts of incomplete data on effect estimates, we argue that sensitivity testing should be utilized to assess the robustness of an effect of interest in any analysis of incomplete data with nonignorable missingness. In the following sections, we briefly review the circumstances in which incomplete criminal justice data are common and the methods by which criminologists have treated incomplete data. We consider the potential impacts of the treatment of incomplete data on research estimates, with a substantive focus on the measurement of race/ethnicity effects as a timely and relevant concern for criminologists, and in line with the current analyses. We then review the limited ways in which criminologists have assessed the sensitivity of race/ethnicity effects given incomplete data and present a novel strategy: multiple imputation (MI) with pattern-mixture modeling (PMM). We highlight that MI and PMM can be used to assess the robustness of effects given incomplete data of any assumed pattern (i.e., MAR, MNAR) and source (e.g., attrition, selection), making it highly applicable and flexible to accommodate common missingness issues faced by criminal justice researchers.

### Common Sources of Incomplete Data in Criminological Research

Criminal justice data may be incomplete for one or more unique reasons—study attrition, sample selection, or inconsistent administrative reporting and data entry practices are common examples. Missingness due to study attrition may occur in a longitudinal data set due to relocation, death, loss of contact, or disinterest in continuing. Kim and Bushway (2018) provide an example of this in their examination of the age-crime relationship using data from the National Longitudinal Survey of Youth \[NLSY\]). They note the potential impacts of sample attrition on their results if the observed dropout rates were more prevalent among those with a higher probability of criminal involvement. To assess this potential threat to validity they examined patterns of missingness among the cohorts and found that, although their exploration did indicate the possibility of selective sample attrition for those with a higher risk of offending, these patterns differed by cohort and did not present a clear indication that incomplete data biased their results in a meaningful way.

Missingness due to selection may occur when individuals have a differential likelihood of inclusion in a study or sample (Heckman, 1976). In policing research, only those individuals who have been stopped by the police have the opportunity to be searched, arrested, or to experience a use-of-force incident. In courts and sentencing research, only individuals who make it through the stages of case processing will receive a final disposition and corresponding sentence length. Studies demonstrate that not all citizens are equally likely to be stopped by the police or to complete all stages of case processing (see Bowling & Phillips, 2007; Kadane & Lamberth, 2009; Kutateladze et al., 2016; Pierson et al., 2020; TenEyck et al., 2024), therefore policing and court data are poised to be incomplete with nonignorable missingness due to sample selection. Gaebler and colleagues (2022) provide a useful causal framework that expressly acknowledges that prosecutorial charging decisions in part rely on people being stopped and arrested in the first place (an area of racial/ethnic disparity), note the importance of considering this from both a theoretical and measurement perspective, and describe an estimand, the “sateM” (i.e., second-stage sample average treatment effect) that can be used to measure discrimination in the second stage of a two-stage decision-making process while accounting for potential discrimination in the first stage.

Missingness due to inconsistent administrative reporting or data entry may occur in studies using data sets that are compiled across multiple independent agencies, such as that from the Uniform Crime Reports (UCR). The UCR relies on the voluntary reporting of crime statistics by over 18,000 law enforcement agencies across the U.S. It is widely understood that such data sets are incomplete as reporting practices are known to vary between agencies and within agencies over time. There have been multiple efforts to improve UCR data and address differential reporting, including those of the National Archive of Criminal Justice Data (NACJD), which involve imputation using county-level measures available through their platform. DeLang and colleagues (2022) highlighted and compared multiple methods of addressing differential reporting in the UCR, including imputation using agencies’ past reporting, multivariate normal imputation (MVNI), and multiple imputation by chained equations (MICE). They demonstrate that multiple imputation is the superior approach because it reduces uncertainty in imputation and produces more accurate standard errors.

### The Treatment of Incomplete Sentencing Data with Nonignorable Missingness

Multiple imputation remains the “gold standard” treatment when MCAR or MAR is the assumed missingness mechanism (Demuth & Steffensmeier, 2004; DeLang et al., 2022; Jordan & Freiburger, 2010). We focus on the treatment of incomplete sentencing data with nonignorable missingness given its relevance to the current analyses.

Complete case analysis (CCA) is a common approach to analyzing incomplete sentencing data (regardless of whether they are assumed MCAR), particularly when the proportion of missing data is considered to be small (see, e.g., Bales & Piquero, 2012; Cassidy & Rydberg, 2020; Johnson et al., 2023; Kutateladze et al., 2014; Light, 2022; Ramos, 2023; Zane et al., 2022). It should be noted, however, that what constitutes a “small” amount of missing data remains unclear (as noted by Brame & Paternoster, 2003). Missingness as low as five percent has been shown to “flip” the direction of effect estimates even in the context of a randomized controlled trial (RCT; Belin, 2009; Graham, 2009).

When data are incomplete due to sample selection (a form of nonignorable missingness), scholars will often employ latent variable models to reduce the degree of bias in estimation (e.g., the Heckman correction or Tobit estimators, see Bärnighausen et al., 2011; Heckman, 1976; Johnson, 2014; Kutateladze et al., 2014; Saha et al., 1997; Stolzenberg et al., 2013). Such models aim to explain the relationships between observed variables (e.g., race and sentence length) by positing the existence of unobserved (or latent) variables which are inferred from the data. Using the two-stage Heckman correction in an analysis of sentence length as an example, the first stage involves estimating the probability of selection into the incarceration sample (the “selection equation”) and using the estimates from this model to calculate the inverse Mills (IM) ratio. The second stage calls for the inclusion of the IM ratio as a control variable to correct for selection bias in the outcome equation modeling sentence length (Heckman, 1976). Importantly, Bushway and colleagues (2007) illustrated that bias may remain a concern even in studies in which the Heckman correction is employed. Many sentencing scholars forego latent variable models altogether for justifiable reasons (e.g., lacking appropriate selection criteria, problematic collinearity between selection criteria and outcome variables), simply noting the potential bias from sample selection as a limitation.

### The Potential Impacts of the Treatment of Incomplete Data on Race/Ethnicity Effect Estimates

It has long been argued that the treatment of incomplete data may alter estimates of the race/ethnicity effect on justice system outcomes (see Baumer, 2013; Bushway et al, 2007; Kadane & Lamberth 2009; Knox et al., 2020; Stockton et al., 2023). We maintain this focus on race/ethnicity as a timely and relevant concern given recent questions as to their sustained presence and magnitude (see, e.g., Light, 2022). We define the race/ethnicity effect as “the differential treatment of people of different races \[and ethnicities\] in the justice system who are otherwise similarly situated with respect to legally relevant characteristics” (Stockton et al., 2023, p.2).

Discussed elsewhere at length (see Stockton et al., 2023), the magnitude of race/ethnicity effects can vary greatly between studies and for myriad reasons (e.g., contextual influences, bias and discrimination in justice actor discretion, the differential impact of policy, differential involvement in crime). Sentencing research has shown race/ethnicity effect estimates varying from non-significant, to small, moderate, or even large, and also often vary according to age, gender, and type of case (see Doerner & Demuth, 2010; Omori & Petersen, 2020; Ridgeway et al., 2020; Steffensmeier et al., 1998). While these differences may be attributable, in part, to study contexts and methodologies, Baumer (2013) notes that the true magnitude of race/ethnicity effects on sentencing will remain uncertain so long as sample selection bias is unaddressed in this literature. This sentiment can be extended to bias from non-ignorable missingness from any source (e.g., study attrition, reporting and data entry issues).

Within individual studies, the difference in race/ethnicity effect estimates can be stark when comparing estimates prior to and after treating bias due to nonignorable missingness. This was recently illustrated by Knox and colleagues (2020) who initially reported that around 10 percent of police uses-of-force against Black and Latino individuals were discriminatory (before bias correction). After bias correction, however, this estimate jumped to 39 percent. While this example is specific to the policing literature, it is feasible that untreated, nonignorable missingness in court and sentencing data could introduce bias and contribute to inaccurate estimates of race/ethnicity effects as well. For the sake of accurate prediction (to the greatest extent possible given data limitations), it is essential to treat nonignorable missingness and to assess the robustness of race/ethnicity effects on sentencing.

From an applied standpoint, it is important to assess the robustness of race/ethnicity effects because these estimates are often cited in demographic impact statements that are used to justify significant policy changes (The Sentencing Project, 2021). Many policymakers rely on these statements to understand the potential consequences of a given policy. For example, demographic or racial impact statements have been used during the consideration of changes to statewide sentencing policies in Minnesota (see Minnesota Sentencing Guidelines Commission, 2023), and mechanisms for creating racial impact statements have been enabled by legislation in at least nine states (The Sentencing Project, 2021).

### Assessing the Sensitivity of Race/Ethnicity Effects Given Incomplete Data

There are strong examples in which scholars have demonstrated methods to assess the robustness of race/ethnicity effects in the presence of incomplete data. Many of these examples extend from Manski’s (2003) work in econometrics, including the use of interval estimates (a range of predicted outcome values in which the true value likely appears) and bounds analysis, in which the goal is to identify the range of possible outcome values under different model parameter assumptions. This work has since been applied to the identification of treatment effects in criminological research (see Manski & Nagin, 2011)—a subject of study in which random assignment is not always possible or ethical, and any uncertainty in treatment effects need be explicit for policymakers to gain a more nuanced understanding.

Brame et al. (2014) built upon this work in proposing the use of a minimal assumptions (MA) estimator. They note its utility particularly in the social sciences in which the underlying mechanisms driving outcomes may be complex and/or poorly understood. In their study of the cumulative arrest prevalence between demographic groupings, the MA estimator used by Brame and colleagues (2014) provided a lower bound (assuming none of the individuals missing survey data were arrested) and an upper bound (assuming all individuals missing survey data were arrested) under the minimal assumptions that the survey responses were accurate and the sample was representative. While this technique is useful for considering maximum uncertainty (unlike MAR), the potentially wide bounds can make it challenging to draw definitive conclusions about an effect of interest. Recent research has employed algorithmic strategies for bounding estimates, illustrating how effects estimated under increasingly more minimal assumptions can gain credibility and lose precision (Hamilton, 2023).

In their study of racial/ethnic discrimination in policing, Knox and colleagues (2020) proposed the use of a Monte Carlo procedure to construct confidence intervals that contain both the true lower and upper bound endpoints with a specified probability (1 - α). They note, however, that these procedures rely on relatively weak assumptions (e.g., all use-of-force encounters with police are reported, race is essentially randomly assigned in encounters, there is no racial discrimination in the decision to stop civilians), which may be plausible given the volume of policing research, but less likely to hold in the examination of understudied topics. Stating assumptions explicitly, as argued by Knox et al. (2020) and others, is essential for the consumers of research to properly evaluate the plausibility and robustness of race/ethnicity effects.

## The Current Study

The goal of the current study is to determine the impact of defendant race/ethnicity on jail/prison sentence length, and to assess the robustness of effect estimates given incomplete race/ethnicity data (a common phenomenon in criminological data). In doing so we present criminological scholars with a novel method for sensitivity testing: multiple imputation and pattern-mixture modeling. This follows a recent call to perform and report sensitivity testing in the analysis of incomplete data, for which the missingness mechanisms can only be assumed (Stockton et al., 2023).

Developed by Rubin (1974) and further since (see Little, 1993, Michiels et al. 1999, Thomas et al., 2016), pattern-mixture models (PMMs) present a flexible approach to sensitivity testing and can provide insights into how the missingness is related to the observed data patterns. Instead of assuming a single mechanism for missingness, PMMs allow for different perturbations, or patterns, to be considered that are based on the observed data.

In PMM, the selection patterns are explicitly modeled, and the analysis is conducted separately for each pattern. This accounts for the fact that different subgroups of individuals may have different reasons for missing a value for race/ethnicity, and the parameters of interest (here, the race/ethnicity effect estimate) can then be estimated by combining information from all patterns. This somewhat reverses traditional criminological thought about treating incomplete data; Put simply, selection models (for example) model the outcome and then model the selection (or non-ignorable missingness) given the outcome. On the other hand, PMMs model the non-ignorable missingness and then model the outcome given the non-ignorable missingness.

There are several advantages to using PMMs for sensitivity testing. They are particularly useful when MCAR or MAR assumptions do not hold, they do not require the use of exclusion variables or data from multiple stages of case processing (which are often absent from administrative data sets), and they can be used with multilevel models. For these reasons (and others demonstrated below), we present MI and PMMs as an advantageous method of assessing the robustness of race/ethnicity effects in sentencing.

### Data

We use the Pennsylvania Commission on Sentencing’s (PCS) case-level sentencing data for individuals convicted in a Court of Common Pleas between January 1, 2010, and December 31, 2019 (N = FULL SAMPLE SIZE?). The unit of analysis if the most serious sentence in a judicial proceeding. We include all adult sentences (excluding homicide 1/2 cases, to which the sentencing guidelines in Pennsylvania do not apply1).

### Methods

We propose using multiple imputation and pattern-mixture models to perform sensitivity analyses for the race effect estimates on the jail/prison sentence length under incompleteness of the race variable. We chose to use hurdle models with a lognormal generalized linear model for the non-zero sentence lengths. Hurdle models can be used to model complex data where the dependent variable is a combination of true zeros and a continuous distribution for non-zero observations. Sentence length is one such instance of this phenomena where offenders sentenced to jail or prison time receive a sentence of $Y > 0$ days ($Y^* = Y/30$ months) while offenders sentenced to a community sentence receive a jail/prison sentence of $Y = 0$ days ($Y^* = 0$ months). Note the distinction between sentence length which would be inclusive of community and jail/prison sentence lengths, which are generally not comparable, and our dependent variable, which is solely jail/prison sentence length. When we refer to sentence length in this manuscript, we are referring specifically to jail/prison sentence length.

Hurdle regression combines selection models that determine boundary points of the dependent variable with an outcome model that determines its nonbounded values. They treat these boundary values as observed rather than censored, i.e., observations where the dependent variable is equal to one of the boundary values are not the result of an inability to observe the distribution above or below a certain point [@wooldridgeEconometricAnalysisCross2010]. As defined by @mullahySpecificationTestingModified1986, “the idea underlying the hurdle formation is that a binomial probability model governs the binary outcome of whether a count variate has a zero or a positive realization. If the realization is positive, the ‘hurdle’ is crossed, and the conditional distribution of the positives is governed by a truncated-at-zero count data model” (p.345). The hurdle model we consider treats the log of the number of days that an individual is sentenced to incarceration as a positive continuous variable and first model the zeros (i.e., individuals that do not receive incarceration sentences). A benefit to hurdle models is that they can be used in a multilevel context and are particularly appropriate when there is an over-dispersed count distribution (both of which are common in sentencing research) or when there's a mixture of a discrete variable and continuous variable [@hesterConditionalRaceDisparities2017; @rydbergPunishingWickedExamining2018; @thompsonContextualInfluencesSentencing2020].

We are defining the race effect measured by the hurdle model as the change in number of jail/prison days sentenced to between Black and White offenders. This effect considers the zeros corresponding to non-incarceration as true zeros, i.e. the offenders are considered to have served zero days in jail/prison.

The incomplete data analysis is performed in the multiple imputation framework [@rubinMultipleImputationNonresponse1987]. In this set-up the incomplete variables sentence length, race, age, recommended minimum, and previous record are imputed or filled in $M$ times using draws from a predictive distribution using a regression model. Less than 5% of observations are incomplete, with race being incomplete on 3% of cases and the other variables incomplete on less than 0.5% of cases each.

The imputations are then used to create $M$ completed data sets that are then analyzed separately using a standard complete data method, in this case a hurdle model. The estimates from each of the $M$ model fits are combined using Rubin's rules. In particular, we are going to use random forests [@wrightRangerFastImplementation2017] to perform the imputations in the multiple imputation by chained equations framework [@raghunathanMultivariateTechniqueMultiply2001; @buuren2010mice].

Pattern-mixture models can be used in conjunction with multiple imputation to perform a sensitivity analysis for the model of interest to particular perturbations of the distribution from which the imputations are drawn [@vanbuuren2018 Sec. 3.8]. This allows us to investigate the impacts nonignorable missingness could potentially have on our analysis.

The pattern-mixture model approach allows the analyst to specify the exact assumptions of the missingness model and assumptions of how the distribution varies over different patterns. In our analysis, the two patterns of interest are the offenders who have a reported race and those who do not. It is plausible that these two groups have different characteristics and that the latter group may not have a reported race because of their true racial/ethnic identity [@stockton2023]. This is a form of what's known as non-ignorable missingness.

To assess the impacts of the non-ignorable missingness on the race effects, we can use slight perturbations of the imputed race labels to get a broader view of the range of potential estimates under different distributional assumptions. Varying the distribution of imputations based on the missing data pattern is what allows us to bring pattern-mixture modeling into the fold.

### Analysis

Our analysis begins with the multiple imputation step.[^1] We create $M = 5$ completed data sets using the chained equations framework under the default MAR assumption. Then the sensitivity analysis perturbs the race/ethnicity label to generate new imputations, followed by estimating the race effects using the lognormal hurdle model.

[^1]: Code for analysis is available on GitHub at: <https://github.com/benjamin-stockton/crim-pattern-mixture> and data is available upon request.

**Dependent Variable**

The length of the jail/prison sentence modeled as a dependent variable by our hurdle models. The length is measured in days and is zero for offenders sentenced to a community sentence.

**Independent Variables**

The independent variables of interest are the offender's race and sex. Race is coded as White, Black, Latino or Other. Sex is coded as male or female.

**Legally Relevant Variables**

We also include legally relevant variables including the crime type, whether the minimum sentence was recommended, if there was a trial or plea, the offender's previous record, and the offense gravity score (OGS). Additionally, the offender's age is included in the model. OGS and age were centered and scaled before inclusion in the model.

**County-level Variable**

County-level random effects are included for both the lognormal model for non-zero lengths, and the logistic model for the incarceration decision. For each county, the average of the offense gravity score was included to provide context for the typical cases in the county as well as the proportion of cases that each county processes out of the total number of cases processed in the state of Pennsylvania.

```{r}
#| label: load-data
df <- readr::read_csv("../Data/PCS-most-serious-sentence-2010-2019-pmm.csv",
                      show_col_types = FALSE)

county_ogs <- df |>
    group_by(COUNTY) |>
    summarize(
        COUNTY_OGS = mean(OGS, na.rm = TRUE),
        NCASES = n()
    )

df <- df |> mutate(
        YEAR = as.factor(YEAR),
        INCAR = case_when(
            JP_MIN == 0 ~ 0,
            JP_MIN > 0 ~ 1,
            TRUE ~ NA
        ),
        SEX = case_when(
            MALE == 1 ~ "Male",
            MALE == 0 ~ "Female",
            TRUE ~ NA
        ),
        OFF_RACE = case_when(
            OFF_RACER == 1 ~ "WHITE",
            OFF_RACER == 2 ~ "BLACK",
            OFF_RACER == 3 ~ "LATINO",
            OFF_RACER == 4 ~ "OTHER",
            TRUE ~ NA
        ),
        PRVREC = case_when(
            PRSR == 0 ~ "0",
            PRSR == 1 ~ "1/2/3",
            PRSR == 2 ~ "4/5",
            PRSR == 3 ~ "REVOC/RFEL",
            TRUE ~ NA
        ),
        CRIME = case_when(
            CRIMETYPE == 1 ~ "Persons",
            CRIMETYPE == 2 ~ "Property",
            CRIMETYPE == 3 ~ "Drug",
            CRIMETYPE == 4 ~ "DUI",
            CRIMETYPE == 5 ~ "Other",
            TRUE ~ NA
        ),
    COUNTYTYPE = case_when(
        COUNTY %in% c("Allegheny", "Philadelphia") ~ "Urban",
        COUNTY %in% c("Chester", "Montgomery", "Berks", "Dauphin", "Bucks", "Lancaster", "York", "Delaware", "Northampton", "Luzerne", "Lackawanna", "Westmoreland", "Lehigh", "Erie") ~ "Medium",
        TRUE ~ "Rural"
    )
    ) |>
    select(
        -c(OFF_RACER, PRSR, MALE, CRIMETYPE)
    ) |>
    left_join(county_ogs, by = "COUNTY") |>
    group_by(COUNTY) |>
    # slice_sample(n = 10000) |>
    ungroup()

df$OFF_RACE <- factor(df$OFF_RACE, levels = c("WHITE", "BLACK", "LATINO", "OTHER"))
df$PCASES <- df$NCASES / sum(county_ogs$NCASES)
```

```{r}
#| label: tbl-descriptives
#| tbl-cap: Descriptive statistics on subject level.
library(gtsummary)

df |> select(
    JP_MIN,
    INCAR,
    OGS,
    OGSQ,
    RECMIN,
    TRIAL,
    PRVREC,
    CRIME,
    DOSAGE,
    DOSAGEQ,
    SEX,
    OFF_RACE,
    YEAR
) |>
    gtsummary::tbl_summary(
        label = list(JP_MIN ~ "Sentence Length (days)",
                  INCAR ~ "Incarceration Decision",
                  OGS ~ "OGS", OGSQ ~ "OGS Squared",
                  RECMIN ~ "Recommended Minimum",
                  TRIAL ~ "Trial", 
                  PRVREC ~ "Previous Record",
                  CRIME ~ "Crime Type",
                  YEAR ~ "Year",
                  DOSAGE ~ "Age", DOSAGEQ ~ "Age Squared", 
                  SEX ~ "Sex", OFF_RACE ~ "Race/Ethnicity"),
        type = list(JP_MIN ~ "continuous2",
                  INCAR ~ "categorical",
                  OGS ~ "continuous2", OGSQ ~ "continuous2",
                  RECMIN ~ "categorical",
                  TRIAL ~ "categorical", 
                  PRVREC ~ "categorical",
                  CRIME ~ "categorical",
                  YEAR ~ "categorical",
                  DOSAGE ~ "continuous2", DOSAGEQ ~ "continuous2", 
                  SEX ~ "categorical", OFF_RACE ~ "categorical"),
        statistic = list(
                all_continuous() ~ c("{mean} ({sd})", "({min}, {max})"),
                all_categorical() ~ "{n} ({p}%)"
        ),
        missing_text = "(Missing)")
```

```{r}
#| label: tbl-county-descriptives
#| tbl-cap: Descriptive statistics on county level.


df |> select(
    COUNTY_OGS,
    COUNTYTYPE,
    NCASES,
    PCASES
) |>
    gtsummary::tbl_summary(
        label = list(COUNTY_OGS ~ "Average OGS by County",
                  COUNTYTYPE ~ "County Type",
                  NCASES ~ "Number of Cases", 
                  PCASES ~ "Proportion of Cases"),
        type = list(COUNTY_OGS ~ "continuous2",
                  COUNTYTYPE ~ "categorical",
                  NCASES ~ "continuous2", 
                  PCASES ~ "continuous2"),
        statistic = list(
                all_continuous() ~ c("{mean} ({sd})", "({min}, {max})"),
                all_categorical() ~ "{n} ({p}%)"
        ),
        missing_text = "(Missing)"
    )
```

```{r}
#| label: fig-miss-pattern
#| fig-cap: Missing data patterns for the full data set. In the right panel, red cells indicate missingness.
#| include: false
df |> 
    select(
        JP_MIN, OFF_RACE, DOSAGE, RECMIN, PRVREC, CRIME, TRIAL, SEX
    ) |> 
    aggr(sortby = "JP_MIN", plot = FALSE) |>
    plot(numbers = TRUE, prop = FALSE)
```

```{r}
#| label: tbl-yearly-summary
#| tbl-cap: Summary statistics on sentence length (days) and incarceration.
#| include: false
df |>
    # filter(JP_MIN > 0) |>
    group_by(YEAR) |>
    summarize(
        mean_JP_MIN = mean(JP_MIN, na.rm = TRUE),
        sd_JP_MIN = sd(JP_MIN, na.rm = TRUE),
        min_JP_MIN = min(JP_MIN, na.rm = TRUE),
        median_JP_MIN = median(JP_MIN, na.rm = TRUE),
        max_JP_MIN = max(JP_MIN, na.rm = TRUE),
        p_INCAR = mean(INCAR, na.rm = TRUE)
    ) |>
    kableExtra::kable(format = "latex", digits = 2, escape = TRUE,
                      col.names = c("Year", "Mean", "SD", "Median", "Min.", "Max.", "P(Incar)"),
                      booktabs = TRUE, padding = 0)
```

```{r}
#| label: fig-sen-len-yearly
#| fig-cap: Density plots for A) Sentence length (day) by year. B) Log sentence lengths (day) plus one day by year.
#| include: false

p1 <- df |>
    # filter(JP_MIN > 0) |>
    ggplot(aes(YEAR, JP_MIN)) +
    geom_violin(draw_quantiles = c(0.025, 0.25, 0.5, 0.75, 0.975)) +
        facet_wrap(YEAR~., scales = "free", nrow = 5) +
    coord_flip() +
    labs(x = "") +
    theme(axis.text.y = element_blank(),
          axis.ticks.y = element_blank())
p2 <- df |>
    # filter(JP_MIN > 0) |>
    ggplot(aes(YEAR, log1p(JP_MIN))) +
    geom_violin(draw_quantiles = c(0.025, 0.25, 0.5, 0.75, 0.975)) +
        facet_wrap(YEAR~., scales = "free", nrow = 5) +
    coord_flip() +
    labs(x = "") +
    theme(axis.text.y = element_blank(),
          axis.ticks.y = element_blank())

cowplot::plot_grid(p1, p2, nrow = 1, labels = "AUTO")
```

<!-- Evaluating the sentence length by most serious crime type in @fig-sen-len-crime shows that the most serious crime type also has a strong impact on the sentence length with DUIs and Other offenses resulting in relatively short and often community sentences. Persons offenses can result in very long offenses but otherwise are similar in distribution to Drug offenses as seen in the distribution of the log sentence lengths. -->

```{r}
#| label: fig-sen-len-crime
#| fig-cap: A) Sentence length (day) by most serious crime type. B) Log of sentence length plus one day by most serious crime type.
#| fig-height: 4
#| include: false

p1 <- ggplot(df, aes(CRIME, JP_MIN)) +
    geom_violin(draw_quantiles = c(0.025, 0.25, 0.5, 0.75, 0.975)) +
    geom_point(size = 0.5)

p2 <- ggplot(df, aes(CRIME, log(JP_MIN))) +
    geom_violin(draw_quantiles = c(0.025, 0.25, 0.5, 0.75, 0.975)) +
    geom_point(size = 0.5)

cowplot::plot_grid(p1, p2, nrow = 1, labels = "AUTO")
```

<!-- The comparison of sentence lengths by offender race in @fig-sen-len-race show that there is little visual difference in the centers of the distribution for each racial group. White offenders receive the longest sentences as the result of four outliers, but otherwise have similar distributions to Black offenders. -->

```{r}
#| label: fig-sen-len-race
#| fig-cap: A) Sentence length by offender race. B) Log of sentence length (plus one day)
#| fig-height: 4
#| include: false
p1 <- ggplot(df, aes(OFF_RACE, JP_MIN)) +
    geom_violin(draw_quantiles = c(0.025, 0.25, 0.5, 0.75, 0.975)) +
    geom_point(size = 0.5, shape = 1)

p2 <- ggplot(df, aes(OFF_RACE, log(JP_MIN))) +
    geom_violin(draw_quantiles = c(0.025, 0.25, 0.5, 0.75, 0.975)) +
    geom_point(size = 0.5, shape = 1)

cowplot::plot_grid(p1, p2, nrow = 1, labels = "AUTO")
```

```{r}
#| label: fig-fluxplot
#| include: false
fluxplot(df)
```

## Results

### Multiple Imputation

Prior to the multiple imputation, the offense gravity score (OGS), offender age, and the square of both OGS and age are centered and scaled. The imputations were generated using random forests in the multiple imputation by chained equations algorithm. We used $M = 5$ since less than 5% of all observations are missing (mainly on offender race) [@grahamHowManyImputations2007]. Random forests are a machine learning method for modeling categorical and continuous outcomes [@breiman2001].

All variables included in the hurdle model are also included in the imputation model; log of sentence length in days, offender race, sex, age, previous record, offense gravity score, trial, minimum sentence recommendation, year, and county.

```{r}
#| label: mice
#| eval: false
df[,c("OGS", "OGSQ", "DOSAGE", "DOSAGEQ")] <- scale(df[,c("OGS", "OGSQ", "DOSAGE", "DOSAGEQ")], center = TRUE, scale = TRUE)

df[,"DOSAGEQ"] <- df[,"DOSAGE"]^2
df[,"OGSQ"] <- df[,"OGS"]^2
imps0 <- mice(df, m = 1, method = "rf", maxit = 0)

mthd <- imps0$method
mthd["JP_MIN_MON"] <-  "~I(JP_MIN / 30)"
mthd["INCAR"] <- "~I(ifelse(JP_MIN > 0, 1, 0))"
# mthd[c("RECMIN", "INCAR", "OFF_RACE", "PRVREC")] <- "cart"
mthd

pred_mat <- imps0$predictorMatrix
pred_mat[,"JPR_ID"] <- 0
pred_mat

imps <- mice(df, m = 5, method = mthd, predictorMatrix = pred_mat, maxit = 10)

plot(imps)

saveRDS(imps, "fits/mice-rf-subsample.rds")
```

```{r}
#| label: load-mice
#| echo: false

imps <- readRDS("../../crim-pattern-mixture/pattern-mixture-modeling/fits/mice-rf-full-data.rds")
```

<!-- ### Logistic Regression on Incarceration -->

<!-- Before modeling the whole sentencing decision, I'll fit a logistic regression on the incarceration decision as a sanity check. The estimated odds ratio for increased odds of incarceration for a Black defendant over a White defendant should be roughly 1.25 as we saw in the complete case analysis in the previous paper. -->

```{r}
#| label: mice-glm
#| eval: false
#| include: false

fit_incar <- with(imps, glm(INCAR ~ DOSAGE + DOSAGEQ + SEX + OFF_RACE + OGS + OGSQ +

                      OGS * PRVREC + PRVREC + RECMIN + CRIME + TRIAL +

                     as.factor(YEAR) + as.factor(COUNTY),

                 family = binomial(link = "logit"),

                 x = FALSE, y = FALSE,

                 model = FALSE))

saveRDS(fit_incar, "fits/fit-glm-logit-incar.rds")

```

```{r}
#| label: tbl-glm-summary
#| include: false
#| tbl-cap: Summary of the logistic regression fit for predicting sentencing decision (In/Out) excluding the year and county estimates for brevity.

fit_incar <- readRDS("fits/fit-glm-logit-incar.rds")

```

```{r}
#| label: log-reg-avg
#| eval: false
#| include: false

smry <- summary(fit_incar)

avg_data <- df |>

    group_by(SEX, OFF_RACE) |>

    summarize(

        DOSAGE = mean(DOSAGE, na.rm = TRUE),

        DOSAGEQ = mean(DOSAGEQ, na.rm = TRUE),

        OGS = mean(OGS, na.rm = TRUE),

        OGSQ = mean(OGSQ, na.rm = TRUE),

        `PRVREC1/2/3` = mean(PRVREC == "1/2/3", na.rm = TRUE),

        `PRVREC4/5` = mean(PRVREC == "4/5", na.rm = TRUE),

        `PRVRECREVOC/RFEL` = mean(PRVREC == "REVOC/RFEL", na.rm = TRUE),

        RECMIN = mean(RECMIN, na.rm = TRUE),

        CRIMEDUI = mean(CRIME == "DUI"),

        CRIMEOther = mean(CRIME == "Other"),

        CRIMEPersons = mean(CRIME == "Persons"),

        CRIMEProperty = mean(CRIME == "Property"),

        TRIAL = mean(TRIAL)

    ) |>

    filter(!is.na(OFF_RACE)) |>

    ungroup() |>

    mutate(

        OFF_RACEBLACK = case_when(OFF_RACE == "BLACK" ~ 1, TRUE ~ 0),

        OFF_RACELATINO = case_when(OFF_RACE == "LATINO" ~ 1, TRUE ~ 0),

        OFF_RACEOTHER = case_when(OFF_RACE == "OTHER" ~ 1, TRUE ~ 0),

        SEXMale = case_when(SEX == "Male" ~ 1, TRUE ~ 0)

    ) |>

    select(DOSAGE, DOSAGEQ, SEXMale, OFF_RACEBLACK, OFF_RACELATINO, OFF_RACEOTHER, OGS,

           OGSQ, `PRVREC1/2/3`, `PRVREC4/5`, `PRVRECREVOC/RFEL`, RECMIN,

           CRIMEDUI, CRIMEOther, CRIMEPersons, CRIMEProperty, TRIAL) |>

    mutate(

        `SEXMale:OFF_RACEBLACK` = SEXMale * OFF_RACEBLACK,

        `SEXMale:OFF_RACELATINO` = SEXMale * OFF_RACELATINO,

        `SEXMale:OFF_RACEOTHER` = SEXMale * OFF_RACEOTHER,

        `OGS:PRVREC1/2/3` = OGS * `PRVREC1/2/3`,

        `OGS:PRVREC4/5` = OGS * `PRVREC4/5`,

        `OGS:PRVRECREVOC/RFEL` = OGS * `PRVRECREVOC/RFEL`

    )

betas <- smry$estimate

names(betas) <- smry$term

pred_probs <- cbind(rep(1, 8), as.matrix(avg_data)) %*% betas

pred_probs <- (1 + exp(-pred_probs))^-1

names(pred_probs) <- c("FW", "FB", "FL", "FO", "MW", "MB", "ML", "MO")

ps <- prop.table(table(df$SEX[s], df$OFF_RACE[s]))

P_B <- ps[2,2] / sum(ps[,2]) * pred_probs["MB"] + ps[1,2] / sum(ps[,2]) * pred_probs["FB"]

P_W <- ps[2,1] / sum(ps[,1]) * pred_probs["MW"] + ps[1,1] / sum(ps[,1]) * pred_probs["FW"]

(P_B / (1 - P_B))  / ((P_W) / (1 - P_W))

```

<!-- We re-analyze the data using a generalized linear mixed model and again taking the binary incarceration decision as the outcome and random effects for the Year and County with random slopes for the most serious Crime type by County. -->

```{r}
#| label: mice-glmm
#| eval: false
#| include: false

fit_incar_mm <- with(imps, lme4::glmer(INCAR ~ DOSAGE + DOSAGEQ + SEX + OFF_RACE + OGS + OGSQ +

                     PRVREC + OGS * PRVREC + RECMIN + CRIME + TRIAL +

                     (1 | YEAR) + (1 + COUNTY_OGS + PCASES || COUNTY),

                 family = binomial(link = "logit"),

                 verbose = 0))

saveRDS(fit_incar_mm, "fits/fit-glmm-logit-incar.rds")

```

```{r}
#| label: tbl-glmm-sum
#| include: false
#| tbl-cap: Summary statistics for the fixed effects from the logistic regression and logistic regression mixed model.

fit_incar_mm <- readRDS("fits/fit-glmm-logit-incar.rds")

smry_incar <- summary(pool(fit_incar))

incar_race <- smry_incar |>

    filter(stringr::str_sub(term, 1,3) != "as.") |>

    mutate(

        se = std.error,

        lb95 = estimate - qt(0.975, df = df) * std.error,

        ub95 = estimate + qt(0.975, df = df) * std.error,

        sig = case_when(

            lb95 > 0 ~ "*",

            ub95 < 0 ~ "*",

            TRUE ~ ""

        )

    ) |>

    select(term, estimate, se, lb95, ub95, sig) |>

    filter(

      stringr::str_detect(term, "RACE")

    ) |>

    mutate(

        model = "GLM - Logistic"

    )

smry_mm <- summary(pool(fit_incar_mm))

incar_race_mm <- smry_mm |>

    mutate(

        se = std.error,

        lb95 = estimate - qt(0.975, df = df) * se,

        ub95 = estimate + qt(0.975, df = df) * se,

        sig = case_when(

            lb95 > 0 ~ "*",

            ub95 < 0 ~ "*",

            TRUE ~ ""

        )

    ) |>

    select(

        term, estimate, se, lb95, ub95, sig

    ) |>

    filter(

      stringr::str_detect(term, "RACE")

    ) |>

    mutate(

        model = "GLMM - Logistic MM"

    )

bind_rows(incar_race, incar_race_mm) |>

    select(model, term, estimate, se, lb95, ub95, sig) |>

    kableExtra::kable(format = "latex", digits = 2, escape = TRUE,

                      col.names = c("Model", "Term", "Estimate", "SE", "LB 95% CI", "UB 95% CI", "Sig."),

                      booktabs = TRUE)

```

<!-- Race effects for the incarceration decision are reported in @tbl-glmm-sum. From the logistic regression fit with MI and $M = 5$ imputations done using predictive mean matching, we found that a Black defendant is `r round(exp(0.243), 2)` (95% CI of (`r round(exp(0.1246), 3)`, `r round(exp(0.3622), 3)`) times more likely to be sentenced to incarceration than an otherwise similar White defendant. The mixed model reports similar estimated race effect of `r round(exp(0.231), 2)` (95% CI of (`r round(exp(0.1129), 3)`, `r round(exp(0.349), 3)`). -->

### Hurdle Models

A hurdle model models data with a high number of zeros (compared to standard distributions). The model is composed of two components: the hurdle for the zeros and the GLM for the non-zero part. Let $\pi_i = P(Y_i = 0)$ be the probability that the $i$th observation is zero and $P(Y_i \neq 0) = f_{y\neq 0}(y_i)$ where $f_{y\neq 0}$ is a truncated probability density function [@craggStatisticalModelsLimited1971]. We are first presenting the analysis of the data with multiple imputations under the MAR assumption to demonstrate how the model fits the data and how to interpret the race effect estimates.

#### Lognormal GLM Hurdle Model with Predictors on Hurdle Parameter

Under this first model, we will model the probability of $Y_i = 0$ as a logistic regression on the independent and legally relevant variables with the R package `brms` [@bürkner2018].

$$
\mathrm{logit}^{-1}(P(Y_i = 0)) = \mathbf{x}_i \boldsymbol{\alpha} + \mathbf{z}_i \mathbf{v}.
$$ {#eq-hurdle-logistic}

$$
\log(Y_i) = \mathbf{x}_i \boldsymbol{\beta} + \mathbf{z}_i \mathbf{u} + \epsilon_i
$$ {#eq-hurdle-glm} where $\mathbf{v}$ and $\mathbf{u}$ are independent MVN with $E(\mathbf{v}) = E(\mathbf{u}) = 0$ and covariance matrices $Cov(\mathbf{v}) = G_v$ and $Cov(\mathbf{u}) = G_u$, $\epsilon_i \overset{iid}{\sim} N(0, \sigma^2)$ and $\mathbf{v}, \mathbf{u}$ and $\boldsymbol{\epsilon}$ are mutually independent. $\mathbf{x}_i$ and $\mathbf{z}_i$ are rows from two known design matrices for the population-level and group-level effects respectively.

Here we include group-level effects for the year, the county, and the crime-type in the county in case the judicial system sentences the different crime-types differently relative to other counties. Each of the population-level regression coefficients are given a weakly-informative normal prior $\beta_j \sim N(0, 100).$ The group-level effects for the intercepts and effects for crime-type are given noncentral t-distributions $v_k \sim t_{3; 0, 2.5}$ and $u_k \sim t_{3; 0, 2.5}$ while the correlations between the county-level slopes for proportion of cases and county-average OGS and county-level intercepts are given $\rho_{i.j} \sim lkj(1)$ priors. The error term's variance also gets a noncentral t prior $\sigma \sim t_{3; 0, 2.5}.$ The coefficients $\boldsymbol{\alpha}$ are given normal priors $\alpha_k \overset{iid}{\sim} N(0, 100).$

Under this model we are assuming the incomplete data are MAR and the missingness can be modeled entirely by the multiple imputation procedure.

```{r}
#| label: brms-hurdle-model-prior2
#| echo: false
#| results: hide

bf1 <- bf(JP_MIN ~ DOSAGE + DOSAGEQ + SEX*OFF_RACE + OGS + OGSQ +
                    OGS * PRVREC + PRVREC + RECMIN + CRIME + TRIAL +
                    (1 + COUNTY_OGS + PCASES || COUNTY),
          hu ~ DOSAGE + DOSAGEQ + SEX * OFF_RACE + RECMIN + OGS + OGSQ + PRVREC + CRIME + TRIAL + (1 + COUNTY_OGS + PCASES || COUNTY))

bprior2 <- get_prior(bf1,
               data = df,
               family = hurdle_lognormal(link = "identity", link_sigma = "log", link_hu = "logit"))
bprior2$class
bprior2 <- prior(normal(0, 100), class = "b") +
    prior(normal(0, 100), class = "b", dpar = "hu") +
    # prior(lkj(1), class = "cor") +
    prior(student_t(3, 0, 2.5), class = "Intercept")
    prior(student_t(3, 0, 2.5), class = "sd", group = "COUNTY", lb = 0) +
    prior(student_t(3, 0, 2.5), class = "sd", group = "YEAR", lb = 0) +
    prior(student_t(3, 0, 2.5), class = "Intercept", dpar = "hu") +
    prior(student_t(3, 0, 2.5), class = "sd", dpar = "hu", lb = 0)

# bprior2$prior[1] <- "normal(0, 25)"
bprior2
```

```{r}
#| label: brms-hurdle-lognormal-2
#| eval: false
#| results: hide

fit_brm2 <- brm_multiple(bf1,
               data = imps,
               family = hurdle_lognormal(link = "identity", link_sigma = "log", link_hu = "logit"),
               prior = bprior2,
               chains = 2, cores = 8, iter = 2000, refresh = 100,
               init = 0,
               threads = threading(8),
               control = list(adapt_delta = 0.82, max_treedepth = 12),
               # file = "brms_fit/brms-hurlde-si.fit",
               # file_refit = "on_change",
               save_model = "stan/brms-hurdle-lognormal-mi.stan")

saveRDS(fit_brm2, "fits/fit-brms-hurdle-lognormal-mi.rds")
```

```{r}
#| label: tbl-brms-hurdle-model-summary-2-racesex
#| tbl-cap: Fixed/population-level effects for the non-zero part of the lognormal hurdle model.

fit_brm2 <- readRDS("../../crim-pattern-mixture/pattern-mixture-modeling/fits/sensitivity-analysis-fit-scale-1.rds")
smry2 <- summary(fit_brm2)
smry2$fixed$term <- rownames(smry2$fixed)
smry2$fixed |>
    filter(
        stringr::str_sub(term, 1,2) != "hu",
      stringr::str_detect(term, "RACE")  
    ) |>
  mutate(
    term2 = case_when(
      stringr::str_sub(term, 1,4) == "OFF_" ~ stringr::str_sub(term, 9, -1),
      stringr::str_sub(term, 1,3) == "SEX" ~ paste0("Male & ", stringr::str_sub(term, 17, -1)),
      TRUE ~ NA)
    ) |>
    select(term2, Estimate, Est.Error, `l-95% CI`, `u-95% CI`) |>
    kableExtra::kable(format = "markdown", digits = 2, escape = TRUE, row.names = FALSE,
                      col.names = c("Term", "Estimate", "SE", "LB 95% CI", "UB 95% CI"),
                      booktabs = TRUE)
```

```{r}
#| label: tbl-brms-hurdle-model-summary-2-zero-racesex
#| tbl-cap: Fixed/population-level effects for the zero part of the lognormal hurdle model.

smry2$fixed |>
    filter(
        stringr::str_sub(term, 1,2) == "hu",
      stringr::str_detect(term, "RACE")  
    ) |>
  mutate(
    term2 = case_when(
      stringr::str_sub(term, 1,7) == "hu_OFF_" ~ stringr::str_sub(term, 12, -1),
      stringr::str_sub(term, 1,6) == "hu_SEX" ~ paste0("Male & ", stringr::str_sub(term, 20, -1)),
      TRUE ~ NA)
    ) |>
    select(term2, Estimate, Est.Error, `l-95% CI`, `u-95% CI`) |>
    kableExtra::kable(format = "markdown", digits = 2, escape = TRUE,
                      col.names = c("Term", "Estimate", "SE", "LB 95% CI", "UB 95% CI"),
                      row.names = FALSE,
                      booktabs = TRUE)
```

The standard deviation parameter $\sigma$ of the lognormal distribution has a posterior mean of `r round(smry2$spec_pars[1,1], 2)` (95% CI: \[`r round(smry2$spec_pars[1,3], 2)`, `r round(smry2$spec_pars[1,4], 2)`\]).

The county-level random/group-level effects and year-level random/group-level effects are reported in @tbl-brms2-re.

```{r}
#| label: tbl-brms2-re
#| tbl-cap: Random/group-level effect standard deviation estimates for the lognormal hurdle model.
r_effs2 <- bind_rows(smry2$random)
# r_effs2$term <- c("sd(Intercept)", "sd(OGS | County)", "sd(Cases%)", "sd(hurdle Intercept)", "sd(hurdle OGS | County)", "sd(hurdle Cases%)")
r_effs2$term <- c("sd(Intercept)", "sd(Hurdle Intercept)")
r_effs2 |>
    select(term, Estimate, Est.Error, `l-95% CI`, `u-95% CI`) |>
    kableExtra::kable(format = "markdown", digits = 2, escape = TRUE,
                      col.names = c("Term", "Estimate", "SE", "LB 95% CI", "UB 95% CI"),
                      row.names = FALSE,
                      booktabs = TRUE)
```

```{r}
#| label: brms2-vars
#| include: false
smry2$spec_pars

smry2$random$COUNTY
smry2$random$YEAR
```

In @fig-cond-eff-brm2-racesex, we can visualize the impacts of race and sex on the sentence length in aggregate (including zero-length sentences) (@fig-cond-eff-brm2-racesex-1) and on the incarceration decision (@fig-cond-eff-brm2-racesex-2). Numeric estimates are reported in @tbl-race-sex-effects. Under this model and the MAR missingness assumption, we find that there is no significant difference in the sentence lengths between the different racial groups within each sex. There is overlap between all four credible intervals for both male offenders and female offenders. Between the sexes, we do see significant differences. Female offenders receive shorter sentences and are more likely to receive community sentences regardless of race. There is a large amount of uncertainty in the effect estimate for Latino offenders. The Other group also has a relatively high amount of uncertainty and the male Other race offenders' CI overlaps with all three other groups' CIs for both sexes, although the mean posterior estimate is lower for sentence length.

```{r}
#| label: fig-cond-eff-brm2-racesex
#| layout: [[47, -6, 47]]
#| fig-cap: Posterior estimates of the interaction between race and sex effects on the sentence length in days.
#| fig-subcap: 
#|   - Estimates on marginal sentence length (includes zeros).
#|   - Estimates on probability of incarceration from the logistic regression on the zero-part of the lognormal hurdle model.

theme_set(theme_bw())
plot(conditional_effects(fit_brm2, effects = "SEX:OFF_RACE"),
     ask = FALSE, plot = FALSE)[[1]] +
  labs(x = "Sex", y = "Sentence Length (days)")# +
  # scale_colour_discrete(name="Race/Ethnicity")

plot(conditional_effects(fit_brm2, dpar = "hu", effects = "SEX:OFF_RACE"),
     ask = FALSE, plot = FALSE)[[1]] +
  labs(x = "Sex", y = "P(Incarceration)") #+
  # scale_color_discrete(name="Race/Ethnicity")
```

Results for the other terms in the model are reported in the Appendix.

```{r}
#| label: tbl-race-sex-effects
#| tbl-cap: Estimates for sentence length combining the non-zero and zero predictions for the eight combinations of race and sex.

ce_data <- conditional_effects(fit_brm2, "SEX:OFF_RACE", plot = FALSE)[[1]]

ce_data |>
    select(effect1__, effect2__, estimate__, se__, lower__, upper__) |>
    kableExtra::kable(format = "markdown", digits = 2, escape = TRUE,
                      col.names = c("Sex", "Race", "Est", "SE", "LB 95% CI", "UB 95% CI"),
                      row.names = FALSE,
                      booktabs = TRUE)
    
```

<!-- #### GLMMadaptive Lognormal Hurdle Model -->

<!-- Complete case analysis to see if the function works and how quickly it runs. -->

<!-- ```{r} -->

<!-- #| label: fit-glmmadaptive-lognormal-hurdle -->

<!-- #| eval: false -->

<!-- # DOSAGE + SEX + OFF_RACE + OGS + PRVREC + RECMIN + CRIME + TRIAL -->

<!-- fit_lnh <- mixed_model(JP_MIN ~ DOSAGE + SEX + OFF_RACE + OGS + PRVREC + RECMIN + CRIME + TRIAL, -->

<!--                     random = ~ 1 | YEAR, -->

<!--                     zi_fixed = ~ DOSAGE + DOSAGEQ + SEX * OFF_RACE + RECMIN + OGS + OGSQ + PRVREC + CRIME + TRIAL, -->

<!--                     zi_random = ~ 1 | YEAR, -->

<!--                     data = df, -->

<!--                     family = hurdle.lognormal(), -->

<!--                     penalized = TRUE, -->

<!--                     iter_EM = 0) -->

<!-- saveRDS(fit_lnh, "fits/fit-glmmadaptive-lognormal-hurdle.rds") -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: load-glmmadaptive-lognormal-hurdle -->

<!-- fit_lnh <- readRDS("fits/fit-glmmadaptive-lognormal-hurdle.rds") -->

<!-- # fit_lnh -->

<!-- # marginal_coefs(fit_lnh) -->

<!-- # ses <- diag(fit_lnh$Hessian) |> sqrt()  -->

<!-- #  -->

<!-- # cbind(c(fit_lnh$coefficients, fit_lnh$gammas), ses) |> round(2) -->

<!-- ``` -->

<!-- Evaluation of predictions -->

<!-- ```{r} -->

<!-- #| label: fig-cdf-glmm-lnh-pred -->

<!-- #| fig-cap: The empirical CDF of the observed sentence lengths (days, log scale) with fitted cdfs simulated from the model fit overlaid in light gray. -->

<!-- par(mar = c(2.5, 2.5, 0, 0), mgp = c(1.1, 0.5, 0), cex.axis = 0.7, cex.lab = 0.8) -->

<!-- y <- df$JP_MIN -->

<!-- y <- y[which(!is.na(y))] -->

<!-- y[y > 0] <- log(y[y > 0]) -->

<!-- x_vals <- seq(min(y)-1, max(y), length.out = 500) -->

<!-- out <- simulate(fit_lnh, nsim = 30, acount_MLEs_var = FALSE) -->

<!-- ind <- out > sqrt(.Machine$double.eps) -->

<!-- out[ind] <- log(out[ind]) -->

<!-- rep_y <- apply(out, 2, function (x, x_vals) ecdf(x)(x_vals), x_vals = x_vals) -->

<!-- matplot(x_vals, rep_y, type = "l", lty = 1, col = "lightgrey",  -->

<!--         xlab = "Response Variable", ylab = "Empirical CDF") -->

<!-- lines(x_vals, ecdf(y)(x_vals)) -->

<!-- legend("bottomright", c("log replicated data", "log observed data"), lty = 1,  -->

<!--        col = c("lightgrey", "black"), bty = "n", cex = 0.8) -->

<!-- ``` -->

<!-- Fitting with MI. -->

<!-- ```{r} -->

<!-- #| label: fit-glmmadaptive-lognormal-hurdle-mi -->

<!-- #| eval: false -->

<!-- start <- Sys.time() -->

<!-- imp_list <- complete(imps, "all", include = FALSE) -->

<!-- fit_lnh_mi <- lapply(imp_list, function(imp) { -->

<!--     fit_lnh <- mixed_model(JP_MIN ~ DOSAGE + DOSAGEQ + SEX * OFF_RACE + OGS *  PRVREC + -->

<!--                     OGSQ + RECMIN + CRIME + TRIAL, -->

<!--                     random = ~ 1 | YEAR, -->

<!--                     zi_fixed = ~ DOSAGE + DOSAGEQ + SEX * OFF_RACE + RECMIN + OGS + OGSQ + PRVREC + CRIME + TRIAL, -->

<!--                     # zi_random = ~ 1 | YEAR, -->

<!--                     data = imp, -->

<!--                     family = hurdle.lognormal(), -->

<!--                     penalized = TRUE, -->

<!--                     optimizer = "nlminb") -->

<!--     return(fit_lnh) -->

<!-- }) -->

<!-- print(Sys.time() - start) -->

<!-- saveRDS(fit_lnh_mi, "fits/fit-glmmadaptive-lognormal-hurdle-mi.rds") -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: tbl-coef-comp -->

<!-- #| tbl-cap: Coefficients from the non-zero portions of the full lognormal hurdle models fit with brms and glmmadaptive after multiple imputation. -->

<!-- fit_lnh_mi <- readRDS("fits/fit-glmmadaptive-lognormal-hurdle-mi.rds") -->

<!-- est_lnh_mi <- lapply(fit_lnh_mi, function(fit) {fit$coefficients}) |> bind_rows() |> apply(2, mean) -->

<!-- hu_lnh_mi <- lapply(fit_lnh_mi, function(fit) {fit$gammas}) |> bind_rows() |> apply(2, mean) -->

<!-- coef_est <- smry2$fixed |> -->

<!--     filter(stringr::str_sub(term, 1, 2) != "hu") |> -->

<!--     select(term, Estimate) -->

<!-- hu_est <- smry2$fixed |> -->

<!--     filter(stringr::str_sub(term, 1, 2) == "hu") |> -->

<!--     mutate( -->

<!--         term2 = stringr::str_sub(term, 4, -1) -->

<!--     ) |> -->

<!--     select(term2, Estimate) -->

<!-- coef_est$glmmadapt <- est_lnh_mi -->

<!-- hu_est$glmmadapt <- hu_lnh_mi -->

<!-- coef_est2 <- left_join(coef_est, hu_est, -->

<!--                       by = c("term" = "term2"), suffix = c("_nz", "_hu")) -->

<!-- coef_est2 |>  -->

<!--     select(term, Estimate_nz, glmmadapt_nz, Estimate_hu, glmmadapt_hu) |> -->

<!--     kableExtra::kable(format = "latex", digits = 4, escape = TRUE, -->

<!--                       col.names = c("Term", "brms - Est.", "glmma - Est.", -->

<!--                                     "brms - Hurdle Est.", "glmma - Hurdle Est."), -->

<!--                       row.names = FALSE, -->

<!--                       booktabs = TRUE) -->

<!-- ``` -->

### Sensitivity Analysis using Pattern-mixture Models

Evaluating the impacts of various nonignorable missingness mechanisms can be accomplished with pattern-mixture models. The values of the incomplete numeric data can be scaled or shifted. The same cannot be done with categorical imputations, instead the proportion of each category can be varied within the imputations compared to the observed distribution or the MAR imputed distribution.

While there are very few missing sentence lengths, I could also modify the imputed sentence lengths with a scale $c$ or shift $\delta$. Scaling maintains that offenders that are non-incarcerated will remain non-incarcerated while the sentences of incarcerated defenders would shift. A positive shift would make all offenders incarcerated while a negative shift would impose negative sentence lengths that would need to be corrected to use Poisson or lognormal hurdle glms.

```{r}
#| label: rf-fit
#| include: false
#| eval: false
c_dat_s <- na.exclude(df)

fit_rf <- ranger::ranger(OFF_RACE ~ DOSAGE + DOSAGEQ + SEX + TRIAL + RECMIN + PRVREC + OGS + OGSQ + CRIME + YEAR + COUNTY + JP_MIN, 
                         data = c_dat_s,
                         probability = TRUE)

print(fit_rf)

fit_rf$confusion.matrix
```

```{r}
#| label: rf-pred

sample_rf_pd <- function(dat, fit, scale = c(1, 1, 1, 1)) {
    pred_rf <- predict(fit, data = dat)
    ppred_rf <- pred_rf$predictions
    
    plab <- unlist(lapply(1:nrow(dat), function(i) {
        p_i <- ppred_rf[i,]
        p_i <- p_i * scale
        p_i <- 1/sum(p_i) * p_i
        sample(colnames(ppred_rf), size = 1, prob = p_i)
        }))
}
```

To perturb the imputations, we modify the vector of probabilities $\mathbf{p} = (p_1, p_2, p_3, p_4)'$ of class membership for each racial/ethnic group under consideration (White, Black, Latino, or Other). A vector of scale parameters $\mathbf{c} = (c_1, c_2, c_3, c_4)'$ is chosen such that the new racial/ethnic group label will be drawn from the normalized vector $\mathbf{p}^* = \frac{1}{\sum_{j=1}^4 c_j p_j} (c_1 p_1, c_2 p_2, c_3 p_3, c_4 p_4)'$ so that the probability vectors remains the same if $c_j = 1$ for each $j = 1,\dots,4.$ For simplicity, we will focus on varying the probability of assigning a White label by changing only $c_1$ while holding $c_2 = c_3 = c_4 = 1.$

The vector of probabilities $\mathbf{p}_i$ is obtained for each incomplete observation $i$ by re-fitting the same random forest used in the initial mulitple imputation step. This vector is then scaled and normalized using the same scaling vector $\mathbf{c}$ for all incomplete observations. The probability vectors $\mathbf{p}^*$ are used to draw new imputed racial/ethnic group labels for the incomplete observations. This procedure is repeated, including the model fitting, across each of the $M$ completed data sets resulting in a new set of $M$ completed data sets with modified imputations. The new set of completed data sets is then analyzed as before using an lognormal hurdle model. Estimates from each variation of $\mathbf{c}$ that is chosen are then compared graphically in @fig-sens-analysis-res.

We choose to vary $c_1$ along the sequence $\{0.1, 0.33, 0.75, 0.9, 1, 1.1, 1.25, 1.5, 2\}.$ When $c_1 = 1$, the imputations correspond to the original imputation model under the MAR assumption. For $c_1 < 1$, fewer observations are imputed with a White label than under the MAR assumption reflecting a more diverse population of offenders who have unknown or unreported race labels. The opposite is true for $c_1 > 1$, reflecting a more White population of offenders. We include cases such as $c_1 = 0.1$ and $c_1 = 0.33$ as well as $c_1 = 1.5$ and $c_1 = 2$ as rather implausible extreme conditions to investigate what could happen in the most extreme circumstances.

```{r}
#| label: fn-modify-imps

modify_race_imps <- function(imps, scale = c(1.5, 1, 1, 1)) {
    
    mis_ind <- which(is.na(imps$data$OFF_RACE))
    imp_list <- complete(imps, "all", include = FALSE)
    
    imp_list2 <- lapply(1:length(imp_list), function(i) {
        df <- imp_list[[i]]
        
        if (sum(is.na(df)) == 0) {
            fit_imp <- ranger::ranger(OFF_RACE ~ DOSAGE + DOSAGEQ + SEX + TRIAL +
                                          RECMIN + PRVREC + OGS + OGSQ + CRIME +
                                          YEAR + COUNTY + JP_MIN, 
                                 data = df[-mis_ind,],
                                 probability = TRUE)
            new_imp <- sample_rf_pd(df[mis_ind,], fit_imp, scale = scale)
            df[mis_ind, "OFF_RACE"] <- new_imp
        }
        
        df$.imp <- i-1
        df$.id <- 1:nrow(df)
        
        return(df)
    })
    
    imps2 <- imp_list2 |>
        bind_rows() |>
        as.mids()
    return(imps2)
}

```

```{r}
#| label: sensitivity-analysis
#| eval: false
# sens_scales <- c(0.1, 0.33, 0.75, 0.9, 1, 1.1, 1.25, 1.5, 2)
sens_scales <- c(0.9, 0.95, 1, 1.05, 1.1)
 # + COUNTY_OGS + PCASES 
bf1 <- bf(JP_MIN ~ DOSAGE + DOSAGEQ + SEX*OFF_RACE + OGS + OGSQ +
                    OGS * PRVREC + PRVREC + RECMIN + CRIME + TRIAL +
                    (1 || COUNTY),
          hu ~ DOSAGE + DOSAGEQ + SEX * OFF_RACE + RECMIN + OGS + OGSQ + PRVREC + CRIME + TRIAL + (1 || COUNTY))

sens_res <- lapply(sens_scales, function(s) {
    
    if (s != 1) {
        scale <- c(s, 1, 1, 1)
        imps2 <- modify_race_imps(imps, scale = scale)
    }
    else {
        imps2 <- imps
    }
    
    fit_lnh_mi <- brm_multiple(bf1,
               data = imps2,
               family = hurdle_lognormal(link = "identity", link_sigma = "log", link_hu = "logit"),
               prior = bprior2,
               chains = 2, cores = 15, iter = 2000, refresh = 100,
               init = 0,
               #control = list(adapt_delta = 0.8, max_treedepth = 12),
               # file = "brms_fit/brms-hurlde-si.fit",
               # file_refit = "on_change",
               save_model = paste0("stan/brms-hurdle-lognormal-mi-scale-", s, ".stan"))

    smry_lnh <- summary(fit_lnh_mi)
    smry_lnh$fixed$term <- rownames(smry_lnh$fixed)

    res <- smry_lnh$fixed |>
        mutate(
            est = Estimate,
            se = Est.Error,
            l95 = `l-95% CI`,
            u95 = `u-95% CI`,
            scale = s
        ) |>
        select(scale, term, est, se, l95, u95)
    
    return(res)
})

# str(sens_res)

saveRDS(sens_res, "fits/sensitivity-analysis-results.rds")
```

```{r}
#| label: load-res
#| results: hide
#| cache: true

sens_scales <- c(0.33, 0.5, 0.75, 0.9, 1, 1.1, 1.25, 1.5, 2)
sens_res <- lapply(sens_scales, function(s) {
  fp <- paste0("../../crim-pattern-mixture/pattern-mixture-modeling/fits/sensitivity-analysis-results-scale-", s, ".rds")
  print(fp)
  readRDS(fp)
})
```

```{r}
#| label: fig-sens-analysis-res
#| layout-ncol: 2
#| fig-cap: Coefficient estimates for the logistic regression model predicting incarceration.
#| fig-subcap: 
#|   - White offenders
#|   - Black offenders
#|   - Latino offenders
#|   - Other ethnicity offenders
#| fig-width: 4
#| fig-height: 3
#| include: false
#| eval: false


# sens_scales <- c(0.1, 0.33, 0.75, 0.9, 1, 1.1, 1.25, 1.5, 2)
# sens_scales <- c(0.33, 0.75, 0.9, 1, 1.1, 1.25, 1.67)
# sens_scales <- c(0.9, 0.95, 1, 1.05, 1.1)

# sens_res <- readRDS("fits/sensitivity-analysis-results.rds")
sens_res <- lapply(1:length(sens_scales), function(i) {
    df <- sens_res[[i]]
    df$scale <- sens_scales[i]
    return(df)
})

# sens_res |>
#     bind_rows() |>
#     filter(term %in% c("OFF_RACEBLACK")) |>
#     ggplot(aes(scale, estimate, color = as.factor(scale))) +
#         geom_point() +
#         scale_color_viridis_d(name = "Scale Factor")

sens_res2 <- sens_res |>
    bind_rows() |> 
    tibble::remove_rownames() |>
    mutate(
        mod = case_when(
            stringr::str_sub(term, 1, 2) == "hu" ~ "logistic",
            TRUE ~ "lognormal"
        )
    )
    

# sens_res2$scale <- rep(sens_scales, each = 2)

# long_res <- sens_res2 |>
#     tidyr::pivot_longer(cols = `(Intercept)`:`OGS:PRVRECREVOC/RFEL`,
#                         names_to = "term",
#                         values_to = "estimate")



race_sex_est <- expand.grid(scale = sens_scales, SEX = c("Male", "Female"), OFF_RACE = c("WHITE", "BLACK", "LATINO", "OTHER"))
race_sex_est$nz_est <- NA
race_sex_est$zi_est <- NA

for (i in 1:nrow(race_sex_est)) {
    sex <- race_sex_est[i, "SEX"]
    race <- race_sex_est[i, "OFF_RACE"]
    eta <- sens_res2 |>
        filter(mod == "lognormal",
               scale == race_sex_est[i,"scale"],
               term == "Intercept") |>
        select(est)
    eta_hu <- sens_res2 |>
        filter(mod == "logistic",
               scale == race_sex_est[i,"scale"],
               term == "hu_Intercept") |>
        select(est)
    if (sex == "Male") {
        eta <- eta + sens_res2 |>
            filter(mod == "lognormal",
                   scale == race_sex_est[i,"scale"],
                   term == "SEXMale") |>
            select(est)
        
        eta_hu <- eta_hu + sens_res2 |>
            filter(mod == "logistic",
                   scale == race_sex_est[i,"scale"],
                   term == "hu_SEXMale") |>
            select(est)
        
    }
    if (race %in% c("BLACK", "LATINO", "OTHER")) {
        eta <- eta + sens_res2 |>
        filter(mod == "lognormal",
               scale == race_sex_est[i,"scale"],
               term == paste0("OFF_RACE", race)) |>
        select(est)
        
        eta_hu <- eta_hu + sens_res2 |>
            filter(mod == "logistic",
                   scale == race_sex_est[i,"scale"],
                   term == paste0("hu_OFF_RACE", race)) |>
            select(est)
    }

    race_sex_est$nz_est[i] <- eta$est[1]
    race_sex_est$zi_est[i] <- eta_hu$est[1]
}

race_sex_est <- race_sex_est |> 
    tidyr::pivot_longer(
        cols = nz_est:zi_est,
        names_to = "mod1",
        values_to = "est"
    ) |>
    mutate(
        Model = case_when(
            mod1 == "nz_est" ~ "Lognormal",
            mod1 == "zi_est" ~ "Logistic",
            TRUE ~ NA
        )
    )


plt_list <- lapply(c("WHITE", "BLACK", "LATINO", "OTHER"), function(race) {
    p1 <- race_sex_est |>
            filter(OFF_RACE == race) |>
            ggplot(aes(scale, est)) +
                geom_point(aes(color = SEX, shape = Model)) +
                geom_line(aes(color = SEX, linetype = Model)) +
                labs(x = "Scale",
                     y = "Estimate") +
                theme_bw() +
                ggthemes::scale_color_colorblind()
    return(p1)
})
plt_list[[1]]
plt_list[[2]]
plt_list[[3]]
plt_list[[4]]

# long_res |>
#     filter(term == "OFF_RACEBLACK") |>
#     ggplot(aes(scale, estimate)) +
#         geom_point(aes(color = as.factor(scale), shape = mod)) +
#         scale_color_viridis_d(name = "Scale Factor") +
#         geom_line(aes(linetype = mod)) +
#         labs(subtitle = "Black Offenders")
# long_res |>
#     filter(term == "OFF_RACELATINO") |>
#     ggplot(aes(scale, estimate)) +
#         geom_point(aes(color = as.factor(scale), shape = mod)) +
#         scale_color_viridis_d(name = "Scale Factor") +
#         geom_line(aes(linetype = mod)) +
#         labs(subtitle = "Latino Offenders")
# long_res |>
#     filter(term == "OFF_RACEOTHER") |>
#     ggplot(aes(scale, estimate)) +
#         geom_point(aes(color = as.factor(scale), shape = mod)) +
#         scale_color_viridis_d(name = "Scale Factor") +
#         geom_line(aes(linetype = mod)) +
#         labs(subtitle = "Other Offenders")
```

@fig-sens-analysis-res displays the race effects on sentence length for the White, Black, Latino, and Other racial/ethnic groups. In general, we see that estimates for each ethnic group's expected incarceration decision and sentence length tend to be very stable regardless of how the distribution of race is perturbed. This indicates that our estimate under the MAR assumption is likely to be robust against violations of the MAR assumption on the race or ethnicity of offenders.

```{r}
sens_scales <- c(0.33, 0.5, 0.75, 0.9, 1, 1.1, 1.25, 1.5, 2)
sens_ce <- lapply(sens_scales, function(s) {
  fp <- paste0("../../crim-pattern-mixture/pattern-mixture-modeling/fits/sensitivity-analysis-fit-scale-", s, ".rds")
  fit_i <- readRDS(fp)
  
  ce_data <- conditional_effects(fit_i, "SEX:OFF_RACE", plot = FALSE)[[1]] |>
      mutate(
          scale = s
      )

  ce_data
})
```

```{r}

ce_dat <- bind_rows(sens_ce)

ggplot(ce_dat, aes(scale, estimate__, color = OFF_RACE)) +
    geom_errorbar(aes(x = scale, ymin = lower__, ymax = upper__)) +
    geom_point() +
    geom_line(linetype = "dashed", linewidth = 0.2) +
    facet_grid(SEX~OFF_RACE) +
    ggthemes::scale_color_colorblind() +
    scale_x_continuous(breaks = sens_scales) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
sens_ce_hu <- lapply(sens_scales, function(s) {
  fp <- paste0("../../crim-pattern-mixture/pattern-mixture-modeling/fits/sensitivity-analysis-fit-scale-", s, ".rds")
  fit_i <- readRDS(fp)
  
  ce_data_hu <- conditional_effects(fit_i, dpar = "hu", effects = "SEX:OFF_RACE", plot = FALSE)[[1]] |>
      mutate(
          scale = s
      )

  ce_data_hu
})

```

```{r}
ce_dat_hu <- bind_rows(sens_ce_hu)

ggplot(ce_dat_hu, aes(scale, estimate__, color = OFF_RACE)) +
    geom_errorbar(aes(x = scale, ymin = lower__, ymax = upper__)) +
    geom_point() +
    geom_line(linetype = "dashed", linewidth = 0.2) +
    facet_grid(SEX~OFF_RACE) +
    ggthemes::scale_color_colorblind() +
    scale_x_continuous(breaks = sens_scales) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
ce_dat2 <- left_join(ce_dat, ce_dat_hu, by = c("scale", "effect1__", "effect2__"), suffix = c("_cond", "_hu")) |>
    mutate(
        estimate__ = estimate___cond / (1 - estimate___hu),
        se__ = (se___cond - se___hu) / (1 - estimate___hu)^2,
        lower__ = estimate__ - 2 * se__,
        upper__ = estimate__ + 2 * se__
        
    ) |>
    select(
        scale, effect1__, effect2__,
        tidyr::starts_with("estimate"),
        tidyr::starts_with("se__"),
        tidyr::starts_with("lower"),
        tidyr::starts_with("upper")
    )

ggplot(ce_dat2, aes(scale, estimate__, color = effect2__)) +
    geom_errorbar(aes(x = scale, ymin = lower__, ymax = upper__)) +
    geom_point() +
    geom_line(linetype = "dashed", linewidth = 0.2) +
    facet_grid(effect1__~effect2__) +
    ggthemes::scale_color_colorblind() +
    scale_x_continuous(breaks = sens_scales) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
ce_dat2 |>
    tidyr::pivot_wider(id_cols = scale, names_from = c("effect1__", "effect2__"),
                       values_from = "estimate___cond") |>
    kableExtra::kbl(format = "markdown", digits = 3, 
                    col.names = c("$k$", "White Female", "Black Female", "Latina Female", "Other Female", "White Male", "Black Male", "Latino Male", "Other Male"))
```

## Discussion/Conclusion

Criminal justice data are often incomplete due to study attrition, sample selection, data entry or reporting issues, and myriad other reasons. Criminologists have inconsistently addressed incomplete data, which may contribute to questions regarding the plausibility and sensitivity of effect estimates. In particular, the presence and magnitude of race/ethnicity effects in court contexts has drawn recent attention. It is important that the field continue developing strategies to assess the robustness of findings against violations of the underlying assumptions of incomplete data (e.g., MAR). Specificity regarding race/ethnicity effect estimates is of the utmost importance given the potential policy impacts of research findings in this topic area.

Building upon a body of work regarding sensitivity testing of race/ethnicity effects, we demonstrate how MI and PMM can be used in conjunction to assess the robustness of race/ethnicity effects given incomplete data of any given pattern (i.e., MAR, MNAR) and source (e.g., attrition, selection). This approach is highly applicable to analyses of criminal justice data, which often include nonignorable missingness and questions about the robustness of effect estimates. We demonstrate that race/ethnicity effects appear stable against violations of the MAR assumption for missingness on the defendant race variable.

The current study has several limitations. First, we have not compared PMMs to other methods of sensitivity analysis, we have simply argued its theoretical utility and demonstrated its application. Future research should compare the robustness of estimates under different methods of sensitivity analysis. Second, reducing the sentencing decisions to a binary incarcerated/not decisions in a study of punishment severity risks losing information about the varying degrees of severity of non-incarcerative sentences (Pina-Sanchez et al., 2020). Future research should consider how these non-incarcerative sentencing options should be quantified in terms of punitiveness and in comparison to incarceration sentence lengths.

## Acknowledgments

The authors would like to thank the Pennsylvania Commission on Sentencing (PCS) for providing data. The points of view presented in this paper are those of the authors and do not necessarily represent the official position of PCS.

\newpage

## References {.unnumbered}

::: {#refs}
:::

\newpage

## Appendix {.unnumbered}

```{r}
#| label: fig-incar-cime
#| fig-cap: Incarceration decision by most serious crime type. INCAR == 1 indicates incarceration. INCAR == 0 indicates parole.
#| fig-height: 4
#| include: true
ggplot(df, aes(as.factor(INCAR), fill = CRIME)) +
    geom_bar(position = "dodge") +
    ggthemes::scale_fill_colorblind() +
  theme_bw()
```

```{r}
#| label: fig-sen-len-qq
#| layout-ncol: 2
#| fig-cap: QQ plots of the sentence length (days)
#| fig-subcap: 
#|   - Sentence length in days
#|   - Log of sentence length plus one day
#| include: true
sent_non0 <- df$JP_MIN[which(df$INCAR == 1)]
s <- sample(length(sent_non0), size = 10000, replace = FALSE)

qqnorm(sent_non0[s])
qqline(sent_non0[s])

qqnorm(log1p(sent_non0[s]))
qqline(log1p(sent_non0[s]))
```

```{r}
#| label: fig-scatter-pairs
#| fig-cap: Scatter plot matrix of the numeric predictors and the log plus one of the sentence length (days).
#| include: false
#| evale: false
df |> 
    mutate(jp_min_l1p = log1p(JP_MIN)) |>
    select(DOSAGE, DOSAGEQ, OGS, OGSQ, TRIAL, RECMIN, jp_min_l1p) |>
    GGally::ggpairs() +
                ggthemes::scale_color_colorblind()
```

```{r}
#| label: tbl-brms-hurdle-model-summary-2
#| tbl-cap: Fixed/population-level effects for the non-zero part of the lognormal hurdle model.

fit_brm2 <- readRDS("../../crim-pattern-mixture/pattern-mixture-modeling/fits/sensitivity-analysis-fit-scale-1.rds")
smry2 <- summary(fit_brm2)
smry2$fixed$term <- rownames(smry2$fixed)
smry2$fixed |>
    select(term, Estimate, Est.Error, `l-95% CI`, `u-95% CI`) |>
    filter(
        stringr::str_sub(term, 1,2) != "hu"
    ) |>
    kableExtra::kable(format = "markdown", digits = 2, escape = TRUE, row.names = FALSE,
                      col.names = c("Term", "Estimate", "SE", "LB 95% CI", "UB 95% CI"),
                      booktabs = TRUE)
```

```{r}
#| label: tbl-brms-hurdle-model-summary-2-zero
#| tbl-cap: Fixed/population-level effects for the zero part of the lognormal hurdle model.

smry2$fixed |>
    select(term, Estimate, Est.Error, `l-95% CI`, `u-95% CI`) |>
    filter(
        stringr::str_sub(term, 1,2) == "hu"
    ) |>
    kableExtra::kable(format = "markdown", digits = 2, escape = TRUE,
                      col.names = c("Term", "Estimate", "SE", "LB 95% CI", "UB 95% CI"),
                      row.names = FALSE,
                      booktabs = TRUE)
```

```{r}
#| label: fig-brm2-mcmc
#| fig-cap: MCMC plots for the lognormal hurdle model.
#| fig-subcap: 
#|   - Age
#|   - Age Squared
#|   - Sex
#|   - Offender Race
#|   - OGS
#|   - OGS Squared
#|   - Prev. Rec.
#|   - Rec. Min. 
#|   - Crime Type
#|   - Trial
#|   - Race by Sex
#|   - OGS by Prev. Rec.
#| layout-ncol: 3
#| layout-nrow: 4
#| include: false
plot(fit_brm2, ask = FALSE, N = 3)
```

```{r}
#| label: fig-cond-eff-brm2-nonzero
#| layout-ncol: 3
#| layout-nrow: 4
#| fig-cap: Posterior estimates of the conditional effects for the non-zero-part of the lognormal hurdle model.
#| fig-subcap: 
#|   - Age
#|   - Age Squared
#|   - Sex
#|   - Offender Race
#|   - OGS
#|   - OGS Squared
#|   - Prev. Rec.
#|   - Rec. Min. 
#|   - Crime Type
#|   - Trial
#|   - Race by Sex
#|   - OGS by Prev. Rec.


theme_set(theme_bw())
plot(conditional_effects(fit_brm2), ask = FALSE)
```

```{r}
#| label: fig-cond-eff-brm2
#| layout-ncol: 3
#| layout-nrow: 4
#| fig-cap: Posterior estimates of the conditional effects for the logistic regression on the zero-part of the lognormal hurdle model.
#| fig-subcap:
#|   - Age
#|   - Age Squared
#|   - Sex
#|   - Offender Race
#|   - OGS
#|   - OGS Squared
#|   - Prev. Rec.
#|   - Rec. Min. 
#|   - Crime Type
#|   - Trial
#|   - Race by Sex
#|   - OGS by Prev. Rec.


theme_set(theme_bw())
plot(conditional_effects(fit_brm2, dpar = "hu"), ask = FALSE)
```

```{r}
#| label: fig-ppc-brm2
#| fig-cap: Posterior Predictive Checks for the lognormal hurdle model.
#| fig-subcap: 
#|   - PPC Density Sentence Length (days)
#|   - PPC Density Log of Sentence Length plus 1
#|   - PPC Scatter of average error by age
#| layout-ncol: 3
#| include: false
#| eval: false
ppred2 <- posterior_predict(fit_brm2)
pp_check(fit_brm2, ndraws = 25) +
    coord_cartesian(xlim = c(0, 60*30)) + 
    labs(title = "Posterior predictive checks",
                  subtitle = "Linear Predictor Hurdle",
         x = "Sentence Length (mon)")

bayesplot::ppc_dens_overlay(y = log1p(fit_brm2$data$JP_MIN),
                            yrep = log1p(ppred2[1:25,])) + 
    labs(title = "Posterior predictive checks",
                  subtitle = "Linear Predictor Hurdle",
         x = "log(Sentence Length (mon))")

bayesplot::ppc_error_scatter_avg_vs_x(y = log1p(fit_brm2$data$JP_MIN),
                            yrep = log1p(ppred2[1:25,]),
                            x = fit_brm2$data$DOSAGE) + 
    labs(title = "Posterior predictive checks",
                  subtitle = "Linear Predictor Hurdle",
         x = "log(Sentence Length (mon))")
```
